Renderer: llvmpipe (LLVM 20.1.2, 256 bits) by Mesa
OpenGL version: 4.5 (Core Profile) Mesa 25.0.7-0ubuntu0.24.04.2
Using optional features:
    GL_ARB_vertex_array_object
    GL_ARB_ES2_compatibility
    GL_ARB_separate_shader_objects
    GL_ARB_robustness
    GL_ARB_texture_storage
    GL_ARB_texture_view
    GL_ARB_framebuffer_no_attachments
    GL_ARB_invalidate_subdata
    GL_ARB_texture_storage_multisample
    GL_ARB_multi_bind
    GL_ARB_direct_state_access
    GL_ARB_get_texture_sub_image
    GL_ARB_texture_filter_anisotropic
    GL_KHR_debug
    GL_KHR_parallel_shader_compile
Using driver workarounds:
    no-layout-qualifiers-on-old-glsl
    mesa-implementation-color-read-format-dsa-explicit-binding
    mesa-dsa-createquery-except-pipeline-stats
    mesa-forward-compatible-line-width-range
[DEBUG] Current script: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments/run_navigation_experiments.py
[DEBUG] Added to path: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments
âœ… METRICS: Initialized trajectory tracker
âœ… METRICS: Initialized collisions tracker = 0
âœ… METRICS: Initialized start_time tracker
ğŸ¯ HabitatStore initialized - Thread-safe simulator access
ğŸ”§ OFFLINE MODE: Using cached OWL-ViT (no internet needed)
ğŸ”§ Running from: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main
ğŸ”§ MAIN_DIR: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments/policies
ğŸ”§ Python path: ['/mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments/policies', '/mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments', '/mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments', '/mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main', '/mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/ORB_SLAM3', '/mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/build/python_module', '/home/kalya/miniconda3/envs/robotics_base/lib/python39.zip', '/home/kalya/miniconda3/envs/robotics_base/lib/python3.9', '/home/kalya/miniconda3/envs/robotics_base/lib/python3.9/lib-dynload', '/home/kalya/miniconda3/envs/robotics_base/lib/python3.9/site-packages', '/home/kalya/miniconda3/envs/robotics_base/lib/python3.9/site-packages/habitat_sim-0.3.3-py3.9-linux-x86_64.egg', '/home/kalya/miniconda3/envs/robotics_base/lib/python3.9/site-packages/pillow-10.4.0-py3.9-linux-x86_64.egg']
ğŸ¯ Initializing DEADLOCK-FREE Frame Buffer: 64 frames
âœ… DEADLOCK-FREE RGB-D Buffer Ready: 64 frames, 93.8 MB
   RGB: (480, 640, 3), Depth: (480, 640)
   Memory layout: RGB=58982400 bytes, Depth=39321600 bytes
[DEBUG] âœ… ALL policies imported successfully!
[DEBUG] Imported all policy entrypoints
[DEBUG] Registered policies: ['ours_full']
[DEBUG] __main__ entry point
[DEBUG] Starting SINGLE episode run
[DEBUG] Loading evaluation_episodes.json
[DEBUG] Looking for JSON at: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments/evaluation_episodes.json
[DEBUG] Loaded 250 episodes
ğŸ¯ SINGLE EPISODE: 00800_tv_003
ğŸƒ RUNNING ALL POLICIES: ['ours_full']

============================================================
ğŸš€ RUNNING POLICY: ours_full
============================================================
ğŸš€ OursFull: 00800_tv_003
ğŸ“‹ DEBUG: Episode keys = ['episode_id', 'scene_path', 'start_pose', 'goal', 'success_radius', 'max_steps', 'seed']
ğŸ“ Frame save directory: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments/results/ours_full/00800_tv_003
âš™ï¸  DEBUG: Config created = {'num_frames': 500, 'scene_path': 'projects/hybrid_zero_shot_slam_nav/main/datasets/00800-TEEsavR23oF/TEEsavR23oF.basis.glb', 'frame_save_dir': '/mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments/results/ours_full/00800_tv_003'}
ğŸ­ DEBUG: Creating OursFullPolicy...
ğŸ“ Frame save directory configured: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments/results/ours_full/00800_tv_003
Central Map Store initialized
âœ… Task Store initialized - Pure task management
ğŸ”„ Creating HM3D environment with direct GLB loading...
ğŸ”„ Loading scene from: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/datasets/00800-TEEsavR23oF/TEEsavR23oF.basis.glb
Agent config: AgentConfiguration(height=1.5, radius=0.1, sensor_specifications=[<habitat_sim._ext.habitat_sim_bindings.CameraSensorSpec object at 0x73be70237810>, <habitat_sim._ext.habitat_sim_bindings.CameraSensorSpec object at 0x73be70237860>], action_space={'move_forward': ActionSpec(name='move_forward', actuation=ActuationSpec(amount=0.25, constraint=None)), 'turn_left': ActionSpec(name='turn_left', actuation=ActuationSpec(amount=10.0, constraint=None)), 'turn_right': ActionSpec(name='turn_right', actuation=ActuationSpec(amount=10.0, constraint=None))}, body_type='cylinder')
Action space: ['move_forward', 'turn_left', 'turn_right']
âœ… HM3D Scene Loaded Successfully!

ğŸ” === SEMANTIC MESH DEBUG (OURS_FULL) ===
[DEBUG] cfg.load_semantic_mesh = True
[DEBUG] sim.semantic_scene is None? False
[DEBUG] semantic objects count = 661
[DEBUG] FIRST 10 SEMANTIC OBJECTS:
  0: 'Unknown'
  1: 'ceiling'
  2: 'wall'
  3: 'wall'
  4: 'door'
  5: 'handle'
  6: 'wall'
  7: 'wall'
  8: 'chandelier'
  9: 'wardrobe'
[DEBUG] scene_dataset_config_file exists: True
ğŸ” === END SEMANTIC DEBUG ===

ğŸ“· Camera intrinsics extracted & stored:
{'fx': 320.00000000000006, 'fy': 320.00000000000006, 'cx': 320.0, 'cy': 240.0, 'width': 640, 'height': 480, 'hfov_deg': 90.0}
ğŸ”„ Loading navmesh: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/datasets/00800-TEEsavR23oF/TEEsavR23oF.basis.navmesh
âœ… NavMesh loaded: True
ğŸ” Test sensors available: ['rgba_camera', 'depth_camera']
 (640, 480, 4) | dtype: uint8
âœ… Depth: (640, 480) | dtype: float32
ğŸ“· RGB: HFOV=Deg(90)Â°, Resolution=Vector(640, 480)
ğŸ“· Depth: HFOV=Deg(90)Â°, Resolution=Vector(640, 480)

ORB-SLAM3 Copyright (C) 2017-2020 Carlos Campos, Richard Elvira, Juan J. GÃ³mez, JosÃ© M.M. Montiel and Juan D. TardÃ³s, University of Zaragoza.
ORB-SLAM2 Copyright (C) 2014-2016 RaÃºl Mur-Artal, JosÃ© M.M. Montiel and Juan D. TardÃ³s, University of Zaragoza.
This program comes with ABSOLUTELY NO WARRANTY;
This is free software, and you are welcome to redistribute it
under certain conditions. See LICENSE.txt.

Input sensor was set to: RGB-D
Loading settings from /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/../../configs/slam/orb_slam3_config.yaml
	-Loaded camera 1
	-Loaded image info
	-Loaded RGB-D calibration
	-Loaded ORB settings
	-Loaded viewer settings
	-Loaded Atlas settings
	-Loaded misc parameters
----------------------------------
SLAM settings: 
	-Camera 1 parameters (Pinhole): [ 320 320 320 240 ]
	-Camera 1 distortion parameters: [  0 0 0 0 0 ]
	-Original image size: [ 640 , 480 ]
	-Current image size: [ 640 , 480 ]
	-Sequence FPS: 30
	-RGB-D depth map factor: 1
	-Features per image: 1000
	-ORB scale factor: 1.2
	-ORB number of scales: 8
	-Initial FAST threshold: 20
	-Min FAST threshold: 7


Loading ORB Vocabulary. This could take a while...
Vocabulary loaded!

Initialization of Atlas from scratch 
Creation of new map with id: 0
Creation of new map with last KF id: 0
Seq. Name: 
There are 1 cameras in the atlas
Camera 0 is pinhole
checking self.env instance for habitat sim  inside create hm3d habitat env function Simulator(config=Configuration(sim_cfg=<habitat_sim._ext.habitat_sim_bindings.SimulatorConfiguration object at 0x73be70240430>, agents=[AgentConfiguration(height=1.5, radius=0.1, sensor_specifications=[<habitat_sim._ext.habitat_sim_bindings.CameraSensorSpec object at 0x73be70237810>, <habitat_sim._ext.habitat_sim_bindings.CameraSensorSpec object at 0x73be70237860>], action_space={'move_forward': ActionSpec(name='move_forward', actuation=ActuationSpec(amount=0.25, constraint=None)), 'turn_left': ActionSpec(name='turn_left', actuation=ActuationSpec(amount=10.0, constraint=None)), 'turn_right': ActionSpec(name='turn_right', actuation=ActuationSpec(amount=10.0, constraint=None))}, body_type='cylinder')], metadata_mediator=None, enable_batch_renderer=False), agents=[Agent(agent_config=AgentConfiguration(height=1.5, radius=0.1, sensor_specifications=[<habitat_sim._ext.habitat_sim_bindings.CameraSensorSpec object at 0x73be70237810>, <habitat_sim._ext.habitat_sim_bindings.CameraSensorSpec object at 0x73be70237860>], action_space={'move_forward': ActionSpec(name='move_forward', actuation=ActuationSpec(amount=0.25, constraint=None)), 'turn_left': ActionSpec(name='turn_left', actuation=ActuationSpec(amount=10.0, constraint=None)), 'turn_right': ActionSpec(name='turn_right', actuation=ActuationSpec(amount=10.0, constraint=None))}, body_type='cylinder'), _sensors={'rgba_camera': <habitat_sim._ext.habitat_sim_bindings.CameraSensor object at 0x73be70240930>, 'depth_camera': <habitat_sim._ext.habitat_sim_bindings.CameraSensor object at 0x73be702409b0>}, controls=ObjectControls(move_filter_fn=<bound method Simulator.step_filter of ...>), body=<_magnum.scenegraph.AbstractFeature3D object at 0x73be70240870>)], _num_total_frames=0, _default_agent_id=0, _Simulator__sensors=[{'rgba_camera': <habitat_sim.simulator.Sensor object at 0x73be70c63790>, 'depth_camera': <habitat_sim.simulator.Sensor object at 0x73be70c637c0>}], _initialized=True, _previous_step_time=0.0, _async_draw_agent_ids=None, _Simulator__last_state={0: AgentState(position=array([-9.448947  ,  0.16337794, -1.182925  ], dtype=float32), rotation=quaternion(1, 0, 2.45858027483337e-05, 0), sensor_states={'rgba_camera': SixDOFPose(position=array([-9.448947,  1.663378, -1.182925], dtype=float32), rotation=quaternion(1, 0, 2.45858027483337e-05, 0)), 'depth_camera': SixDOFPose(position=array([-9.448947,  1.663378, -1.182925], dtype=float32), rotation=quaternion(1, 0, 2.45858027483337e-05, 0))})})
âœ… Simulator stored 
ğŸ¯ Initializing robot at center with exploration...
  ğŸ” Initial position: [-9.448947    0.16337794 -1.182925  ]
  ğŸ” Initial rotation: quaternion(1, 0, 2.45858027483337e-05, 0)
  ğŸ¯ Initial positioning 1/16: move_forward
  ğŸ¯ Initial positioning 2/16: move_forward
  ğŸ¯ Initial positioning 3/16: move_forward
  ğŸ¯ Initial positioning 4/16: turn_left
  ğŸ¯ Initial positioning 5/16: turn_left
  ğŸ¯ Initial positioning 6/16: turn_left
  ğŸ¯ Initial positioning 7/16: turn_left
  ğŸ¯ Initial positioning 8/16: move_forward
  ğŸ¯ Initial positioning 9/16: move_forward
  ğŸ¯ Initial positioning 10/16: turn_right
  ğŸ¯ Initial positioning 11/16: turn_right
  ğŸ¯ Initial positioning 12/16: turn_right
  ğŸ¯ Initial positioning 13/16: move_forward
  ğŸ¯ Initial positioning 14/16: move_forward
  ğŸ¯ Initial positioning 15/16: turn_left
  ğŸ¯ Initial positioning 16/16: move_forward
âœ… Robot positioned at center with exploration pattern
Occupancy grid: 50x50 cells, 0.05m resolution
ORBSLAMIntegration instance created
Initializing ORB-SLAM3 system...
ğŸ”„ ORB-SLAM: Starting initialization...
ğŸ“ ORB-SLAM: Config path: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/configs/slam/orb_slam3_config.yaml
ğŸ“ ORB-SLAM: Vocab path: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/ORB_SLAM3/Vocabulary/ORBvoc.txt
Looking for config at: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/configs/slam/orb_slam3_config.yaml
Looking for vocab at: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/ORB_SLAM3/Vocabulary/ORBvoc.txt
Config file size: 758 bytes
Vocab file size: 145250924 bytes
Creating ORB-SLAM3 System instance for RGB-D...
ORB-SLAM3 RGB-D initialized and processing started
âœ… ORB-SLAM: Successfully initialized!
ğŸ¦‰ OWLIntegration created - Consumer only (reads from global buffer)
ğŸ¯ HABITAT ACTUAL POSITION: [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: None
ğŸ”   last_processed_time: None
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK: âœ… First frame, will process
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ BUFFER DEBUG: Frame ID requested: -1
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWLIntegration created - Consumer only (reads from global buffer)
ğŸ“¢ PredictionStore: New subscriber added. Total: 1
Integrated ReasoningPipeline instance created
ğŸ”® IjepaPredictor: Loading prototypes directly from JSON...
âœ… free_space: shape torch.Size([1, 1280])
âœ… Loaded 1 centroids
ğŸ”® IjepaPredictor: Starting I-JEPA initialization...
ğŸ”® IjepaPredictor: Loading model facebook/ijepa_vith14_1k...
ğŸ¯ HABITAT ACTUAL POSITION: [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: None
ğŸ”   last_processed_time: None
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK: âœ… First frame, will process
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ BUFFER DEBUG: Frame ID requested: -1
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¯ HABITAT ACTUAL POSITION: [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: None
ğŸ”   last_processed_time: None
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK: âœ… First frame, will process
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ BUFFER DEBUG: Frame ID requested: -1
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¯ HABITAT ACTUAL POSITION: [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: None
ğŸ”   last_processed_time: None
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK: âœ… First frame, will process
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ BUFFER DEBUG: Frame ID requested: -1
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¯ HABITAT ACTUAL POSITION: [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: None
ğŸ”   last_processed_time: None
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK: âœ… First frame, will process
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ BUFFER DEBUG: Frame ID requested: -1
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¯ HABITAT ACTUAL POSITION: [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: None
ğŸ”   last_processed_time: None
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK: âœ… First frame, will process
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ BUFFER DEBUG: Frame ID requested: -1
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¯ HABITAT ACTUAL POSITION: [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: None
ğŸ”   last_processed_time: None
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK: âœ… First frame, will process
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ BUFFER DEBUG: Frame ID requested: -1
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¯ HABITAT ACTUAL POSITION: [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: None
ğŸ”   last_processed_time: None
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-9.931099    0.16337794 -2.6233222 ]
ğŸ” MOTION CHECK: âœ… First frame, will process
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ BUFFER DEBUG: Frame ID requested: -1
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ”® IjepaPredictor: âœ… Model loaded successfully via transformers
ğŸ”® IjepaPredictor: âœ… I-JEPA fully initialized with transformers!
ğŸš€ Action Executor initialized - Continuous Navigation Built-in
TaskStore[127265840103232] new subscriber â†’ <bound method ActionPipeline.on_action_plan_ready of <experiments.paper_experiments.Main_Experiments.policies.ours_full.run_ours_full_action_pipeline.ActionPipeline object at 0x73be67ab6340>>
ğŸ›ï¸ Main Orchestrator - ORB-SLAM created, simulator ready
âœ… DEBUG: Orchestrator created = <experiments.paper_experiments.Main_Experiments.policies.ours_full.ours_full_policy.OursFullPolicy object at 0x73bf8492f610>
ğŸ¯ DEBUG: Setting episode start pose...
ğŸ‘¤ DEBUG: Agent 0 acquired
ğŸ“ DEBUG: Current state = pos:[-9.931099    0.16337794 -2.6233222 ], rot:quaternion(0.98480349779129, 0, 0.173672437667847, 0)
ğŸ“ DEBUG: Set position = [-1.31  1.5   1.53]
ğŸ”„ DEBUG: Yaw = 0.74
ğŸ”„ DEBUG: New rotation quat = [0.         0.36161543 0.         0.93232735]
âœ… DEBUG: Start pose applied!
ğŸ’¾ DEBUG: initial_pose set in HabitatStore: {'position': [-1.31, 1.5, 1.53], 'rotation': quaternion(0.932327345606034, 0, 0.361615431964962, 0)}
ğŸ” DEBUG: Searching for goal object...
ğŸ¯ DEBUG: Target category = 'tv'
ğŸ“Š DEBUG: Found 661 semantic objects
  ğŸ” Obj 0: 'Unknown'
  ğŸ” Obj 1: 'ceiling'
  ğŸ” Obj 2: 'wall'
  ğŸ” Obj 3: 'wall'
  ğŸ” Obj 4: 'door'
  ğŸ” Obj 5: 'handle'
  ğŸ” Obj 6: 'wall'
  ğŸ” Obj 7: 'wall'
  ğŸ” Obj 8: 'chandelier'
  ğŸ” Obj 9: 'wardrobe'
  ğŸ” Obj 10: 'tv'
âœ… DEBUG: GOAL FOUND! pos = [-0.15094196796417236, 2.3524980545043945, -2.4764504432678223]
ğŸ’¾ DEBUG: Goal stored to HabitatStore: tv at [-0.15094196796417236, 2.3524980545043945, -2.4764504432678223]
ğŸ¯ DEBUG: Final goal_pos = [-0.15094196796417236, 2.3524980545043945, -2.4764504432678223]
ğŸš€ DEBUG: Starting orchestration...
ğŸš€ Starting main orchestration for task: find tv
checking self.env instance for habitat sim inside run_orchestration function Simulator(config=Configuration(sim_cfg=<habitat_sim._ext.habitat_sim_bindings.SimulatorConfiguration object at 0x73be70240430>, agents=[AgentConfiguration(height=1.5, radius=0.1, sensor_specifications=[<habitat_sim._ext.habitat_sim_bindings.CameraSensorSpec object at 0x73be70237810>, <habitat_sim._ext.habitat_sim_bindings.CameraSensorSpec object at 0x73be70237860>], action_space={'move_forward': ActionSpec(name='move_forward', actuation=ActuationSpec(amount=0.25, constraint=None)), 'turn_left': ActionSpec(name='turn_left', actuation=ActuationSpec(amount=10.0, constraint=None)), 'turn_right': ActionSpec(name='turn_right', actuation=ActuationSpec(amount=10.0, constraint=None))}, body_type='cylinder')], metadata_mediator=None, enable_batch_renderer=False), agents=[Agent(agent_config=AgentConfiguration(height=1.5, radius=0.1, sensor_specifications=[<habitat_sim._ext.habitat_sim_bindings.CameraSensorSpec object at 0x73be70237810>, <habitat_sim._ext.habitat_sim_bindings.CameraSensorSpec object at 0x73be70237860>], action_space={'move_forward': ActionSpec(name='move_forward', actuation=ActuationSpec(amount=0.25, constraint=None)), 'turn_left': ActionSpec(name='turn_left', actuation=ActuationSpec(amount=10.0, constraint=None)), 'turn_right': ActionSpec(name='turn_right', actuation=ActuationSpec(amount=10.0, constraint=None))}, body_type='cylinder'), _sensors={'rgba_camera': <habitat_sim._ext.habitat_sim_bindings.CameraSensor object at 0x73be70240930>, 'depth_camera': <habitat_sim._ext.habitat_sim_bindings.CameraSensor object at 0x73be702409b0>}, controls=ObjectControls(move_filter_fn=<bound method Simulator.step_filter of ...>), body=<_magnum.scenegraph.AbstractFeature3D object at 0x73be70240870>)], _num_total_frames=16, _default_agent_id=0, _Simulator__sensors=[{'rgba_camera': <habitat_sim.simulator.Sensor object at 0x73be70c63790>, 'depth_camera': <habitat_sim.simulator.Sensor object at 0x73be70c637c0>}], _initialized=True, _previous_step_time=0.00016570091247558594, _async_draw_agent_ids=None, _Simulator__last_state={0: AgentState(position=array([-9.931099  ,  0.16337794, -2.6233222 ], dtype=float32), rotation=quaternion(0.98480349779129, 0, 0.173672437667847, 0), sensor_states={'rgba_camera': SixDOFPose(position=array([-9.931099 ,  1.663378 , -2.6233222], dtype=float32), rotation=quaternion(0.98480349779129, 0, 0.173672437667847, 0)), 'depth_camera': SixDOFPose(position=array([-9.931099 ,  1.663378 , -2.6233222], dtype=float32), rotation=quaternion(0.98480349779129, 0, 0.173672437667847, 0))})})
ğŸ“ METRICS: START POS = [-1.31  1.5   1.53]
ğŸ“ METRICS: Trajectory len = 1
â±ï¸  METRICS: Start time recorded
ğŸ¯ Processing limit: 500 frames
ğŸ¯ Mission goals set: ['tv']
ğŸ“‹ Added task: explore - Explore unknown area
ğŸ¯ Initial task set: find tv
âœ… Created task: find tv
ğŸ“‹ Complete reasoning plan stored with ID: reason_1766634510284
ğŸ“Š Action plan has 2 waypoints
ğŸ“ Also stored in intermediate_reasoning for OWL access
ğŸ”„ Starting pipeline processing...
Initializing OWL integration...
Loading OWL-ViT model: google/owlvit-base-patch32
ğŸ¯ HABITAT ACTUAL POSITION: [-1.31  1.5   1.53]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: None
ğŸ”   last_processed_time: None
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.31  1.5   1.53]
ğŸ” MOTION CHECK: âœ… First frame, will process
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ BUFFER DEBUG: Frame ID requested: -1
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¯ HABITAT ACTUAL POSITION: [-1.31  1.5   1.53]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: None
ğŸ”   last_processed_time: None
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.31  1.5   1.53]
ğŸ” MOTION CHECK: âœ… First frame, will process
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ BUFFER DEBUG: Frame ID requested: -1
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
âœ… OWL-ViT model loaded successfully
ğŸ” OWL Model device after loading: cuda:0
ğŸ¦‰ OWL DEBUG: Model loaded successfully - ready for frame buffer processing
âœ… OWL integration ready - Processing every 3 frames from buffer
ğŸŸ¢ OWL continuous processing loop started
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: Started continuous processing
Initializing integrated reasoning pipeline...
Spatial Reasoning Integration initialized - Stores available: True
âœ… Spatial Reasoning Integration initialized
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.31  1.5   1.53]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: None
ğŸ”   last_processed_time: None
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.31  1.5   1.53]
ğŸ” MOTION CHECK: âœ… First frame, will process
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ BUFFER DEBUG: Frame ID requested: -1
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ LLM DEBUG: Starting to load TinyLlama/TinyLlama-1.1B-Chat-v1.0
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.31  1.5   1.53]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: None
ğŸ”   last_processed_time: None
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.31  1.5   1.53]
ğŸ” MOTION CHECK: âœ… First frame, will process
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ BUFFER DEBUG: Frame ID requested: -1
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
âœ… LLM DEBUG: Pipeline loaded successfully on cpu
ğŸŒ³ Tree of Thoughts Integration initialized with LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0
   ğŸ“ Map Store: âœ…
   ğŸ¯ User Command Store: âœ…
   ğŸ“Š Prediction Store: âœ…
   ğŸ“ Task Store: âœ…
âœ… Tree of Thoughts Integration initialized
ğŸ”§ INITIALIZATION DEBUG:
   Spatial Reasoning: True
   Tree of Thoughts: True
âœ… Integrated reasoning pipeline initialized successfully
   global_habitat_store imported: True
ğŸŒ± Exploration step 1
ğŸ“¤ Pushing exploration action 'velocity_control' to HabitatStore
ğŸ“¥ HABITATSTORE: Pushing action 'velocity_control' to queue
ğŸ†• HABITATSTORE: Creating new action queue
ğŸ“¦ HABITATSTORE: Added metadata to action 'velocity_control'
âœ… HABITATSTORE: Action 'velocity_control' queued (ID: action_1766634512479_0)
ğŸ“Š HABITATSTORE: Queue size: 1
â³ Waiting for exploration action to complete...
â³ No result yet for action_1766634512479_0
Result from action pipeline exploration: None
âš ï¸ No result received for exploration step 1
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸŒ± Exploration step 2
ğŸ“¤ Pushing exploration action 'velocity_control' to HabitatStore
ğŸ“¥ HABITATSTORE: Pushing action 'velocity_control' to queue
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ“¦ HABITATSTORE: Added metadata to action 'velocity_control'
âœ… HABITATSTORE: Action 'velocity_control' queued (ID: action_1766634512600_1)
ğŸ“Š HABITATSTORE: Queue size: 2
â³ Waiting for exploration action to complete...
â³ No result yet for action_1766634512600_1
Result from action pipeline exploration: None
âš ï¸ No result received for exploration step 2
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸŒ± Exploration step 3
ğŸ“¤ Pushing exploration action 'velocity_control' to HabitatStore
ğŸ“¥ HABITATSTORE: Pushing action 'velocity_control' to queue
ğŸ“¦ HABITATSTORE: Added metadata to action 'velocity_control'
âœ… HABITATSTORE: Action 'velocity_control' queued (ID: action_1766634512722_2)
ğŸ“Š HABITATSTORE: Queue size: 3
â³ Waiting for exploration action to complete...
â³ No result yet for action_1766634512722_2
Result from action pipeline exploration: None
âš ï¸ No result received for exploration step 3
ğŸ¯ HABITAT ACTUAL POSITION: [-1.31  1.5   1.53]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: None
ğŸ”   last_processed_time: None
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.31  1.5   1.53]
ğŸ” MOTION CHECK: âœ… First frame, will process
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ BUFFER DEBUG: Frame ID requested: -1
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸŒ± Exploration step 4
ğŸ“¤ Pushing exploration action 'velocity_control' to HabitatStore
ğŸ“¥ HABITATSTORE: Pushing action 'velocity_control' to queue
ğŸ“¦ HABITATSTORE: Added metadata to action 'velocity_control'
âœ… HABITATSTORE: Action 'velocity_control' queued (ID: action_1766634512845_3)
ğŸ“Š HABITATSTORE: Queue size: 4
â³ Waiting for exploration action to complete...
â³ No result yet for action_1766634512845_3
Result from action pipeline exploration: None
âš ï¸ No result received for exploration step 4
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸŒ± Exploration step 5
ğŸ“¤ Pushing exploration action 'velocity_control' to HabitatStore
ğŸ“¥ HABITATSTORE: Pushing action 'velocity_control' to queue
ğŸ“¦ HABITATSTORE: Added metadata to action 'velocity_control'
âœ… HABITATSTORE: Action 'velocity_control' queued (ID: action_1766634512966_4)
ğŸ“Š HABITATSTORE: Queue size: 5
â³ Waiting for exploration action to complete...
â³ No result yet for action_1766634512966_4
Result from action pipeline exploration: None
âš ï¸ No result received for exploration step 5
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
âœ… Exploration budget exhausted, returning control
âœ… All pipelines started successfully
ğŸ” Mission check: 1 total goals, 1 remaining
   Mission goals: {'tv'}
   Remaining goals: ['tv']

--- FRAME 1/500 ---
ğŸ”„ Stepping simulator 3 times to warm up sensors...
DEBUG: about to step simulator
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
DEBUG: about to step simulator
DEBUG: about to step simulator
âœ… Simulator warm-up complete, first frames ready
First KF:0; Map init KF:0
New Map created with 987 points
DEBUG: about to get sensor observations
ğŸ¦‰ OWL: starting to Process REAL frame -1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ CURRENT SENSORS: ['rgba_camera', 'depth_camera']
DEBUG: about to extract rgb and depth
ğŸ¯ DEPTH STATS - Min: 0.0, Max: 6.9150872230529785, Mean: 1.456
âœ… Depth frame has 307199 valid pixels
DEBUG: RGBA -> RGB conversion for frame 0
DEBUG: Transposing rgb_frame from (640, 480, 3) to (480, 640, 3)
DEBUG: Depth frame shape before transpose: (640, 480)
DEBUG: Depth frame shape after transpose: (480, 640)
DEBUG: Converted depth dtype to uint16
DEBUG: about to rotate rgb frame
DEBUG: about to write rgb frame
ğŸ¯ HABITAT ACTUAL POSITION: [-1.5029818   0.16337794  0.2222702 ]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: None
ğŸ”   last_processed_time: None
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.5029818   0.16337794  0.2222702 ]
ğŸ” MOTION CHECK: âœ… First frame, will process
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ BUFFER DEBUG: Frame ID requested: -1
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
DEBUG: about to scale depth
ğŸ¯ DEBUG: Frame 0 - RGB shape: (480, 640, 3), Depth shape: (480, 640)
ğŸ¯ DEBUG: RGB dtype: uint8, Depth dtype: uint16
âœ… DEBUG: Frame 0 written to buffer - Slot 0
âœ… DEBUG: Buffer RGB dtype: uint8, Depth dtype: uint16
  Wrote frame 0 to buffer successfully âœ…
  âœ… Frame 1 captured - RGB: (480, 640, 3), Depth: (480, 640)
ğŸŸ¦ DEBUG: Calling ORB-SLAM process_frame() for frame 0
ğŸ¯ HABITAT ACTUAL POSITION: [-1.5029818   0.16337794  0.2222702 ]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: None
ğŸ”   last_processed_time: None
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.5029818   0.16337794  0.2222702 ]
ğŸ” MOTION CHECK: âœ… First frame, will process
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'tuple'>
âœ… BUFFER DEBUG: Got frame_dict type: <class 'dict'>
âœ… BUFFER DEBUG: Got metadata type: <class 'dict'>
ğŸ¯ DEPTH SCALE CHECK: range=(0.0, 6915.0) dtype=uint16
âœ… ORB-SLAM: PROCESSED FRAME #1 (buffer ID: 0) - first_frame
   Current position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”„ CONVERTING: Depth appears to be in millimeters
   Before: 0.0 to 6915.0 (uint16)
   After: 0.000m to 6.915m (float32)
ğŸ” DEPTH VALIDATION: 307199 valid pixels
ğŸ” RGB-D Frames: RGB=(480, 640, 3), Depth=(480, 640)
ğŸ” Depth info: dtype=float32, range=(0.000, 6.915)
ğŸ” DEBUG process_frame parameters:
   RGB: dtype=uint8, shape=(480, 640, 3)
   Depth: dtype=float32, shape=(480, 640)
   Timestamp: 0.0
ğŸ” ORB-SLAM RAW RESULT KEYS: ['current_pose', 'tracking_status', 'visible_points']
Current result dict pose : [[1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 0. 1.]]
ğŸ” ORB-SLAM INTERNAL DEBUG:
   Raw tracking_info keys: ['tracking_ok', 'tracking_lost', 'system_shutdown', 'visible_points_count']
   ğŸ“ tracking_ok: True
   ğŸ“ tracking_lost: False
   ğŸ“ system_shutdown: False
   ğŸ¯ TRACKING STATUS: âœ… TRACKING
   ğŸ¯ LOST STATUS: âœ… NOT LOST
ğŸ” FEATURE DEBUG:
   ğŸ‘ï¸ Visible points: 987
ğŸ¤– ORB-SLAM: Received valid 4x4 pose matrix
â³ Coordinates from orbslam Not aligned yet, storing SLAM-local pose
ğŸ“¦ Stored pose pair #1
   SLAM position: [[1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 0. 1.]]
   Habitat position: [[ 0.73846858  0.          0.67428788 -1.50298178]
 [ 0.          1.          0.          0.16337794]
 [-0.67428788  0.          0.73846858  0.22227021]
 [ 0.          0.          0.          1.        ]]
ğŸ¯ STORE DEBUG - Frame 0:
   ğŸ“¦ Current pose type: <class 'numpy.ndarray'>
   ğŸ“¦ Pose shape: (4, 4)
   ğŸ“¦ Translation: [[1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 0. 1.]]
ğŸ—ºï¸ ORB-SLAM: 987 tracked points this frame
âœ… ORB-SLAM: Pose stored successfully
ğŸ“¦ Stored complete 4x4 pose matrix for frame 0
ğŸ¤– ORB-SLAM: Complete pose stored directly
ğŸ” Point conversion: 987 success, 987 rich points
ğŸ’¾ ORB-SLAM: Added 987 points to memory map
   ğŸ¯ Current Frame: 0
   ğŸ¤– Robot Position: [[1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 0. 1.]]
ğŸ’¾ ORB-SLAM: Frame 0 WORLD-ALIGNED data stored
ğŸ“Š ORB-SLAM FRAME 0 SUMMARY:
   Method: rgbd_processing
   Pose estimated: True
   Visible points: 987
   Tracking: OK
ğŸŸ© DEBUG: Returned from ORB-SLAM process_frame() for frame 0
ğŸ” Checking for queued actions...
ğŸ¯ HABITATSTORE: Pulling next action from queue
ğŸš€ HABITATSTORE: Pulled VELOCITY CONTROL command
   ğŸ“¦ Metadata: ['exploration_step', 'continuous_nav', 'exploration_mode', 'velocity_data']
ğŸ“Š HABITATSTORE: Remaining in queue: 4
ğŸ¯ MAINORCH: Executing action 'velocity_control'
================================================================================
ğŸ¯ Executing Habitat action: velocity_control
ğŸ“¦ Parameters: {'linear_velocity': [0.0, 0.0, 0.0], 'angular_velocity': [0.0, 0.0, 0.7], 'duration': 0.5}
ğŸ“¦ Metadata: {'exploration_step': 1, 'continuous_nav': True, 'exploration_mode': 'random_turn_left', 'velocity_data': {'lin_vel': 0.0, 'ang_vel': 0.7, 'duration': 0.5}}
================================================================================
ğŸ§© Current agent position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ§© Current agent rotation (quat): quaternion(0.932327330112457, 0, 0.361615419387817, 0)
ğŸ“ prev_pos saved: [-1.5029818   0.16337794  0.2222702 ]
ğŸ§® Linear velocity: [0.0, 0.0, 0.0], angular velocity: [0.0, 0.0, 0.7], dt=0.5
ğŸ“ RigidState before integrate: pos=Vector(-1.50298, 0.163378, 0.22227), rot=Quaternion({0, 0.361615, 0}, 0.932327)
ğŸ“ Predicted new_pos from VelocityControl: [-1.5029817819595337, 0.16337794065475464, 0.222270205616951]
ğŸ§­ is_navigable(new_pos) = True
âœ… Using XZ from new_pos with preserved Y: [-1.5029817819595337, 1.0, 0.222270205616951]
ğŸ“Œ Final applied position: [-1.5029817819595337, 1.0, 0.222270205616951]
ğŸ“Œ Final applied rotation (quat): quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857)
ğŸ›‘ STALL CHECK: moved=0.8366m, stalled=True
ğŸ”„ METRICS: Velocity control - tracking position
ğŸ“ METRICS: Post-velocity pos = [-1.5029818  1.         0.2222702]
ğŸ“ METRICS: Trajectory len = 2
Observations from _execute_habitat_action : velocity control : {'rgba_camera': array([[[142, 132, 115, 255],
        [142, 132, 115, 255],
        [142, 132, 115, 255],
        ...,
        [181, 154, 140, 255],
        [181, 154, 140, 255],
        [179, 154, 138, 255]],

       [[142, 132, 115, 255],
        [142, 132, 115, 255],
        [142, 132, 115, 255],
        ...,
        [181, 154, 140, 255],
        [181, 154, 140, 255],
        [179, 154, 138, 255]],

       [[142, 132, 115, 255],
        [142, 132, 115, 255],
        [142, 132, 115, 255],
        ...,
        [180, 153, 139, 255],
        [181, 154, 140, 255],
        [179, 154, 138, 255]],

       ...,

       [[147, 163, 171, 255],
        [122, 132, 139, 255],
        [122, 132, 139, 255],
        ...,
        [145, 101,  71, 255],
        [145, 101,  71, 255],
        [143,  97,  63, 255]],

       [[110, 126, 138, 255],
        [125, 136, 142, 255],
        [ 59,  40,  28, 255],
        ...,
        [144, 103,  77, 255],
        [145, 101,  71, 255],
        [143,  97,  63, 255]],

       [[ 65,  51,  41, 255],
        [ 60,  44,  32, 255],
        [130, 138, 146, 255],
        ...,
        [147, 105,  79, 255],
        [144, 103,  77, 255],
        [143,  97,  63, 255]]], dtype=uint8), 'depth_camera': array([[0.41971582, 0.42041945, 0.42112437, ..., 0.6024412 , 0.60159004,
        0.6007457 ],
       [0.41998583, 0.42069036, 0.4213962 , ..., 0.6039071 , 0.60305184,
        0.6022033 ],
       [0.4202562 , 0.42096165, 0.4216684 , ..., 0.6053802 , 0.60452074,
        0.6036681 ],
       ...,
       [0.8939023 , 0.8983681 , 0.9014477 , ..., 2.6627388 , 2.666929  ,
        2.6711326 ],
       [0.88979673, 0.8960507 , 0.8250862 , ..., 2.6512516 , 2.6554058 ,
        2.659573  ],
       [0.81859267, 0.8220662 , 0.89677393, ..., 2.6398628 , 2.6439815 ,
        2.648113  ]], dtype=float32)}
ğŸ‘ rgba_camera shape: (640, 480, 4)
ğŸŒŠ depth_camera shape: (640, 480)
âœ… Velocity motion applied (projected)
ğŸ“¤ Result (velocity_control): status=completed, collided=True, new_position=[-1.5029817819595337, 1.0, 0.222270205616951]
================================================================================
ğŸ“¤ Result (velocity_control):{'action_id': 'action_1766634512479_0', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[142, 132, 115, 255],
        [142, 132, 115, 255],
        [142, 132, 115, 255],
        ...,
        [181, 154, 140, 255],
        [181, 154, 140, 255],
        [179, 154, 138, 255]],

       [[142, 132, 115, 255],
        [142, 132, 115, 255],
        [142, 132, 115, 255],
        ...,
        [181, 154, 140, 255],
        [181, 154, 140, 255],
        [179, 154, 138, 255]],

       [[142, 132, 115, 255],
        [142, 132, 115, 255],
        [142, 132, 115, 255],
        ...,
        [180, 153, 139, 255],
        [181, 154, 140, 255],
        [179, 154, 138, 255]],

       ...,

       [[147, 163, 171, 255],
        [122, 132, 139, 255],
        [122, 132, 139, 255],
        ...,
        [145, 101,  71, 255],
        [145, 101,  71, 255],
        [143,  97,  63, 255]],

       [[110, 126, 138, 255],
        [125, 136, 142, 255],
        [ 59,  40,  28, 255],
        ...,
        [144, 103,  77, 255],
        [145, 101,  71, 255],
        [143,  97,  63, 255]],

       [[ 65,  51,  41, 255],
        [ 60,  44,  32, 255],
        [130, 138, 146, 255],
        ...,
        [147, 105,  79, 255],
        [144, 103,  77, 255],
        [143,  97,  63, 255]]], dtype=uint8), 'depth_camera': array([[0.41971582, 0.42041945, 0.42112437, ..., 0.6024412 , 0.60159004,
        0.6007457 ],
       [0.41998583, 0.42069036, 0.4213962 , ..., 0.6039071 , 0.60305184,
        0.6022033 ],
       [0.4202562 , 0.42096165, 0.4216684 , ..., 0.6053802 , 0.60452074,
        0.6036681 ],
       ...,
       [0.8939023 , 0.8983681 , 0.9014477 , ..., 2.6627388 , 2.666929  ,
        2.6711326 ],
       [0.88979673, 0.8960507 , 0.8250862 , ..., 2.6512516 , 2.6554058 ,
        2.659573  ],
       [0.81859267, 0.8220662 , 0.89677393, ..., 2.6398628 , 2.6439815 ,
        2.648113  ]], dtype=float32)}, 'collided': True, 'new_position': [-1.5029817819595337, 1.0, 0.222270205616951], 'new_rotation': quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857), 'timestamp': 1766634513.6181881}
Action in habitat simulation result : {'action_id': 'action_1766634512479_0', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[142, 132, 115, 255],
        [142, 132, 115, 255],
        [142, 132, 115, 255],
        ...,
        [181, 154, 140, 255],
        [181, 154, 140, 255],
        [179, 154, 138, 255]],

       [[142, 132, 115, 255],
        [142, 132, 115, 255],
        [142, 132, 115, 255],
        ...,
        [181, 154, 140, 255],
        [181, 154, 140, 255],
        [179, 154, 138, 255]],

       [[142, 132, 115, 255],
        [142, 132, 115, 255],
        [142, 132, 115, 255],
        ...,
        [180, 153, 139, 255],
        [181, 154, 140, 255],
        [179, 154, 138, 255]],

       ...,

       [[147, 163, 171, 255],
        [122, 132, 139, 255],
        [122, 132, 139, 255],
        ...,
        [145, 101,  71, 255],
        [145, 101,  71, 255],
        [143,  97,  63, 255]],

       [[110, 126, 138, 255],
        [125, 136, 142, 255],
        [ 59,  40,  28, 255],
        ...,
        [144, 103,  77, 255],
        [145, 101,  71, 255],
        [143,  97,  63, 255]],

       [[ 65,  51,  41, 255],
        [ 60,  44,  32, 255],
        [130, 138, 146, 255],
        ...,
        [147, 105,  79, 255],
        [144, 103,  77, 255],
        [143,  97,  63, 255]]], dtype=uint8), 'depth_camera': array([[0.41971582, 0.42041945, 0.42112437, ..., 0.6024412 , 0.60159004,
        0.6007457 ],
       [0.41998583, 0.42069036, 0.4213962 , ..., 0.6039071 , 0.60305184,
        0.6022033 ],
       [0.4202562 , 0.42096165, 0.4216684 , ..., 0.6053802 , 0.60452074,
        0.6036681 ],
       ...,
       [0.8939023 , 0.8983681 , 0.9014477 , ..., 2.6627388 , 2.666929  ,
        2.6711326 ],
       [0.88979673, 0.8960507 , 0.8250862 , ..., 2.6512516 , 2.6554058 ,
        2.659573  ],
       [0.81859267, 0.8220662 , 0.89677393, ..., 2.6398628 , 2.6439815 ,
        2.648113  ]], dtype=float32)}, 'collided': True, 'new_position': [-1.5029817819595337, 1.0, 0.222270205616951], 'new_rotation': quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857), 'timestamp': 1766634513.6181881}
âœ… Storing SINGLE result for action_1766634512479_0
Storing result using store_action_result() for action_1766634512479_0 | All stored IDs: ['action_1766634512479_0']
Stored results for id : action_1766634512479_0 : {'action_id': 'action_1766634512479_0', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[142, 132, 115, 255],
        [142, 132, 115, 255],
        [142, 132, 115, 255],
        ...,
        [181, 154, 140, 255],
        [181, 154, 140, 255],
        [179, 154, 138, 255]],

       [[142, 132, 115, 255],
        [142, 132, 115, 255],
        [142, 132, 115, 255],
        ...,
        [181, 154, 140, 255],
        [181, 154, 140, 255],
        [179, 154, 138, 255]],

       [[142, 132, 115, 255],
        [142, 132, 115, 255],
        [142, 132, 115, 255],
        ...,
        [180, 153, 139, 255],
        [181, 154, 140, 255],
        [179, 154, 138, 255]],

       ...,

       [[147, 163, 171, 255],
        [122, 132, 139, 255],
        [122, 132, 139, 255],
        ...,
        [145, 101,  71, 255],
        [145, 101,  71, 255],
        [143,  97,  63, 255]],

       [[110, 126, 138, 255],
        [125, 136, 142, 255],
        [ 59,  40,  28, 255],
        ...,
        [144, 103,  77, 255],
        [145, 101,  71, 255],
        [143,  97,  63, 255]],

       [[ 65,  51,  41, 255],
        [ 60,  44,  32, 255],
        [130, 138, 146, 255],
        ...,
        [147, 105,  79, 255],
        [144, 103,  77, 255],
        [143,  97,  63, 255]]], dtype=uint8), 'depth_camera': array([[0.41971582, 0.42041945, 0.42112437, ..., 0.6024412 , 0.60159004,
        0.6007457 ],
       [0.41998583, 0.42069036, 0.4213962 , ..., 0.6039071 , 0.60305184,
        0.6022033 ],
       [0.4202562 , 0.42096165, 0.4216684 , ..., 0.6053802 , 0.60452074,
        0.6036681 ],
       ...,
       [0.8939023 , 0.8983681 , 0.9014477 , ..., 2.6627388 , 2.666929  ,
        2.6711326 ],
       [0.88979673, 0.8960507 , 0.8250862 , ..., 2.6512516 , 2.6554058 ,
        2.659573  ],
       [0.81859267, 0.8220662 , 0.89677393, ..., 2.6398628 , 2.6439815 ,
        2.648113  ]], dtype=float32)}, 'collided': True, 'new_position': [-1.5029817819595337, 1.0, 0.222270205616951], 'new_rotation': quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857), 'timestamp': 1766634513.6181881}
âœ… Action 'velocity_control' completed, result stored
ğŸ¯ HABITATSTORE: Pulling next action from queue
ğŸš€ HABITATSTORE: Pulled VELOCITY CONTROL command
   ğŸ“¦ Metadata: ['exploration_step', 'continuous_nav', 'exploration_mode', 'velocity_data']
ğŸ“Š HABITATSTORE: Remaining in queue: 3
ğŸ¯ MAINORCH: Executing action 'velocity_control'
================================================================================
ğŸ¯ Executing Habitat action: velocity_control
ğŸ“¦ Parameters: {'linear_velocity': [0.3, 0.0, 0.0], 'angular_velocity': [0.0, 0.0, 0.0], 'duration': 0.5}
ğŸ“¦ Metadata: {'exploration_step': 2, 'continuous_nav': True, 'exploration_mode': 'random_walk', 'velocity_data': {'lin_vel': 0.3, 'ang_vel': 0.0, 'duration': 0.5}}
================================================================================
ğŸ§© Current agent position: [-1.5029818  1.         0.2222702]
ğŸ§© Current agent rotation (quat): quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857)
ğŸ“ prev_pos saved: [-1.5029818  1.         0.2222702]
ğŸ§® Linear velocity: [0.3, 0.0, 0.0], angular velocity: [0.0, 0.0, 0.0], dt=0.5
ğŸ“ RigidState before integrate: pos=Vector(-1.50298, 1, 0.22227), rot=Quaternion({0.0629602, 0.356092, 0.162326}, 0.918087)
ğŸ“ Predicted new_pos from VelocityControl: [-1.3989272117614746, 1.0514346361160278, 0.1272590458393097]
ğŸ§­ is_navigable(new_pos) = True
âœ… Using XZ from new_pos with preserved Y: [-1.3989272117614746, 1.0, 0.1272590458393097]
ğŸ“Œ Final applied position: [-1.3989272117614746, 1.0, 0.1272590458393097]
ğŸ“Œ Final applied rotation (quat): quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857)
ğŸ›‘ STALL CHECK: moved=0.1409m, stalled=True
ğŸ”„ METRICS: Velocity control - tracking position
ğŸ“ METRICS: Post-velocity pos = [-1.3989272   1.          0.12725905]
ğŸ“ METRICS: Trajectory len = 3
Observations from _execute_habitat_action : velocity control : {'rgba_camera': array([[[142, 125, 101, 255],
        [142, 125, 101, 255],
        [142, 125, 101, 255],
        ...,
        [183, 168, 150, 255],
        [181, 166, 148, 255],
        [179, 164, 146, 255]],

       [[142, 125, 101, 255],
        [142, 125, 101, 255],
        [142, 125, 101, 255],
        ...,
        [184, 169, 151, 255],
        [185, 170, 152, 255],
        [182, 167, 149, 255]],

       [[141, 128, 104, 255],
        [142, 125, 101, 255],
        [142, 125, 101, 255],
        ...,
        [186, 171, 153, 255],
        [185, 170, 152, 255],
        [183, 168, 150, 255]],

       ...,

       [[182, 147, 116, 255],
        [153, 121,  87, 255],
        [126,  60,  18, 255],
        ...,
        [134,  89,  60, 255],
        [134,  89,  60, 255],
        [137,  91,  63, 255]],

       [[126,  60,  18, 255],
        [126,  60,  18, 255],
        [126,  60,  18, 255],
        ...,
        [140,  97,  66, 255],
        [140,  93,  66, 255],
        [134,  89,  60, 255]],

       [[126,  60,  18, 255],
        [126,  60,  18, 255],
        [126,  60,  19, 255],
        ...,
        [140,  97,  66, 255],
        [140,  97,  66, 255],
        [145,  97,  65, 255]]], dtype=uint8), 'depth_camera': array([[0.4819661 , 0.4827191 , 0.48347443, ..., 0.5857882 , 0.5849631 ,
        0.58414024],
       [0.4823652 , 0.4831194 , 0.48387602, ..., 0.5872152 , 0.586386  ,
        0.58555925],
       [0.48276493, 0.48352042, 0.48427826, ..., 0.58865124, 0.587818  ,
        0.58698714],
       ...,
       [0.956625  , 0.9568596 , 1.5305598 , ..., 2.6624007 , 2.6666749 ,
        2.6709626 ],
       [1.5241083 , 1.5252999 , 1.5264796 , ..., 2.6507487 , 2.6549854 ,
        2.6592357 ],
       [1.5200623 , 1.5212477 , 1.522421  , ..., 2.639323  , 2.64344   ,
        2.6476114 ]], dtype=float32)}
ğŸ‘ rgba_camera shape: (640, 480, 4)
ğŸŒŠ depth_camera shape: (640, 480)
âœ… Velocity motion applied (projected)
ğŸ“¤ Result (velocity_control): status=completed, collided=True, new_position=[-1.3989272117614746, 1.0, 0.1272590458393097]
================================================================================
ğŸ“¤ Result (velocity_control):{'action_id': 'action_1766634512600_1', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[142, 125, 101, 255],
        [142, 125, 101, 255],
        [142, 125, 101, 255],
        ...,
        [183, 168, 150, 255],
        [181, 166, 148, 255],
        [179, 164, 146, 255]],

       [[142, 125, 101, 255],
        [142, 125, 101, 255],
        [142, 125, 101, 255],
        ...,
        [184, 169, 151, 255],
        [185, 170, 152, 255],
        [182, 167, 149, 255]],

       [[141, 128, 104, 255],
        [142, 125, 101, 255],
        [142, 125, 101, 255],
        ...,
        [186, 171, 153, 255],
        [185, 170, 152, 255],
        [183, 168, 150, 255]],

       ...,

       [[182, 147, 116, 255],
        [153, 121,  87, 255],
        [126,  60,  18, 255],
        ...,
        [134,  89,  60, 255],
        [134,  89,  60, 255],
        [137,  91,  63, 255]],

       [[126,  60,  18, 255],
        [126,  60,  18, 255],
        [126,  60,  18, 255],
        ...,
        [140,  97,  66, 255],
        [140,  93,  66, 255],
        [134,  89,  60, 255]],

       [[126,  60,  18, 255],
        [126,  60,  18, 255],
        [126,  60,  19, 255],
        ...,
        [140,  97,  66, 255],
        [140,  97,  66, 255],
        [145,  97,  65, 255]]], dtype=uint8), 'depth_camera': array([[0.4819661 , 0.4827191 , 0.48347443, ..., 0.5857882 , 0.5849631 ,
        0.58414024],
       [0.4823652 , 0.4831194 , 0.48387602, ..., 0.5872152 , 0.586386  ,
        0.58555925],
       [0.48276493, 0.48352042, 0.48427826, ..., 0.58865124, 0.587818  ,
        0.58698714],
       ...,
       [0.956625  , 0.9568596 , 1.5305598 , ..., 2.6624007 , 2.6666749 ,
        2.6709626 ],
       [1.5241083 , 1.5252999 , 1.5264796 , ..., 2.6507487 , 2.6549854 ,
        2.6592357 ],
       [1.5200623 , 1.5212477 , 1.522421  , ..., 2.639323  , 2.64344   ,
        2.6476114 ]], dtype=float32)}, 'collided': True, 'new_position': [-1.3989272117614746, 1.0, 0.1272590458393097], 'new_rotation': quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857), 'timestamp': 1766634513.6587594}
Action in habitat simulation result : {'action_id': 'action_1766634512600_1', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[142, 125, 101, 255],
        [142, 125, 101, 255],
        [142, 125, 101, 255],
        ...,
        [183, 168, 150, 255],
        [181, 166, 148, 255],
        [179, 164, 146, 255]],

       [[142, 125, 101, 255],
        [142, 125, 101, 255],
        [142, 125, 101, 255],
        ...,
        [184, 169, 151, 255],
        [185, 170, 152, 255],
        [182, 167, 149, 255]],

       [[141, 128, 104, 255],
        [142, 125, 101, 255],
        [142, 125, 101, 255],
        ...,
        [186, 171, 153, 255],
        [185, 170, 152, 255],
        [183, 168, 150, 255]],

       ...,

       [[182, 147, 116, 255],
        [153, 121,  87, 255],
        [126,  60,  18, 255],
        ...,
        [134,  89,  60, 255],
        [134,  89,  60, 255],
        [137,  91,  63, 255]],

       [[126,  60,  18, 255],
        [126,  60,  18, 255],
        [126,  60,  18, 255],
        ...,
        [140,  97,  66, 255],
        [140,  93,  66, 255],
        [134,  89,  60, 255]],

       [[126,  60,  18, 255],
        [126,  60,  18, 255],
        [126,  60,  19, 255],
        ...,
        [140,  97,  66, 255],
        [140,  97,  66, 255],
        [145,  97,  65, 255]]], dtype=uint8), 'depth_camera': array([[0.4819661 , 0.4827191 , 0.48347443, ..., 0.5857882 , 0.5849631 ,
        0.58414024],
       [0.4823652 , 0.4831194 , 0.48387602, ..., 0.5872152 , 0.586386  ,
        0.58555925],
       [0.48276493, 0.48352042, 0.48427826, ..., 0.58865124, 0.587818  ,
        0.58698714],
       ...,
       [0.956625  , 0.9568596 , 1.5305598 , ..., 2.6624007 , 2.6666749 ,
        2.6709626 ],
       [1.5241083 , 1.5252999 , 1.5264796 , ..., 2.6507487 , 2.6549854 ,
        2.6592357 ],
       [1.5200623 , 1.5212477 , 1.522421  , ..., 2.639323  , 2.64344   ,
        2.6476114 ]], dtype=float32)}, 'collided': True, 'new_position': [-1.3989272117614746, 1.0, 0.1272590458393097], 'new_rotation': quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857), 'timestamp': 1766634513.6587594}
âœ… Storing SINGLE result for action_1766634512600_1
Storing result using store_action_result() for action_1766634512600_1 | All stored IDs: ['action_1766634512479_0', 'action_1766634512600_1']
Stored results for id : action_1766634512600_1 : {'action_id': 'action_1766634512600_1', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[142, 125, 101, 255],
        [142, 125, 101, 255],
        [142, 125, 101, 255],
        ...,
        [183, 168, 150, 255],
        [181, 166, 148, 255],
        [179, 164, 146, 255]],

       [[142, 125, 101, 255],
        [142, 125, 101, 255],
        [142, 125, 101, 255],
        ...,
        [184, 169, 151, 255],
        [185, 170, 152, 255],
        [182, 167, 149, 255]],

       [[141, 128, 104, 255],
        [142, 125, 101, 255],
        [142, 125, 101, 255],
        ...,
        [186, 171, 153, 255],
        [185, 170, 152, 255],
        [183, 168, 150, 255]],

       ...,

       [[182, 147, 116, 255],
        [153, 121,  87, 255],
        [126,  60,  18, 255],
        ...,
        [134,  89,  60, 255],
        [134,  89,  60, 255],
        [137,  91,  63, 255]],

       [[126,  60,  18, 255],
        [126,  60,  18, 255],
        [126,  60,  18, 255],
        ...,
        [140,  97,  66, 255],
        [140,  93,  66, 255],
        [134,  89,  60, 255]],

       [[126,  60,  18, 255],
        [126,  60,  18, 255],
        [126,  60,  19, 255],
        ...,
        [140,  97,  66, 255],
        [140,  97,  66, 255],
        [145,  97,  65, 255]]], dtype=uint8), 'depth_camera': array([[0.4819661 , 0.4827191 , 0.48347443, ..., 0.5857882 , 0.5849631 ,
        0.58414024],
       [0.4823652 , 0.4831194 , 0.48387602, ..., 0.5872152 , 0.586386  ,
        0.58555925],
       [0.48276493, 0.48352042, 0.48427826, ..., 0.58865124, 0.587818  ,
        0.58698714],
       ...,
       [0.956625  , 0.9568596 , 1.5305598 , ..., 2.6624007 , 2.6666749 ,
        2.6709626 ],
       [1.5241083 , 1.5252999 , 1.5264796 , ..., 2.6507487 , 2.6549854 ,
        2.6592357 ],
       [1.5200623 , 1.5212477 , 1.522421  , ..., 2.639323  , 2.64344   ,
        2.6476114 ]], dtype=float32)}, 'collided': True, 'new_position': [-1.3989272117614746, 1.0, 0.1272590458393097], 'new_rotation': quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857), 'timestamp': 1766634513.6587594}
âœ… Action 'velocity_control' completed, result stored
ğŸ¯ HABITATSTORE: Pulling next action from queue
ğŸš€ HABITATSTORE: Pulled VELOCITY CONTROL command
   ğŸ“¦ Metadata: ['exploration_step', 'continuous_nav', 'exploration_mode', 'velocity_data']
ğŸ“Š HABITATSTORE: Remaining in queue: 2
ğŸ¯ MAINORCH: Executing action 'velocity_control'
================================================================================
ğŸ¯ Executing Habitat action: velocity_control
ğŸ“¦ Parameters: {'linear_velocity': [0.3, 0.0, 0.0], 'angular_velocity': [0.0, 0.0, 0.0], 'duration': 0.5}
ğŸ“¦ Metadata: {'exploration_step': 3, 'continuous_nav': True, 'exploration_mode': 'random_walk', 'velocity_data': {'lin_vel': 0.3, 'ang_vel': 0.0, 'duration': 0.5}}
================================================================================
ğŸ§© Current agent position: [-1.3989272   1.          0.12725905]
ğŸ§© Current agent rotation (quat): quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857)
ğŸ“ prev_pos saved: [-1.3989272   1.          0.12725905]
ğŸ§® Linear velocity: [0.3, 0.0, 0.0], angular velocity: [0.0, 0.0, 0.0], dt=0.5
ğŸ“ RigidState before integrate: pos=Vector(-1.39893, 1, 0.127259), rot=Quaternion({0.0629602, 0.356092, 0.162326}, 0.918087)
ğŸ“ Predicted new_pos from VelocityControl: [-1.2948726415634155, 1.0514346361160278, 0.032247886061668396]
ğŸ§­ is_navigable(new_pos) = True
âœ… Using XZ from new_pos with preserved Y: [-1.2948726415634155, 1.0, 0.032247886061668396]
ğŸ“Œ Final applied position: [-1.2948726415634155, 1.0, 0.032247886061668396]
ğŸ“Œ Final applied rotation (quat): quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857)
ğŸ›‘ STALL CHECK: moved=0.1409m, stalled=True
ğŸ”„ METRICS: Velocity control - tracking position
ğŸ“ METRICS: Post-velocity pos = [-1.2948726   1.          0.03224789]
ğŸ“ METRICS: Trajectory len = 4
Observations from _execute_habitat_action : velocity control : {'rgba_camera': array([[[121,  96,  63, 255],
        [122,  97,  64, 255],
        [123,  98,  65, 255],
        ...,
        [190, 175, 165, 255],
        [183, 168, 158, 255],
        [186, 170, 161, 255]],

       [[122,  97,  64, 255],
        [123,  98,  65, 255],
        [123,  98,  65, 255],
        ...,
        [191, 175, 166, 255],
        [183, 168, 158, 255],
        [185, 170, 160, 255]],

       [[122,  97,  64, 255],
        [123,  98,  65, 255],
        [123,  98,  65, 255],
        ...,
        [194, 178, 169, 255],
        [185, 169, 160, 255],
        [185, 169, 160, 255]],

       ...,

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [140,  93,  66, 255],
        [140,  93,  66, 255],
        [137,  86,  63, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [140,  93,  66, 255],
        [140,  93,  66, 255],
        [140,  93,  66, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [140,  93,  57, 255],
        [140,  93,  66, 255],
        [140,  93,  66, 255]]], dtype=uint8), 'depth_camera': array([[0.5409407 , 0.5418719 , 0.5428063 , ..., 0.5691648 , 0.5683627 ,
        0.5675628 ],
       [0.5412775 , 0.5422099 , 0.5431455 , ..., 0.5705526 , 0.56974655,
        0.56894284],
       [0.5416148 , 0.5425483 , 0.5434851 , ..., 0.57194716, 0.57113725,
        0.57032955],
       ...,
       [0.        , 0.        , 0.        , ..., 2.6623163 , 2.66659   ,
        2.6708775 ],
       [0.        , 0.        , 0.        , ..., 2.650665  , 2.6549015 ,
        2.6591516 ],
       [0.        , 0.        , 0.        , ..., 2.6391153 , 2.643315  ,
        2.647528  ]], dtype=float32)}
ğŸ‘ rgba_camera shape: (640, 480, 4)
ğŸŒŠ depth_camera shape: (640, 480)
âœ… Velocity motion applied (projected)
ğŸ“¤ Result (velocity_control): status=completed, collided=True, new_position=[-1.2948726415634155, 1.0, 0.032247886061668396]
================================================================================
ğŸ“¤ Result (velocity_control):{'action_id': 'action_1766634512722_2', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[121,  96,  63, 255],
        [122,  97,  64, 255],
        [123,  98,  65, 255],
        ...,
        [190, 175, 165, 255],
        [183, 168, 158, 255],
        [186, 170, 161, 255]],

       [[122,  97,  64, 255],
        [123,  98,  65, 255],
        [123,  98,  65, 255],
        ...,
        [191, 175, 166, 255],
        [183, 168, 158, 255],
        [185, 170, 160, 255]],

       [[122,  97,  64, 255],
        [123,  98,  65, 255],
        [123,  98,  65, 255],
        ...,
        [194, 178, 169, 255],
        [185, 169, 160, 255],
        [185, 169, 160, 255]],

       ...,

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [140,  93,  66, 255],
        [140,  93,  66, 255],
        [137,  86,  63, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [140,  93,  66, 255],
        [140,  93,  66, 255],
        [140,  93,  66, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [140,  93,  57, 255],
        [140,  93,  66, 255],
        [140,  93,  66, 255]]], dtype=uint8), 'depth_camera': array([[0.5409407 , 0.5418719 , 0.5428063 , ..., 0.5691648 , 0.5683627 ,
        0.5675628 ],
       [0.5412775 , 0.5422099 , 0.5431455 , ..., 0.5705526 , 0.56974655,
        0.56894284],
       [0.5416148 , 0.5425483 , 0.5434851 , ..., 0.57194716, 0.57113725,
        0.57032955],
       ...,
       [0.        , 0.        , 0.        , ..., 2.6623163 , 2.66659   ,
        2.6708775 ],
       [0.        , 0.        , 0.        , ..., 2.650665  , 2.6549015 ,
        2.6591516 ],
       [0.        , 0.        , 0.        , ..., 2.6391153 , 2.643315  ,
        2.647528  ]], dtype=float32)}, 'collided': True, 'new_position': [-1.2948726415634155, 1.0, 0.032247886061668396], 'new_rotation': quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857), 'timestamp': 1766634513.6988542}
Action in habitat simulation result : {'action_id': 'action_1766634512722_2', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[121,  96,  63, 255],
        [122,  97,  64, 255],
        [123,  98,  65, 255],
        ...,
        [190, 175, 165, 255],
        [183, 168, 158, 255],
        [186, 170, 161, 255]],

       [[122,  97,  64, 255],
        [123,  98,  65, 255],
        [123,  98,  65, 255],
        ...,
        [191, 175, 166, 255],
        [183, 168, 158, 255],
        [185, 170, 160, 255]],

       [[122,  97,  64, 255],
        [123,  98,  65, 255],
        [123,  98,  65, 255],
        ...,
        [194, 178, 169, 255],
        [185, 169, 160, 255],
        [185, 169, 160, 255]],

       ...,

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [140,  93,  66, 255],
        [140,  93,  66, 255],
        [137,  86,  63, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [140,  93,  66, 255],
        [140,  93,  66, 255],
        [140,  93,  66, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [140,  93,  57, 255],
        [140,  93,  66, 255],
        [140,  93,  66, 255]]], dtype=uint8), 'depth_camera': array([[0.5409407 , 0.5418719 , 0.5428063 , ..., 0.5691648 , 0.5683627 ,
        0.5675628 ],
       [0.5412775 , 0.5422099 , 0.5431455 , ..., 0.5705526 , 0.56974655,
        0.56894284],
       [0.5416148 , 0.5425483 , 0.5434851 , ..., 0.57194716, 0.57113725,
        0.57032955],
       ...,
       [0.        , 0.        , 0.        , ..., 2.6623163 , 2.66659   ,
        2.6708775 ],
       [0.        , 0.        , 0.        , ..., 2.650665  , 2.6549015 ,
        2.6591516 ],
       [0.        , 0.        , 0.        , ..., 2.6391153 , 2.643315  ,
        2.647528  ]], dtype=float32)}, 'collided': True, 'new_position': [-1.2948726415634155, 1.0, 0.032247886061668396], 'new_rotation': quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857), 'timestamp': 1766634513.6988542}
âœ… Storing SINGLE result for action_1766634512722_2
Storing result using store_action_result() for action_1766634512722_2 | All stored IDs: ['action_1766634512479_0', 'action_1766634512600_1', 'action_1766634512722_2']
Stored results for id : action_1766634512722_2 : {'action_id': 'action_1766634512722_2', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[121,  96,  63, 255],
        [122,  97,  64, 255],
        [123,  98,  65, 255],
        ...,
        [190, 175, 165, 255],
        [183, 168, 158, 255],
        [186, 170, 161, 255]],

       [[122,  97,  64, 255],
        [123,  98,  65, 255],
        [123,  98,  65, 255],
        ...,
        [191, 175, 166, 255],
        [183, 168, 158, 255],
        [185, 170, 160, 255]],

       [[122,  97,  64, 255],
        [123,  98,  65, 255],
        [123,  98,  65, 255],
        ...,
        [194, 178, 169, 255],
        [185, 169, 160, 255],
        [185, 169, 160, 255]],

       ...,

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [140,  93,  66, 255],
        [140,  93,  66, 255],
        [137,  86,  63, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [140,  93,  66, 255],
        [140,  93,  66, 255],
        [140,  93,  66, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [140,  93,  57, 255],
        [140,  93,  66, 255],
        [140,  93,  66, 255]]], dtype=uint8), 'depth_camera': array([[0.5409407 , 0.5418719 , 0.5428063 , ..., 0.5691648 , 0.5683627 ,
        0.5675628 ],
       [0.5412775 , 0.5422099 , 0.5431455 , ..., 0.5705526 , 0.56974655,
        0.56894284],
       [0.5416148 , 0.5425483 , 0.5434851 , ..., 0.57194716, 0.57113725,
        0.57032955],
       ...,
       [0.        , 0.        , 0.        , ..., 2.6623163 , 2.66659   ,
        2.6708775 ],
       [0.        , 0.        , 0.        , ..., 2.650665  , 2.6549015 ,
        2.6591516 ],
       [0.        , 0.        , 0.        , ..., 2.6391153 , 2.643315  ,
        2.647528  ]], dtype=float32)}, 'collided': True, 'new_position': [-1.2948726415634155, 1.0, 0.032247886061668396], 'new_rotation': quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857), 'timestamp': 1766634513.6988542}
âœ… Action 'velocity_control' completed, result stored
ğŸ¯ HABITATSTORE: Pulling next action from queue
ğŸš€ HABITATSTORE: Pulled VELOCITY CONTROL command
   ğŸ“¦ Metadata: ['exploration_step', 'continuous_nav', 'exploration_mode', 'velocity_data']
ğŸ“Š HABITATSTORE: Remaining in queue: 1
ğŸ¯ MAINORCH: Executing action 'velocity_control'
================================================================================
ğŸ¯ Executing Habitat action: velocity_control
ğŸ“¦ Parameters: {'linear_velocity': [0.3, 0.0, 0.0], 'angular_velocity': [0.0, 0.0, 0.0], 'duration': 0.5}
ğŸ“¦ Metadata: {'exploration_step': 4, 'continuous_nav': True, 'exploration_mode': 'random_walk', 'velocity_data': {'lin_vel': 0.3, 'ang_vel': 0.0, 'duration': 0.5}}
================================================================================
ğŸ§© Current agent position: [-1.2948726   1.          0.03224789]
ğŸ§© Current agent rotation (quat): quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857)
ğŸ“ prev_pos saved: [-1.2948726   1.          0.03224789]
ğŸ§® Linear velocity: [0.3, 0.0, 0.0], angular velocity: [0.0, 0.0, 0.0], dt=0.5
ğŸ“ RigidState before integrate: pos=Vector(-1.29487, 1, 0.0322479), rot=Quaternion({0.0629602, 0.356092, 0.162326}, 0.918087)
ğŸ“ Predicted new_pos from VelocityControl: [-1.1908180713653564, 1.0514346361160278, -0.0627632737159729]
ğŸ§­ is_navigable(new_pos) = True
âœ… Using XZ from new_pos with preserved Y: [-1.1908180713653564, 1.0, -0.0627632737159729]
ğŸ“Œ Final applied position: [-1.1908180713653564, 1.0, -0.0627632737159729]
ğŸ“Œ Final applied rotation (quat): quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857)
ğŸ›‘ STALL CHECK: moved=0.1409m, stalled=True
ğŸ”„ METRICS: Velocity control - tracking position
ğŸ“ METRICS: Post-velocity pos = [-1.1908181   1.         -0.06276327]
ğŸ“ METRICS: Trajectory len = 5
Observations from _execute_habitat_action : velocity control : {'rgba_camera': array([[[126, 100,  68, 255],
        [125,  99,  67, 255],
        [123,  98,  65, 255],
        ...,
        [192, 184, 183, 255],
        [191, 183, 182, 255],
        [192, 184, 183, 255]],

       [[126, 100,  68, 255],
        [124,  99,  66, 255],
        [123,  98,  65, 255],
        ...,
        [194, 186, 185, 255],
        [192, 184, 183, 255],
        [192, 184, 183, 255]],

       [[126, 100,  65, 255],
        [124,  98,  66, 255],
        [123,  98,  65, 255],
        ...,
        [194, 186, 185, 255],
        [192, 184, 183, 255],
        [190, 182, 181, 255]],

       ...,

       [[176, 177, 178, 255],
        [182, 184, 183, 255],
        [108, 106, 108, 255],
        ...,
        [127, 127, 122, 255],
        [161, 163, 163, 255],
        [172, 173, 174, 255]],

       [[183, 184, 183, 255],
        [183, 184, 183, 255],
        [109, 107, 110, 255],
        ...,
        [126, 125, 121, 255],
        [160, 163, 163, 255],
        [172, 174, 175, 255]],

       [[183, 184, 183, 255],
        [107, 105, 108, 255],
        [145, 144, 145, 255],
        ...,
        [158, 160, 161, 255],
        [172, 174, 175, 255],
        [182, 184, 185, 255]]], dtype=uint8), 'depth_camera': array([[0.60265106, 0.6036746 , 0.6047037 , ..., 0.5525401 , 0.5517624 ,
        0.5509868 ],
       [0.6030258 , 0.6040506 , 0.605081  , ..., 0.5538882 , 0.55310667,
        0.55232733],
       [0.6034032 , 0.60442924, 0.605461  , ..., 0.555241  , 0.55445564,
        0.5536725 ],
       ...,
       [0.22105318, 0.22462662, 0.22794585, ..., 1.2657591 , 1.2592231 ,
        1.2527635 ],
       [0.22296771, 0.22623715, 0.2296045 , ..., 1.2634904 , 1.2569777 ,
        1.2505411 ],
       [0.22455446, 0.22787124, 0.23128778, ..., 1.2612205 , 1.2547309 ,
        1.2483172 ]], dtype=float32)}
ğŸ‘ rgba_camera shape: (640, 480, 4)
ğŸŒŠ depth_camera shape: (640, 480)
âœ… Velocity motion applied (projected)
ğŸ“¤ Result (velocity_control): status=completed, collided=True, new_position=[-1.1908180713653564, 1.0, -0.0627632737159729]
================================================================================
ğŸ“¤ Result (velocity_control):{'action_id': 'action_1766634512845_3', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[126, 100,  68, 255],
        [125,  99,  67, 255],
        [123,  98,  65, 255],
        ...,
        [192, 184, 183, 255],
        [191, 183, 182, 255],
        [192, 184, 183, 255]],

       [[126, 100,  68, 255],
        [124,  99,  66, 255],
        [123,  98,  65, 255],
        ...,
        [194, 186, 185, 255],
        [192, 184, 183, 255],
        [192, 184, 183, 255]],

       [[126, 100,  65, 255],
        [124,  98,  66, 255],
        [123,  98,  65, 255],
        ...,
        [194, 186, 185, 255],
        [192, 184, 183, 255],
        [190, 182, 181, 255]],

       ...,

       [[176, 177, 178, 255],
        [182, 184, 183, 255],
        [108, 106, 108, 255],
        ...,
        [127, 127, 122, 255],
        [161, 163, 163, 255],
        [172, 173, 174, 255]],

       [[183, 184, 183, 255],
        [183, 184, 183, 255],
        [109, 107, 110, 255],
        ...,
        [126, 125, 121, 255],
        [160, 163, 163, 255],
        [172, 174, 175, 255]],

       [[183, 184, 183, 255],
        [107, 105, 108, 255],
        [145, 144, 145, 255],
        ...,
        [158, 160, 161, 255],
        [172, 174, 175, 255],
        [182, 184, 185, 255]]], dtype=uint8), 'depth_camera': array([[0.60265106, 0.6036746 , 0.6047037 , ..., 0.5525401 , 0.5517624 ,
        0.5509868 ],
       [0.6030258 , 0.6040506 , 0.605081  , ..., 0.5538882 , 0.55310667,
        0.55232733],
       [0.6034032 , 0.60442924, 0.605461  , ..., 0.555241  , 0.55445564,
        0.5536725 ],
       ...,
       [0.22105318, 0.22462662, 0.22794585, ..., 1.2657591 , 1.2592231 ,
        1.2527635 ],
       [0.22296771, 0.22623715, 0.2296045 , ..., 1.2634904 , 1.2569777 ,
        1.2505411 ],
       [0.22455446, 0.22787124, 0.23128778, ..., 1.2612205 , 1.2547309 ,
        1.2483172 ]], dtype=float32)}, 'collided': True, 'new_position': [-1.1908180713653564, 1.0, -0.0627632737159729], 'new_rotation': quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857), 'timestamp': 1766634513.739171}
Action in habitat simulation result : {'action_id': 'action_1766634512845_3', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[126, 100,  68, 255],
        [125,  99,  67, 255],
        [123,  98,  65, 255],
        ...,
        [192, 184, 183, 255],
        [191, 183, 182, 255],
        [192, 184, 183, 255]],

       [[126, 100,  68, 255],
        [124,  99,  66, 255],
        [123,  98,  65, 255],
        ...,
        [194, 186, 185, 255],
        [192, 184, 183, 255],
        [192, 184, 183, 255]],

       [[126, 100,  65, 255],
        [124,  98,  66, 255],
        [123,  98,  65, 255],
        ...,
        [194, 186, 185, 255],
        [192, 184, 183, 255],
        [190, 182, 181, 255]],

       ...,

       [[176, 177, 178, 255],
        [182, 184, 183, 255],
        [108, 106, 108, 255],
        ...,
        [127, 127, 122, 255],
        [161, 163, 163, 255],
        [172, 173, 174, 255]],

       [[183, 184, 183, 255],
        [183, 184, 183, 255],
        [109, 107, 110, 255],
        ...,
        [126, 125, 121, 255],
        [160, 163, 163, 255],
        [172, 174, 175, 255]],

       [[183, 184, 183, 255],
        [107, 105, 108, 255],
        [145, 144, 145, 255],
        ...,
        [158, 160, 161, 255],
        [172, 174, 175, 255],
        [182, 184, 185, 255]]], dtype=uint8), 'depth_camera': array([[0.60265106, 0.6036746 , 0.6047037 , ..., 0.5525401 , 0.5517624 ,
        0.5509868 ],
       [0.6030258 , 0.6040506 , 0.605081  , ..., 0.5538882 , 0.55310667,
        0.55232733],
       [0.6034032 , 0.60442924, 0.605461  , ..., 0.555241  , 0.55445564,
        0.5536725 ],
       ...,
       [0.22105318, 0.22462662, 0.22794585, ..., 1.2657591 , 1.2592231 ,
        1.2527635 ],
       [0.22296771, 0.22623715, 0.2296045 , ..., 1.2634904 , 1.2569777 ,
        1.2505411 ],
       [0.22455446, 0.22787124, 0.23128778, ..., 1.2612205 , 1.2547309 ,
        1.2483172 ]], dtype=float32)}, 'collided': True, 'new_position': [-1.1908180713653564, 1.0, -0.0627632737159729], 'new_rotation': quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857), 'timestamp': 1766634513.739171}
âœ… Storing SINGLE result for action_1766634512845_3
Storing result using store_action_result() for action_1766634512845_3 | All stored IDs: ['action_1766634512479_0', 'action_1766634512600_1', 'action_1766634512722_2', 'action_1766634512845_3']
Stored results for id : action_1766634512845_3 : {'action_id': 'action_1766634512845_3', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[126, 100,  68, 255],
        [125,  99,  67, 255],
        [123,  98,  65, 255],
        ...,
        [192, 184, 183, 255],
        [191, 183, 182, 255],
        [192, 184, 183, 255]],

       [[126, 100,  68, 255],
        [124,  99,  66, 255],
        [123,  98,  65, 255],
        ...,
        [194, 186, 185, 255],
        [192, 184, 183, 255],
        [192, 184, 183, 255]],

       [[126, 100,  65, 255],
        [124,  98,  66, 255],
        [123,  98,  65, 255],
        ...,
        [194, 186, 185, 255],
        [192, 184, 183, 255],
        [190, 182, 181, 255]],

       ...,

       [[176, 177, 178, 255],
        [182, 184, 183, 255],
        [108, 106, 108, 255],
        ...,
        [127, 127, 122, 255],
        [161, 163, 163, 255],
        [172, 173, 174, 255]],

       [[183, 184, 183, 255],
        [183, 184, 183, 255],
        [109, 107, 110, 255],
        ...,
        [126, 125, 121, 255],
        [160, 163, 163, 255],
        [172, 174, 175, 255]],

       [[183, 184, 183, 255],
        [107, 105, 108, 255],
        [145, 144, 145, 255],
        ...,
        [158, 160, 161, 255],
        [172, 174, 175, 255],
        [182, 184, 185, 255]]], dtype=uint8), 'depth_camera': array([[0.60265106, 0.6036746 , 0.6047037 , ..., 0.5525401 , 0.5517624 ,
        0.5509868 ],
       [0.6030258 , 0.6040506 , 0.605081  , ..., 0.5538882 , 0.55310667,
        0.55232733],
       [0.6034032 , 0.60442924, 0.605461  , ..., 0.555241  , 0.55445564,
        0.5536725 ],
       ...,
       [0.22105318, 0.22462662, 0.22794585, ..., 1.2657591 , 1.2592231 ,
        1.2527635 ],
       [0.22296771, 0.22623715, 0.2296045 , ..., 1.2634904 , 1.2569777 ,
        1.2505411 ],
       [0.22455446, 0.22787124, 0.23128778, ..., 1.2612205 , 1.2547309 ,
        1.2483172 ]], dtype=float32)}, 'collided': True, 'new_position': [-1.1908180713653564, 1.0, -0.0627632737159729], 'new_rotation': quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857), 'timestamp': 1766634513.739171}
âœ… Action 'velocity_control' completed, result stored
ğŸ¯ HABITATSTORE: Pulling next action from queue
ğŸš€ HABITATSTORE: Pulled VELOCITY CONTROL command
   ğŸ“¦ Metadata: ['exploration_step', 'continuous_nav', 'exploration_mode', 'velocity_data']
ğŸ“Š HABITATSTORE: Remaining in queue: 0
ğŸ¯ MAINORCH: Executing action 'velocity_control'
================================================================================
ğŸ¯ Executing Habitat action: velocity_control
ğŸ“¦ Parameters: {'linear_velocity': [0.0, 0.0, 0.0], 'angular_velocity': [0.0, 0.0, 0.7], 'duration': 0.5}
ğŸ“¦ Metadata: {'exploration_step': 5, 'continuous_nav': True, 'exploration_mode': 'random_turn_left', 'velocity_data': {'lin_vel': 0.0, 'ang_vel': 0.7, 'duration': 0.5}}
================================================================================
ğŸ§© Current agent position: [-1.1908181   1.         -0.06276327]
ğŸ§© Current agent rotation (quat): quaternion(0.918087482452393, 0.062960185110569, 0.356092303991318, 0.162325769662857)
ğŸ“ prev_pos saved: [-1.1908181   1.         -0.06276327]
ğŸ§® Linear velocity: [0.0, 0.0, 0.0], angular velocity: [0.0, 0.0, 0.7], dt=0.5
ğŸ“ RigidState before integrate: pos=Vector(-1.19082, 1, -0.0627633), rot=Quaternion({0.0629602, 0.356092, 0.162326}, 0.918087)
ğŸ“ Predicted new_pos from VelocityControl: [-1.1908180713653564, 1.0, -0.0627632737159729]
ğŸ§­ is_navigable(new_pos) = True
âœ… Using XZ from new_pos with preserved Y: [-1.1908180713653564, 1.0, -0.0627632737159729]
ğŸ“Œ Final applied position: [-1.1908180713653564, 1.0, -0.0627632737159729]
ğŸ“Œ Final applied rotation (quat): quaternion(0.875802874565125, 0.123997122049332, 0.339691668748856, 0.319692999124527)
ğŸ›‘ STALL CHECK: moved=0.0000m, stalled=True
ğŸ”„ METRICS: Velocity control - tracking position
ğŸ“ METRICS: Post-velocity pos = [-1.1908181   1.         -0.06276327]
ğŸ“ METRICS: Trajectory len = 6
Observations from _execute_habitat_action : velocity control : {'rgba_camera': array([[[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [160, 135, 102, 255],
        [164, 137, 106, 255],
        [149, 126,  91, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [160, 135, 102, 255],
        [164, 137, 106, 255],
        [153, 129,  95, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [160, 135, 102, 255],
        [164, 137, 106, 255],
        [159, 133, 101, 255]],

       ...,

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [144, 137, 127, 255],
        [145, 138, 128, 255],
        [145, 138, 128, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [142, 126, 117, 255],
        [143, 137, 126, 255],
        [144, 137, 127, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [139, 124, 114, 255],
        [140, 127, 117, 255],
        [144, 137, 127, 255]]], dtype=uint8), 'depth_camera': array([[0.        , 0.        , 0.        , ..., 0.79891545, 0.7971541 ,
        0.79540056],
       [0.        , 0.        , 0.        , ..., 0.8003904 , 0.7986226 ,
        0.7968626 ],
       [0.        , 0.        , 0.        , ..., 0.80187476, 0.8001003 ,
        0.7983338 ],
       ...,
       [0.        , 0.        , 0.        , ..., 2.8187337 , 2.8156116 ,
        2.8124964 ],
       [0.        , 0.        , 0.        , ..., 2.8157532 , 2.8126378 ,
        2.809529  ],
       [0.        , 0.        , 0.        , ..., 2.8128264 , 2.8097174 ,
        2.806615  ]], dtype=float32)}
ğŸ‘ rgba_camera shape: (640, 480, 4)
ğŸŒŠ depth_camera shape: (640, 480)
âœ… Velocity motion applied (projected)
ğŸ“¤ Result (velocity_control): status=completed, collided=True, new_position=[-1.1908180713653564, 1.0, -0.0627632737159729]
================================================================================
ğŸ“¤ Result (velocity_control):{'action_id': 'action_1766634512966_4', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [160, 135, 102, 255],
        [164, 137, 106, 255],
        [149, 126,  91, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [160, 135, 102, 255],
        [164, 137, 106, 255],
        [153, 129,  95, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [160, 135, 102, 255],
        [164, 137, 106, 255],
        [159, 133, 101, 255]],

       ...,

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [144, 137, 127, 255],
        [145, 138, 128, 255],
        [145, 138, 128, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [142, 126, 117, 255],
        [143, 137, 126, 255],
        [144, 137, 127, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [139, 124, 114, 255],
        [140, 127, 117, 255],
        [144, 137, 127, 255]]], dtype=uint8), 'depth_camera': array([[0.        , 0.        , 0.        , ..., 0.79891545, 0.7971541 ,
        0.79540056],
       [0.        , 0.        , 0.        , ..., 0.8003904 , 0.7986226 ,
        0.7968626 ],
       [0.        , 0.        , 0.        , ..., 0.80187476, 0.8001003 ,
        0.7983338 ],
       ...,
       [0.        , 0.        , 0.        , ..., 2.8187337 , 2.8156116 ,
        2.8124964 ],
       [0.        , 0.        , 0.        , ..., 2.8157532 , 2.8126378 ,
        2.809529  ],
       [0.        , 0.        , 0.        , ..., 2.8128264 , 2.8097174 ,
        2.806615  ]], dtype=float32)}, 'collided': True, 'new_position': [-1.1908180713653564, 1.0, -0.0627632737159729], 'new_rotation': quaternion(0.875802874565125, 0.123997122049332, 0.339691668748856, 0.319692999124527), 'timestamp': 1766634513.7847874}
Action in habitat simulation result : {'action_id': 'action_1766634512966_4', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [160, 135, 102, 255],
        [164, 137, 106, 255],
        [149, 126,  91, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [160, 135, 102, 255],
        [164, 137, 106, 255],
        [153, 129,  95, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [160, 135, 102, 255],
        [164, 137, 106, 255],
        [159, 133, 101, 255]],

       ...,

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [144, 137, 127, 255],
        [145, 138, 128, 255],
        [145, 138, 128, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [142, 126, 117, 255],
        [143, 137, 126, 255],
        [144, 137, 127, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [139, 124, 114, 255],
        [140, 127, 117, 255],
        [144, 137, 127, 255]]], dtype=uint8), 'depth_camera': array([[0.        , 0.        , 0.        , ..., 0.79891545, 0.7971541 ,
        0.79540056],
       [0.        , 0.        , 0.        , ..., 0.8003904 , 0.7986226 ,
        0.7968626 ],
       [0.        , 0.        , 0.        , ..., 0.80187476, 0.8001003 ,
        0.7983338 ],
       ...,
       [0.        , 0.        , 0.        , ..., 2.8187337 , 2.8156116 ,
        2.8124964 ],
       [0.        , 0.        , 0.        , ..., 2.8157532 , 2.8126378 ,
        2.809529  ],
       [0.        , 0.        , 0.        , ..., 2.8128264 , 2.8097174 ,
        2.806615  ]], dtype=float32)}, 'collided': True, 'new_position': [-1.1908180713653564, 1.0, -0.0627632737159729], 'new_rotation': quaternion(0.875802874565125, 0.123997122049332, 0.339691668748856, 0.319692999124527), 'timestamp': 1766634513.7847874}
âœ… Storing SINGLE result for action_1766634512966_4
Storing result using store_action_result() for action_1766634512966_4 | All stored IDs: ['action_1766634512479_0', 'action_1766634512600_1', 'action_1766634512722_2', 'action_1766634512845_3', 'action_1766634512966_4']
Stored results for id : action_1766634512966_4 : {'action_id': 'action_1766634512966_4', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [160, 135, 102, 255],
        [164, 137, 106, 255],
        [149, 126,  91, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [160, 135, 102, 255],
        [164, 137, 106, 255],
        [153, 129,  95, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [160, 135, 102, 255],
        [164, 137, 106, 255],
        [159, 133, 101, 255]],

       ...,

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [144, 137, 127, 255],
        [145, 138, 128, 255],
        [145, 138, 128, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [142, 126, 117, 255],
        [143, 137, 126, 255],
        [144, 137, 127, 255]],

       [[  0,   0,   0, 255],
        [  0,   0,   0, 255],
        [  0,   0,   0, 255],
        ...,
        [139, 124, 114, 255],
        [140, 127, 117, 255],
        [144, 137, 127, 255]]], dtype=uint8), 'depth_camera': array([[0.        , 0.        , 0.        , ..., 0.79891545, 0.7971541 ,
        0.79540056],
       [0.        , 0.        , 0.        , ..., 0.8003904 , 0.7986226 ,
        0.7968626 ],
       [0.        , 0.        , 0.        , ..., 0.80187476, 0.8001003 ,
        0.7983338 ],
       ...,
       [0.        , 0.        , 0.        , ..., 2.8187337 , 2.8156116 ,
        2.8124964 ],
       [0.        , 0.        , 0.        , ..., 2.8157532 , 2.8126378 ,
        2.809529  ],
       [0.        , 0.        , 0.        , ..., 2.8128264 , 2.8097174 ,
        2.806615  ]], dtype=float32)}, 'collided': True, 'new_position': [-1.1908180713653564, 1.0, -0.0627632737159729], 'new_rotation': quaternion(0.875802874565125, 0.123997122049332, 0.339691668748856, 0.319692999124527), 'timestamp': 1766634513.7847874}
âœ… Action 'velocity_control' completed, result stored
ğŸ¯ HABITATSTORE: Pulling next action from queue
ğŸ“­ HABITATSTORE: Action queue is empty
ğŸ”® Running prediction cycle for frame 0...
ğŸ”® DEBUG: Starting prediction cycle for frame 0
ğŸ”® Prediction Cycle 0: Starting predictors...
ğŸ” DEBUG: Checking TaskStore.current_action_plan...
ğŸ” DEBUG: TaskStore has current_action_plan: True
âœ… Found task_id in task_context: reason_1766634510284
ğŸ“Š PredictionStore: Created entry pred_reason_1766634510284_1766634513785
âœ… Created prediction entry: pred_reason_1766634510284_1766634513785
ğŸ”® DEBUG: Prediction ID = pred_reason_1766634510284_1766634513785
ğŸ”® DEBUG: Calling collision predictor...
ğŸš§ CollisionRiskPredictor: Starting prediction...
  ğŸ“ Frame start time: 03:48:33
ğŸš§ CollisionRiskPredictor: Step 1 - Getting prediction ID...
ğŸš§ CollisionRiskPredictor: Prediction ID = pred_reason_1766634510284_1766634513785
ğŸš§ CollisionRiskPredictor: Step 2 - Fetching data from stores...
  ğŸ“ Robot position: [0.0, 0.0, 0.0]
  ğŸ“Š Semantic objects: 0 found
  ğŸ” Extracting ORB-SLAM map points...
  ğŸ“Š ORB-SLAM points: 0 points
  ğŸ¯ Planned actions: ['move_forward', 'turn_left', 'turn_right']
ğŸš§ CollisionRiskPredictor: Step 3 - Processing risks...
  ğŸ“ Computing immediate geometric risks...
    Immediate danger: False
    Closest obstacle: infm
    Risk score: 0.00
  ğŸ·ï¸ Computing semantic object risks...
    High risk zones: 0
    Medium risk zones: 0
  ğŸ¯ Calculating action-specific risks...
    Analyzing action: move_forward
      Base risk: 0.00
      Front risks: 0 â†’ Action risk: 0.00
      Final: risk_level=low, feasible=True, speed=normal
    Analyzing action: turn_left
      Base risk: 0.00
      Right risks: 0 â†’ Action risk: 0.00
      Final: risk_level=low, feasible=True, speed=normal
    Analyzing action: turn_right
      Base risk: 0.00
      Left risks: 0 â†’ Action risk: 0.00
      Final: risk_level=low, feasible=True, speed=normal
ğŸš§ CollisionRiskPredictor: Step 4 - Generating overall assessment...
  ğŸ“Š All risk scores: ['0.00', '0.00', '0.00']
  ğŸ“ˆ Max collision risk: 0.00
  ğŸ›¡ï¸ Safe navigation confidence: 1.00
  ğŸ¯ Overall risk level: low
ğŸš§ CollisionRiskPredictor: Step 5 - Building result...
ğŸš§ CollisionRiskPredictor: Step 6 - Saving to prediction store...
ğŸ“Š PredictionStore: Updated pred_reason_1766634510284_1766634513785 with collision_risk results
ğŸš§ CollisionRiskPredictor: âœ… Results stored successfully
ğŸ“Š PredictionStore: Updated pred_reason_1766634510284_1766634513785 with collision_risk results
âœ… CollisionRiskPredictor: Results stored in PredictionStore
ğŸš§ CollisionRiskPredictor: Prediction completed in 0.1ms

ğŸ“‹ PREDICTION SUMMARY:
   Prediction ID: pred_reason_1766634510284_1766634513785
   Overall risk: low
   Actions analyzed: ['move_forward', 'turn_left', 'turn_right']
   Processing time: 0.1ms
ğŸ”® DEBUG: Collision result type = <class 'dict'>
ğŸ”® DEBUG: Calling I-JEPA predictor...
ğŸ”® IjepaPredictor: Called with prediction_id=pred_reason_1766634510284_1766634513785
ğŸ”® IjepaPredictor: _execute_prediction_logic started
ğŸ”® DEBUG: prediction_id = pred_reason_1766634510284_1766634513785
ğŸ”® DEBUG: self.model available = True
ğŸ”® DEBUG: self.processor available = True
ğŸ”® DEBUG: prototypes loaded = 1
ğŸ”® IjepaPredictor: Getting frame from buffer
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'

ğŸŸ¢ OWL DEBUG â”€â”€â”€ START FRAME 0 â”€â”€â”€
ğŸ“¦ Frame metadata:
   â€¢ frame_id      = 0
   â€¢ timestamp     = 0.0
   â€¢ rgb shape     = (480, 640, 3)
   â€¢ depth shape   = (480, 640)
ğŸ¯ Detection input: shape=(480, 640, 3), dtype=uint8
ğŸ” FRAME STATS:
   - Min pixel value: 0
   - Max pixel value: 255
   - Mean brightness: 150.9
   - Brightness std: 41.4
ğŸ¦‰ OWL DEBUG: Starting _get_text_queries_from_task_store
ğŸ¦‰ OWL DEBUG: self.task_store exists = True
ğŸ¯ OWL HABITATSTORE: Using stored goal 'tv'
ğŸ” CURRENT QUERIES DEBUG:
   - Query count: 1
   - First 3 queries: ['tv']
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 0.52s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” INPUTS DEBUG:
   - input_ids shape: torch.Size([1, 16])
   - attention_mask shape: torch.Size([1, 16])
   - pixel_values shape: torch.Size([1, 3, 768, 768])
   - pixel_values dtype: torch.float32
   - pixel_values min/max: -1.792/2.146
ğŸ” OWL DEBUG: Inputs keys: ['input_ids', 'attention_mask', 'pixel_values']
ğŸ” OWL DEBUG: Text queries: ['tv']
ğŸ” DEBUG: stack[1].function = get_latest_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/stores/frame_buffer.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='get_latest_frame [frame_buffer.py]', Model='unknown'
ğŸ”® DEBUG: frame_data = True
ğŸ”® IjepaPredictor: Got frame 0
ğŸ”® DEBUG: metadata keys = dict_keys(['frame_id', 'timestamp', 'slot', 'buffer_size', 'rgb_shape', 'depth_shape', 'is_valid', 'is_copy', 'processed_by'])
ğŸ”® DEBUG: last_processed_frame_id = -1, frame_skip = 8
ğŸ”® DEBUG: Processing frame 0
ğŸ”® IjepaPredictor: Frame is dict, extracting numpy array
ğŸ”® DEBUG: dict keys = dict_keys(['rgb', 'depth'])
ğŸ”® DEBUG: Using 'rgb' key, shape = (480, 640, 3)
ğŸ”® IjepaPredictor: Frame shape: (480, 640, 3)
ğŸ”® DEBUG: Frame dtype = uint8, min = 0, max = 255
ğŸ”® DEBUG: Calling extract_structural_embedding()
ğŸ”® I-JEPA: Processing (480, 640, 3) â†’ PIL (640, 480)
ğŸ”® I-JEPA: Input tensor torch.Size([1, 3, 224, 224])
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 1.06s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ”® I-JEPA: Hidden state torch.Size([1, 256, 1280])
ğŸ”® I-JEPA: âœ… Embedding torch.Size([1, 1280])
ğŸ”® DEBUG: Embedding returned = True
ğŸ”® IjepaPredictor: Embedding shape: torch.Size([1, 1280])
ğŸ”® DEBUG: Embedding device = cuda:0
ğŸ”® DEBUG: Embedding dtype = torch.float32
ğŸ”® DEBUG: Raw embedding numpy shape = (1280,)
ğŸ”® DEBUG: Stored embedding for frame 0
ğŸ”® IjepaPredictor: Analyzing current structure with prototypes
============================================================
ğŸ”® _analyze_current_structure CALLED
ğŸ”® embedding_tensor shape = torch.Size([1, 1280])
ğŸ”® embedding_tensor device = cuda:0
ğŸ”® embedding_tensor dtype = torch.float32
============================================================
ğŸ”® _compute_similarities CALLED
ğŸ”® embedding_tensor initial shape = torch.Size([1, 1280])
ğŸ”® embedding_tensor device = cuda:0
----------------------------------------
ğŸ”® Prototype name = free_space
ğŸ”® Prototype original type = <class 'torch.Tensor'>
ğŸ”® Prototype shape = torch.Size([1, 1280])
ğŸ”® Embedding shape = torch.Size([1, 1280])
ğŸ”® similarity = 0.4829
ğŸ”® distance = 0.5171
ğŸ”® best_match = False
ğŸ”® FINAL similarities dict:
{'free_space': {'similarity': 0.4828524887561798, 'distance': 0.5171475112438202, 'best_match': False}}
ğŸ”® Raw similarities:
{'free_space': {'similarity': 0.4828524887561798, 'distance': 0.5171475112438202, 'best_match': False}}
ğŸ”® Extracted similarity_scores:
{'free_space': 0.4828524887561798}
ğŸ”® _analyze_current_structure RESULT â†“â†“â†“
{'type': 'free_space', 'confidence': 0.4828524887561798, 'properties': {'free_space': 0.4828524887561798}}
ğŸ”® _analyze_current_structure RESULT â†‘â†‘â†‘
ğŸ”® IjepaPredictor: Structure type = free_space
ğŸ”® IjepaPredictor: Confidence = 0.483
ğŸ”® DEBUG: Structure properties = ['free_space']
ğŸ”® DEBUG: free_space: 0.483
ğŸ”® DEBUG: Task store available, getting planned actions
ğŸ”® DEBUG: Could not get planned actions: 'TaskStore' object has no attribute 'get_current_task'
ğŸ”® DEBUG: Using actions = ['move_forward', 'turn_left', 'turn_right']
ğŸ”® IjepaPredictor: Analyzing 3 actions
ğŸ”® IjepaPredictor: Analyzing action: move_forward
============================================================
ğŸ”® _analyze_action_continuity CALLED
ğŸ”® action = move_forward
ğŸ”® current_structure = {'type': 'free_space', 'confidence': 0.4828524887561798, 'properties': {'free_space': 0.4828524887561798}}
ğŸ”® map_context = None
ğŸ”® current_embedding shape = torch.Size([1, 1280])
ğŸ”® current_embedding device = cuda:0
ğŸ”® current_embedding dtype = torch.float32
============================================================
ğŸ”® _compute_similarities CALLED
ğŸ”® embedding_tensor initial shape = torch.Size([1, 1280])
ğŸ”® embedding_tensor device = cuda:0
----------------------------------------
ğŸ”® Prototype name = free_space
ğŸ”® Prototype original type = <class 'torch.Tensor'>
ğŸ”® Prototype shape = torch.Size([1, 1280])
ğŸ”® Embedding shape = torch.Size([1, 1280])
ğŸ”® similarity = 0.4829
ğŸ”® distance = 0.5171
ğŸ”® best_match = False
ğŸ”® FINAL similarities dict:
{'free_space': {'similarity': 0.4828524887561798, 'distance': 0.5171475112438202, 'best_match': False}}
ğŸ”® Raw similarities returned:
{'free_space': {'similarity': 0.4828524887561798, 'distance': 0.5171475112438202, 'best_match': False}}
ğŸ”® Extracted similarity_scores:
{'free_space': 0.4828524887561798}
ğŸ”® continuity_score (from similarities) = 0.2414262443780899
ğŸ”® No map_context â†’ default map_confidence = 0.5
ğŸ”® _analyze_action_continuity RESULT â†“â†“â†“
{'action': 'move_forward', 'continuity_score': 0.2414262443780899, 'map_context_confidence': 0.5, 'structural_confidence': 0.4828524887561798, 'embedding_stability': 0.5, 'continuity_type': 'structural_break', 'similarity_scores': {'free_space': 0.4828524887561798}}
ğŸ”® _analyze_action_continuity RESULT â†‘â†‘â†‘
[DEBUG] Assessing risk: continuity=0.241, stability=0.500
[DEBUG] Prototype stats: avg=0.483, max=0.483
[DEBUG] Risk components: continuity=0.379, prototype=0.155
[DEBUG] Final risk: score=0.534, level=medium
ğŸ”® DEBUG: move_forward continuity = 0.241, risk = medium
ğŸ”® IjepaPredictor: Analyzing action: turn_left
============================================================
ğŸ”® _analyze_action_continuity CALLED
ğŸ”® action = turn_left
ğŸ”® current_structure = {'type': 'free_space', 'confidence': 0.4828524887561798, 'properties': {'free_space': 0.4828524887561798}}
ğŸ”® map_context = None
ğŸ”® current_embedding shape = torch.Size([1, 1280])
ğŸ”® current_embedding device = cuda:0
ğŸ”® current_embedding dtype = torch.float32
============================================================
ğŸ”® _compute_similarities CALLED
ğŸ”® embedding_tensor initial shape = torch.Size([1, 1280])
ğŸ”® embedding_tensor device = cuda:0
----------------------------------------
ğŸ”® Prototype name = free_space
ğŸ”® Prototype original type = <class 'torch.Tensor'>
ğŸ”® Prototype shape = torch.Size([1, 1280])
ğŸ”® Embedding shape = torch.Size([1, 1280])
ğŸ”® similarity = 0.4829
ğŸ”® distance = 0.5171
ğŸ”® best_match = False
ğŸ”® FINAL similarities dict:
{'free_space': {'similarity': 0.4828524887561798, 'distance': 0.5171475112438202, 'best_match': False}}
ğŸ”® Raw similarities returned:
{'free_space': {'similarity': 0.4828524887561798, 'distance': 0.5171475112438202, 'best_match': False}}
ğŸ”® Extracted similarity_scores:
{'free_space': 0.4828524887561798}
ğŸ”® continuity_score (from similarities) = 0.14485574662685394
ğŸ”® No map_context â†’ default map_confidence = 0.5
ğŸ”® _analyze_action_continuity RESULT â†“â†“â†“
{'action': 'turn_left', 'continuity_score': 0.14485574662685394, 'map_context_confidence': 0.5, 'structural_confidence': 0.4828524887561798, 'embedding_stability': 0.5, 'continuity_type': 'structural_break', 'similarity_scores': {'free_space': 0.4828524887561798}}
ğŸ”® _analyze_action_continuity RESULT â†‘â†‘â†‘
[DEBUG] Assessing risk: continuity=0.145, stability=0.500
[DEBUG] Prototype stats: avg=0.483, max=0.483
[DEBUG] Risk components: continuity=0.428, prototype=0.155
[DEBUG] Final risk: score=0.583, level=medium
ğŸ”® DEBUG: turn_left continuity = 0.145, risk = medium
ğŸ”® IjepaPredictor: Analyzing action: turn_right
============================================================
ğŸ”® _analyze_action_continuity CALLED
ğŸ”® action = turn_right
ğŸ”® current_structure = {'type': 'free_space', 'confidence': 0.4828524887561798, 'properties': {'free_space': 0.4828524887561798}}
ğŸ”® map_context = None
ğŸ”® current_embedding shape = torch.Size([1, 1280])
ğŸ”® current_embedding device = cuda:0
ğŸ”® current_embedding dtype = torch.float32
============================================================
ğŸ”® _compute_similarities CALLED
ğŸ”® embedding_tensor initial shape = torch.Size([1, 1280])
ğŸ”® embedding_tensor device = cuda:0
----------------------------------------
ğŸ”® Prototype name = free_space
ğŸ”® Prototype original type = <class 'torch.Tensor'>
ğŸ”® Prototype shape = torch.Size([1, 1280])
ğŸ”® Embedding shape = torch.Size([1, 1280])
ğŸ”® similarity = 0.4829
ğŸ”® distance = 0.5171
ğŸ”® best_match = False
ğŸ”® FINAL similarities dict:
{'free_space': {'similarity': 0.4828524887561798, 'distance': 0.5171475112438202, 'best_match': False}}
ğŸ”® Raw similarities returned:
{'free_space': {'similarity': 0.4828524887561798, 'distance': 0.5171475112438202, 'best_match': False}}
ğŸ”® Extracted similarity_scores:
{'free_space': 0.4828524887561798}
ğŸ”® continuity_score (from similarities) = 0.14485574662685394
ğŸ”® No map_context â†’ default map_confidence = 0.5
ğŸ”® _analyze_action_continuity RESULT â†“â†“â†“
{'action': 'turn_right', 'continuity_score': 0.14485574662685394, 'map_context_confidence': 0.5, 'structural_confidence': 0.4828524887561798, 'embedding_stability': 0.5, 'continuity_type': 'structural_break', 'similarity_scores': {'free_space': 0.4828524887561798}}
ğŸ”® _analyze_action_continuity RESULT â†‘â†‘â†‘
[DEBUG] Assessing risk: continuity=0.145, stability=0.500
[DEBUG] Prototype stats: avg=0.483, max=0.483
[DEBUG] Risk components: continuity=0.428, prototype=0.155
[DEBUG] Final risk: score=0.583, level=medium
ğŸ”® DEBUG: turn_right continuity = 0.145, risk = medium
ğŸ”® IjepaPredictor: Building result dictionary
ğŸ”® DEBUG: Updating structural history
ğŸ”® IjepaPredictor: Attempting to save results to store...
ğŸ“Š PredictionStore: Updated pred_reason_1766634510284_1766634513785 with structural_continuity results
ğŸ’¾ 1 predictions saved to predictions.json
ğŸ”® IjepaPredictor: âœ… Successfully saved to prediction store
ğŸ”® DEBUG: Attempting save_to_disk()
ğŸ’¾ 1 predictions saved to predictions.json
ğŸ”® DEBUG: save_to_disk() completed
ğŸ”® DEBUG: Cached analysis for frame skipping
ğŸ”® IjepaPredictor: _execute_prediction_logic completed successfully in 882.0ms
ğŸ”® DEBUG: Final result keys = ['prediction_id', 'predictor_type', 'timestamp', 'status', 'structural_analysis', 'continuity_predictions', 'structural_risks', 'processing_metrics', 'data_sources_used']
ğŸ”® IjepaPredictor: Result type = <class 'dict'>
Result dict : {'prediction_id': 'pred_reason_1766634510284_1766634513785', 'predictor_type': 'structural_continuity', 'timestamp': 1766634514.6331477, 'status': 'completed', 'structural_analysis': {'current_structure_type': 'free_space', 'structural_confidence': 0.4828524887561798, 'embedding_stability': 0.5, 'frame_id': 0, 'similarity_scores': {'free_space': 0.4828524887561798}}, 'continuity_predictions': {'move_forward': {'action': 'move_forward', 'continuity_score': 0.2414262443780899, 'map_context_confidence': 0.5, 'structural_confidence': 0.4828524887561798, 'embedding_stability': 0.5, 'continuity_type': 'structural_break', 'similarity_scores': {'free_space': 0.4828524887561798}}, 'turn_left': {'action': 'turn_left', 'continuity_score': 0.14485574662685394, 'map_context_confidence': 0.5, 'structural_confidence': 0.4828524887561798, 'embedding_stability': 0.5, 'continuity_type': 'structural_break', 'similarity_scores': {'free_space': 0.4828524887561798}}, 'turn_right': {'action': 'turn_right', 'continuity_score': 0.14485574662685394, 'map_context_confidence': 0.5, 'structural_confidence': 0.4828524887561798, 'embedding_stability': 0.5, 'continuity_type': 'structural_break', 'similarity_scores': {'free_space': 0.4828524887561798}}}, 'structural_risks': {'move_forward': {'risk_level': 'medium', 'risk_score': 0.5344311311841011, 'continuity_score': 0.2414262443780899, 'prototype_similarity': 0.4828524887561798, 'stability': 0.5, 'factors': {'low_continuity': True, 'low_prototype_similarity': False, 'low_stability': True, 'structural_break': True, 'no_prototype_match': False}, 'similarity_contributions': {'free_space': 0.4828524887561798}, 'risk_components': {'continuity_risk': 0.37928687781095505, 'prototype_risk': 0.15514425337314605, 'stability_penalty': 0.0}}, 'turn_left': {'risk_level': 'medium', 'risk_score': 0.582716380059719, 'continuity_score': 0.14485574662685394, 'prototype_similarity': 0.4828524887561798, 'stability': 0.5, 'factors': {'low_continuity': True, 'low_prototype_similarity': False, 'low_stability': True, 'structural_break': True, 'no_prototype_match': False}, 'similarity_contributions': {'free_space': 0.4828524887561798}, 'risk_components': {'continuity_risk': 0.427572126686573, 'prototype_risk': 0.15514425337314605, 'stability_penalty': 0.0}}, 'turn_right': {'risk_level': 'medium', 'risk_score': 0.582716380059719, 'continuity_score': 0.14485574662685394, 'prototype_similarity': 0.4828524887561798, 'stability': 0.5, 'factors': {'low_continuity': True, 'low_prototype_similarity': False, 'low_stability': True, 'structural_break': True, 'no_prototype_match': False}, 'similarity_contributions': {'free_space': 0.4828524887561798}, 'risk_components': {'continuity_risk': 0.427572126686573, 'prototype_risk': 0.15514425337314605, 'stability_penalty': 0.0}}}, 'processing_metrics': {'processing_time_ms': 847.2316265106201, 'frame_id': 0, 'model_used': 'ijepa', 'prototypes_loaded': 1}, 'data_sources_used': {'frame_buffer': True, 'prediction_store': True, 'task_store': True, 'prototypes_available': True}}
ğŸ”® DEBUG: I-JEPA result type = <class 'dict'>
ğŸ”® DEBUG: I-JEPA result is dict with keys: dict_keys(['prediction_id', 'predictor_type', 'timestamp', 'status', 'structural_analysis', 'continuity_predictions', 'structural_risks', 'processing_metrics', 'data_sources_used'])
ğŸ”® DEBUG: Waiting for fused results...

ğŸ§  FUSION START | prediction_id = pred_reason_1766634510284_1766634513785
âœ… Data extracted: collision_actions=3, structural_risks=3
ğŸ“Š Data Quality | collision=0.90 structural=0.60
â¡ï¸ move_forward: combined_risk=0.204 decision=proceed
â¡ï¸ turn_left: combined_risk=0.204 decision=proceed
â¡ï¸ turn_right: combined_risk=0.204 decision=proceed
ğŸ›¡ï¸ Overall Safety=safe | Recommended=move_forward | MaxRisk=0.204
ğŸ“Š ReasoningPipeline: Trigger count: 1 - running reasoning cycle
Storing metadata/init for cycle: reason_1766634514668
ğŸ’¾ TASKSTORE: Saved current state to experiments/results/task_store_current.json
ğŸ§  Starting reasoning cycle: reason_1766634514668
  ğŸ§  Running Tree of Thoughts sequentially...
ğŸ¯ TOT DEBUG: LLM available: True
ğŸ” DEBUG generate_comprehensive_plan: Starting with cycle_id=reason_1766634514668
ğŸ” DEBUG generate_comprehensive_plan: LLM available=True
ğŸ” DEBUG generate_comprehensive_plan: Stage 1 completed
ğŸ” DEBUG generate_comprehensive_plan: Stage 2 completed
ğŸ” DEBUG _stage1_strategy_selection: Starting stage 1
ğŸ” DEBUG _extract_minimal_context: Extracting from stores
ğŸ” DEBUG _extract_minimal_context: self.map_store=True, self.user_command_store=True
ğŸ” DEBUG _get_mission_from_command_store: active_command=False
ğŸ” DEBUG _get_world_from_map_store: Entering
ğŸ” CENTRAL MAP STORE DEBUG:
   self.current_pose type: <class 'src.stores.central_map_store.CameraPose'>
   self.current_pose value: CameraPose(position=[0.0, 0.0, 0.0], rotation_quat=[0.0, 0.0, 0.0, 1.0], transform_matrix=[[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]], timestamp=0.0, frame_id=0, tracking_quality=1.0)
âœ… Retrieved map_summary successfully
ğŸ” map_summary keys: ['geometric_points_count', 'trajectory_length', 'explored_positions_count', 'stored_poses_count', 'objects_count', 'found_targets', 'remaining_targets', 'current_room', 'rooms_visited_count', 'room_history', 'current_pose', 'has_transform_matrix', 'last_update_time', 'uptime_seconds', 'update_counts', 'exploration_progress']
ğŸ” room_type: unknown
âœ… Got 0 objects from semantic_objects
âœ… _get_world_from_map_store SUCCESS (no CameraPose): {'room_type': 'unknown', 'key_objects': [], 'objects_count': 0, 'current_position': [0, 0]}
ğŸ” DEBUG _get_risk_from_prediction_store: Entering
ğŸ” DEBUG _get_risk_from_prediction_store: self.prediction_store type=<class 'src.stores.prediction_store.PredictionStore'>
ğŸ” DEBUG _stage1_strategy_selection: Context extracted: mission=unknown
ğŸ” DEBUG Prompt length: 375 chars
ğŸ” MODEL OUTPUT DEBUG:
   - Output type: <class 'transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput'>
   - logits shape: torch.Size([1, 576, 1])
   - logits min/max: -18.753/-6.603
   - pred_boxes shape: torch.Size([1, 576, 4])
ğŸ” POST-PROCESSING DEBUG:
   - target_sizes: tensor([[480, 640]], device='cuda:0')
   - confidence_threshold: 0.05
ğŸ” POST-PROCESS RESULTS DEBUG:
   - Results type: <class 'list'>
   - Results length: 1
ğŸ” RAW RESULT DEBUG:
   - Scores shape: torch.Size([0])
   - Labels shape: torch.Size([0])
   - Boxes shape: torch.Size([0, 4])
ğŸ¯ ALL SCORES:
ğŸ” CONFIDENCE STATS in:
   - Total predictions: 0
   - Passing threshold: 0
ğŸ¯ DEBUG: Raw results - scores: 0, labels: 0
âœ… OWL-ViT found 0 objects
ğŸ¯ 2D detections: 0
âœ… OWL: Got ALREADY-TRANSFORMED pose for frame 0
ğŸ“ Camera pose available
ğŸ”„ Projecting detections to 3D
ğŸ¯ PROJECTION: Starting 3D projection for 0 detections
ğŸ¯ PROJECTION: Camera pose available: True
ğŸ¯ PROJ DEBUG 1: Starting projection for frame 0
ğŸ¯ PROJ DEBUG 1.5: Detections count: 0
ğŸ¯ ORB-SLAM VERIFY: Camera pose OK for frame 0
   Position: [[1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 0. 1.]]
ğŸ¯ PROJ DEBUG 9: Using passed frame_dict, keys: dict_keys(['rgb', 'depth'])
ğŸ¯ PROJ DEBUG 10: depth_frame type: <class 'numpy.ndarray'>
ğŸ¯ PROJ DEBUG 13: Height: 480, Width: 640
ğŸ¯ PROJ DEBUG 14: Intrinsics: fx=320.00000000000006, fy=320.00000000000006
âœ… Projected 0 objects using pixel+depth+pose
ğŸ¯ PROJECTION: ORB-SLAM returned 0 objects
ğŸŒ 3D objects projected: 0
Tracking 0 objects after update
ğŸ§­ Objects after tracking: 0
Updated semantic store for frame 0 with 0 objects
ğŸ—ºï¸  Shared map store updated
â±ï¸  Processing time: 0.958s
ğŸ“Š Counters â†’ processed=1, avg_time=0.958s
âœ… OWL DEBUG â”€â”€â”€ END FRAME 0 â”€â”€â”€

ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 1.58s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 2.10s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 2.61s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 3.13s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 3.65s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ” DEBUG Raw response: Proximity First
âœ… Extracted strategy: proximity_first
ğŸ” DEBUG _stage1_strategy_selection: Strategy decision: proximity_first
ğŸ” STAGE2 DEBUG: strategy_decision keys = ['strategy_type', 'strategy_reasoning', 'confidence', 'mission_analysis', 'key_considerations', 'raw_prompt', 'raw_response', 'context_objects']
ğŸ” STAGE2 DEBUG: strategy_type = proximity_first
ğŸ” STAGE2 DEBUG: raw_response = Proximity First
ğŸ” DEBUG _get_world_from_map_store: Entering
ğŸ” CENTRAL MAP STORE DEBUG:
   self.current_pose type: <class 'src.stores.central_map_store.CameraPose'>
   self.current_pose value: CameraPose(position=[0.0, 0.0, 0.0], rotation_quat=[0.0, 0.0, 0.0, 1.0], transform_matrix=[[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]], timestamp=0.0, frame_id=0, tracking_quality=1.0)
âœ… Retrieved map_summary successfully
ğŸ” map_summary keys: ['geometric_points_count', 'trajectory_length', 'explored_positions_count', 'stored_poses_count', 'objects_count', 'found_targets', 'remaining_targets', 'current_room', 'rooms_visited_count', 'room_history', 'current_pose', 'has_transform_matrix', 'last_update_time', 'uptime_seconds', 'update_counts', 'exploration_progress']
ğŸ” room_type: unknown
âœ… Got 0 objects from semantic_objects
âœ… _get_world_from_map_store SUCCESS (no CameraPose): {'room_type': 'unknown', 'key_objects': [], 'objects_count': 0, 'current_position': [0, 0]}
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 4.16s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 4.68s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 5.19s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 5.71s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 6.23s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 6.74s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 7.26s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 7.78s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 8.30s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 8.82s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 9.34s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 9.86s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 10.37s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 10.89s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
ğŸ” DEBUG Stage 2 Raw response: {"target_objects": ["object1", "object2", "object3"], "search_pattern": "pattern description", "reasoning": "why this order", "expected_success": 0.7, "fallback_objects": ["object4", "object5"]}...
ğŸ” TEXT PARSER: Parsing 194 chars
ğŸ” TEXT PARSER INPUT: '{"target_objects": ["object1", "object2", "object3"], "search_pattern": "pattern description", "reasoning": "why this order", "expected_success": 0.7, "fallback_objects": ["object4", "object5"]}...'
ğŸ” TEXT PARSER: Cleaned from 194 to 194 chars
ğŸ” TEXT PARSER CLEANED: '{"target_objects": ["object1", "object2", "object3"], "search_pattern": "pattern description", "reasoning": "why this order", "expected_success": 0.7,...'
ğŸ” TEXT PARSER PHASE 1: Looking for JSON pattern...
âœ… TEXT PARSER: Found JSON pattern, 194 chars
ğŸ” TEXT PARSER JSON EXTRACT: '{"target_objects": ["object1", "object2", "object3"], "search_pattern": "pattern description", "reasoning": "why this order", "expected_success": 0.7,...'
âœ… TEXT PARSER: JSON parsed successfully
ğŸ” TEXT PARSER JSON TYPE: <class 'dict'>
âœ… TEXT PARSER SUCCESS: Extracted 3 targets from JSON
ğŸ” TEXT PARSER JSON RESULT: targets=['object1', 'object2', 'object3'], pattern=pattern description
ğŸ” TEXT PARSER JSON FULL: {'target_objects': ['object1', 'object2', 'object3'], 'search_pattern': 'pattern description', 'reasoning': 'why this order', 'expected_success': 0.7, 'fallback_objects': ['object4', 'object5'], 'parsed_from_text': True, 'parser_debug': {}, 'parsed_from_json': True}
ğŸ” STAGE2 PARSED plan_data type: <class 'dict'>
ğŸ” STAGE2 PARSED plan_data keys: ['target_objects', 'search_pattern', 'reasoning', 'expected_success', 'fallback_objects', 'parsed_from_text', 'parser_debug', 'parsed_from_json']
âš ï¸ No Umeyama goal found for 'object1', returning empty list
âš ï¸ No Umeyama goal found for 'object2', returning empty list
âš ï¸ No Umeyama goal found for 'object3', returning empty list
ğŸ” DEBUG _build_complete_plan: Building complete plan for cycle_id=reason_1766634514668
ğŸ” DEBUG _build_complete_plan: strategy_type=proximity_first
ğŸ” DEBUG _extract_minimal_context: Extracting from stores
ğŸ” DEBUG _extract_minimal_context: self.map_store=True, self.user_command_store=True
ğŸ” DEBUG _get_mission_from_command_store: active_command=False
ğŸ” DEBUG _get_world_from_map_store: Entering
ğŸ” CENTRAL MAP STORE DEBUG:
   self.current_pose type: <class 'src.stores.central_map_store.CameraPose'>
   self.current_pose value: CameraPose(position=[0.0, 0.0, 0.0], rotation_quat=[0.0, 0.0, 0.0, 1.0], transform_matrix=[[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]], timestamp=0.0, frame_id=0, tracking_quality=1.0)
âœ… Retrieved map_summary successfully
ğŸ” map_summary keys: ['geometric_points_count', 'trajectory_length', 'explored_positions_count', 'stored_poses_count', 'objects_count', 'found_targets', 'remaining_targets', 'current_room', 'rooms_visited_count', 'room_history', 'current_pose', 'has_transform_matrix', 'last_update_time', 'uptime_seconds', 'update_counts', 'exploration_progress']
ğŸ” room_type: unknown
âœ… Got 0 objects from semantic_objects
âœ… _get_world_from_map_store SUCCESS (no CameraPose): {'room_type': 'unknown', 'key_objects': [], 'objects_count': 0, 'current_position': [0, 0]}
ğŸ” DEBUG _get_risk_from_prediction_store: Entering
ğŸ” DEBUG _get_risk_from_prediction_store: self.prediction_store type=<class 'src.stores.prediction_store.PredictionStore'>
ğŸ” DEBUG _get_world_from_map_store: Entering
ğŸ” CENTRAL MAP STORE DEBUG:
   self.current_pose type: <class 'src.stores.central_map_store.CameraPose'>
   self.current_pose value: CameraPose(position=[0.0, 0.0, 0.0], rotation_quat=[0.0, 0.0, 0.0, 1.0], transform_matrix=[[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]], timestamp=0.0, frame_id=0, tracking_quality=1.0)
âœ… Retrieved map_summary successfully
ğŸ” map_summary keys: ['geometric_points_count', 'trajectory_length', 'explored_positions_count', 'stored_poses_count', 'objects_count', 'found_targets', 'remaining_targets', 'current_room', 'rooms_visited_count', 'room_history', 'current_pose', 'has_transform_matrix', 'last_update_time', 'uptime_seconds', 'update_counts', 'exploration_progress']
ğŸ” room_type: unknown
âœ… Got 0 objects from semantic_objects
âœ… _get_world_from_map_store SUCCESS (no CameraPose): {'room_type': 'unknown', 'key_objects': [], 'objects_count': 0, 'current_position': [0, 0]}
ğŸ” TOT FULL PLAN (reason_1766634514668): {'reasoning_cycle_id': 'reason_1766634514668', 'timestamp': 1766634524.5729399, 'planning_stages': ['strategy_selection', 'detailed_planning'], 'llm_used': True, 'strategy_decision': {'strategy_type': 'proximity_first', 'strategy_reasoning': 'Proximity First', 'confidence': 0.7, 'mission_analysis': 'Finding keys in unknown', 'key_considerations': ['speed', 'accuracy', 'object locations'], 'raw_prompt': 'Choose best search strategy for finding keys:\n\n        Options: proximity_first, priority_first, systematic_search, room_exploration\n\n        Mission: unknown\n        Location: unknown\n        Objects available: \n\n        Answer with ONE word from the options above, then a brief reason.\n\n        Example: "proximity_first - check closest objects first"\n\n        Your answer:', 'raw_response': 'Proximity First', 'context_objects': []}, 'detailed_plan': {'target_objects': ['object1', 'object2', 'object3'], 'search_pattern': 'pattern description', 'object_priority': ['object1', 'object2', 'object3'], 'reasoning': 'why this order', 'expected_success': 0.7, 'fallback_objects': ['object4', 'object5']}, 'reasoning_chain': [{'stage': 'strategy_selection', 'timestamp': 1766634517.1907246, 'input_context': {'mission': 'unknown', 'room_type': 'unknown', 'key_objects': [], 'objects_count': 0, 'risk_level': 'medium'}, 'llm_prompt': 'Choose best search strategy for finding keys:\n\n        Options: proximity_first, priority_first, systematic_search, room_exploration\n\n        Mission: unknown\n        Location: unknown\n        Objects available: \n\n        Answer with ONE word from the options above, then a brief reason.\n\n        Exa', 'llm_response': 'Proximity First', 'output_reasoning': {'strategy_type': 'proximity_first', 'strategy_reasoning': 'Proximity First', 'confidence': 0.7}}, {'stage': 'detailed_planning', 'timestamp': 1766634524.5729165, 'input_context': {'mission': 'Finding keys in unknown', 'strategy': 'proximity_first', 'stage1_reasoning': 'Proximity First', 'available_objects': []}, 'llm_prompt': 'Based on this strategy reasoning, create a search plan:\n\n        STRATEGY CHOSEN: proximity_first\n        REASONING: Proximity First\n\n        CREATE SEARCH PLAN FOR: Finding keys in unknown\n        AVAILABLE OBJECTS: \n\n        Return ONLY this JSON:\n        {\n            "target_objects": ["object1"', 'llm_response': '{"target_objects": ["object1", "object2", "object3"], "search_pattern": "pattern description", "reasoning": "why this order", "expected_success": 0.7, "fallback_objects": ["object4", "object5"]}', 'output_reasoning': {'target_objects': ['object1', 'object2', 'object3'], 'search_pattern': 'pattern description', 'object_priority': ['object1', 'object2', 'object3'], 'reasoning': 'why this order', 'expected_success': 0.7, 'parsed_from_json': True}}], 'input_context': {'minimal_context': {'mission': 'unknown', 'room_type': 'unknown', 'key_objects': [], 'objects_count': 0, 'risk_level': 'medium'}, 'focused_context': {'mission': 'Finding keys in unknown', 'room_type': 'unknown', 'relevant_objects': [], 'current_position': [0, 0]}}, 'execution_ready_plan': {'strategy_type': 'proximity_first', 'search_pattern': 'pattern description', 'expected_duration': 'medium'}}
Storing INTERMEDIATE reasoning for cycle: reason_1766634514668
ğŸ’¾ TASKSTORE: Saved current state to experiments/results/task_store_current.json
âœ… Generated comprehensive plan from Tree Of Thoughts reasoning reason_1766634514668
ğŸ” TOT DEBUG: Time after completion: 1766634524.579631
  âœ… Tree of Thoughts completed
  ğŸ” DEBUG: Tree of Thoughts output: {'reasoning_cycle_id': 'reason_1766634514668'}
  ğŸ—ºï¸ Running Spatial Reasoning sequentially...
ğŸ¯ Spatial Reasoning: Starting analysis for cycle reason_1766634514668
ğŸ” DEBUG: Found 0 semantic objects in MapStore
================================================================================
ğŸ” _fetch_all_spatial_data() - DEBUG START
================================================================================
ğŸ“Š STORE CHECK:
  â€¢ map_store exists: True
  â€¢ map_store type: <class 'src.stores.central_map_store.CentralMapStore'>
  â€¢ map_store dir: ['add_collision', 'add_geometric_points_rich', 'clear_collisions', 'clear_map', 'collision_points', 'config', 'creation_time', 'current_pose', 'current_room', 'explored_positions']...

ğŸ¯ STEP 1: Fetching semantic objects
  âœ… Found semantic_objects attribute
  â€¢ Total semantic objects: 0

ğŸ“ STEP 2: Fetching robot position
  âœ… Found current_pose attribute
  â€¢ Robot position: [0.0, 0.0, 0.0]
  â€¢ Position type: <class 'list'>

ğŸšª STEP 3: Fetching current room
  âœ… Found current_room attribute
  â€¢ Current room: unknown

ğŸ“¦ MAP_DATA SUMMARY:
  â€¢ Objects: 0 items
  â€¢ Robot position: [0.0, 0.0, 0.0]
  â€¢ Current room: unknown

ğŸ¯ STEP 4: Fetching task goal
  âœ… task_store exists
  â€¢ Calling get_task_summary()
  âŒ Failed to get task goal: 'dict' object has no attribute 'id'
  â€¢ Final task_goal keys: ['current_mission', 'search_progress', 'explored_rooms', 'current_task', 'umeyama_aligned_goals', 'mission_goals_list']

ğŸ”® STEP 5: Fetching predictions
  âœ… prediction_store exists
  âŒ prediction_store has no get_predictions method

âœ… FINAL DATA STRUCTURE:
  â€¢ map_data objects: 0
  â€¢ task_goal keys: 6
  â€¢ predictions type: <class 'dict'>
================================================================================
âœ… _fetch_all_spatial_data() - DEBUG COMPLETE
================================================================================
================================================================================
ğŸ” _fetch_all_spatial_data() - DEBUG START
================================================================================
ğŸ“Š STORE CHECK:
  â€¢ map_store exists: True
  â€¢ map_store type: <class 'src.stores.central_map_store.CentralMapStore'>
  â€¢ map_store dir: ['add_collision', 'add_geometric_points_rich', 'clear_collisions', 'clear_map', 'collision_points', 'config', 'creation_time', 'current_pose', 'current_room', 'explored_positions']...

ğŸ¯ STEP 1: Fetching semantic objects
  âœ… Found semantic_objects attribute
  â€¢ Total semantic objects: 0

ğŸ“ STEP 2: Fetching robot position
  âœ… Found current_pose attribute
  â€¢ Robot position: [0.0, 0.0, 0.0]
  â€¢ Position type: <class 'list'>

ğŸšª STEP 3: Fetching current room
  âœ… Found current_room attribute
  â€¢ Current room: unknown

ğŸ“¦ MAP_DATA SUMMARY:
  â€¢ Objects: 0 items
  â€¢ Robot position: [0.0, 0.0, 0.0]
  â€¢ Current room: unknown

ğŸ¯ STEP 4: Fetching task goal
  âœ… task_store exists
  â€¢ Calling get_task_summary()
  âŒ Failed to get task goal: 'dict' object has no attribute 'id'
  â€¢ Final task_goal keys: ['current_mission', 'search_progress', 'explored_rooms', 'current_task', 'umeyama_aligned_goals', 'mission_goals_list']

ğŸ”® STEP 5: Fetching predictions
  âœ… prediction_store exists
  âŒ prediction_store has no get_predictions method

âœ… FINAL DATA STRUCTURE:
  â€¢ map_data objects: 0
  â€¢ task_goal keys: 6
  â€¢ predictions type: <class 'dict'>
================================================================================
âœ… _fetch_all_spatial_data() - DEBUG COMPLETE
================================================================================
ğŸ” get_all_map_points: Found 987 points
ğŸ” CENTRAL MAP STORE DEBUG:
   self.current_pose type: <class 'src.stores.central_map_store.CameraPose'>
   self.current_pose value: CameraPose(position=[0.0, 0.0, 0.0], rotation_quat=[0.0, 0.0, 0.0, 1.0], transform_matrix=[[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]], timestamp=0.0, frame_id=0, tracking_quality=1.0)
ğŸ” get_all_map_points: Found 987 points
ğŸ“ Found ORB-SLAM map: projects/hybrid_zero_shot_slam_nav/main/experiments/results/logs/orbslam_map.json
âœ… Robot parameters calculated from real data:
   Radius: 0.50m, Clearance: 0.35m, Reach: 0.06m
âœ… Spatial reasoning stored in intermediate_reasoning for reason_1766634514668
âœ… Spatial reasoning completed for cycle: reason_1766634514668
   ğŸ“Š Results: 0 reachable objects, 0 risk zones, 0 prioritized objects
ğŸ” SPATIAL REASONING FULL OUTPUT (reason_1766634514668):
{
  "reachability_analysis": {
    "reachable_objects": [],
    "total_reachable": 0,
    "unreachable_reason": "distance_or_risk"
  },
  "search_strategy": {
    "priority_list": [],
    "top_priority": null,
    "total_objects_prioritized": 0
  },
  "safety_analysis": {
    "risk_zones": [],
    "high_risk_count": 0,
    "medium_risk_count": 0
  },
  "perception_analysis": {
    "visibility_scores": {},
    "best_visible": null,
    "worst_visible": null
  },
  "navigation_analysis": {
    "path_feasibility": {},
    "most_feasible_room": null,
    "least_feasible_room": null
  },
  "exploration_analysis": {
    "room_connectivity": [],
    "unexplored_rooms": [],
    "next_exploration_target": null
  },
  "recommendations": [
    "Continue current exploration pattern"
  ],
  "summary_metrics": {
    "total_objects_analyzed": 0,
    "safe_navigation_paths": 0,
    "exploration_progress": 0.0
  },
  "reasoning_cycle_id": "reason_1766634514668",
  "component_name": "spatial_reasoning",
  "pipeline_timestamp": 1766634524.613625,
  "processing_stages": [
    "reachability",
    "priority",
    "risk",
    "visibility",
    "navigation",
    "exploration"
  ]
}
  âœ… Spatial Reasoning completed
  ğŸ” DEBUG: Spatial Reasoning output: {'reasoning_cycle_id': 'reason_1766634514668'}
ğŸ“Š ReasoningPipeline: Components completed - ['tree_of_thoughts', 'spatial_reasoning']
âœ… Retrieved Tree of Thoughts from intermediate_reasoning
âœ… Retrieved Spatial Reasoning from intermediate_reasoning
ğŸ“Š ReasoningPipeline: Retrieved data from 2 components: ['tree_of_thoughts', 'spatial_reasoning']
================================================================================
ğŸ” FUSION DEBUG: INPUT COMPONENT DATA
All component keys: ['tree_of_thoughts', 'spatial_reasoning']
================================================================================
âœ… FUSING BOTH: tree_of_thoughts + spatial_reasoning
Spatial Object Map: {}
ğŸ” SPATIAL DATA CHECK:
  â€¢ No spatial priority list
âœ… FUSED BOTH: Created 3 rich actions
================================================================================
ğŸ” FUSION DEBUG: FINAL FUSED RESULT {'selected_action': {'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object1', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}, 'planned_actions': [{'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object1', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}, {'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object2', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}, {'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object3', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}], 'final_confidence': 0.48999999999999994, 'components_contributing': ['tree_of_thoughts', 'spatial_reasoning'], 'execution_readiness': 'needs_confirmation', 'plan_metadata': {'target_objects': ['object1', 'object2', 'object3'], 'search_pattern': 'pattern description', 'spatial_priorities': [], 'top_priority': None}, 'has_coordinates': False, 'target_objects': ['object1', 'object2', 'object3']}
  Selected action: {'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object1', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}
  Planned actions: 3
================================================================================
ğŸ¯ ReasoningPipeline: Fusion logic = None, Action = {'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object1', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}, Confidence = 0.49
ğŸ’¾ Saving action plan via write_reasoning_plan...
FINAL ACTION PLAN DETECTED â†’ {'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object1', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}} (cycle reason_1766634514668)
Triggering action_plan_ready for {'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object1', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}
TaskStore[127265840103232] notifying 1 subscribers about action_plan_ready
ğŸ§µ Active threads Before on_actionplan_ready: 3
Thread names: ['MainThread', 'Thread-1', 'Thread-2']
ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””
ACTION PLAN RECEIVED â†’ STARTING EXECUTION THREAD
ğŸ”” TASKSTORE TRIGGER RECEIVED!
   ğŸ“¡ Event type: action_plan_ready
   ğŸ“¦ Data keys: ['action_plan', 'timestamp', 'reasoning_cycle_id']
ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””ğŸ””
ğŸ“Š Trigger count: 1
ğŸ“‹ Action plan queue created
ğŸ“¥ ACTION PLAN QUEUED:
   ğŸ¯ Action: {'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object1', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}
   ğŸ“Š Confidence: 0.49
   ğŸ“ Queue position: 1
   ğŸ†” Plan ID: reason_1766634514668
ğŸ“ˆ Action plan counter initialized
ğŸ“¥ ACTION PLAN started: using _event_driven_started trigger and run_event_driven finction call 
ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯
ğŸ¦¾ ACTION PIPELINE: STARTING EVENT-DRIVEN EXECUTION
ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯
   ğŸ“¡ Mode: Event-driven (no loops)
   â³ Waiting for action plans from ReasoningPipeline...
   ğŸ”” Listening for TaskStore triggers...
Action execution started, start_execution called
âœ… Action executor started
ğŸ”„ Entering event-driven wait loop...
ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯
ğŸš€ EXECUTING ACTION PLAN: '{'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object1', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}'
   ğŸ“Š Confidence: 0.49
   ğŸ“‹ Plan ID: reason_1766634514668
   ğŸ“ Source: unknown
ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯
ğŸ¦¾ Initiating run_execution_cycle method from action executor...
Run Execution cycle from action executor reached
Current Planned actions : [{'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object1', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}, {'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object2', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}, {'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object3', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}]
Current Planned reference : {'reasoning_cycle_id': 'reason_1766634514668', 'timestamp': 1766634524.6137612, 'selected_action': {'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object1', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}, 'planned_actions': [{'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object1', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}, {'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object2', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}, {'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object3', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}}], 'confidence': 0.48999999999999994, 'reasoning_source': 'unknown', 'components_used': ['tree_of_thoughts', 'spatial_reasoning'], 'component': 'final_action', 'task_status': 'pending_execution', 'execution_attempts': 0, 'last_execution_attempt': None, 'completion_status': 'not_started', 'execution_metadata': {'requires_confirmation': True, 'estimated_duration': 'pending', 'prerequisites_met': True, 'safety_clearance': 'pending'}}
DEBUG: _execute_actions_in_habitat called with 3 actions
ğŸ¯ ACTION EXECUTOR: Starting execution of execute_single_action: action type : 'move_forward'
ğŸ“ Valid initial pose: [-1.31, 1.5, 1.53]
Target coordinates inside action executor: None
âš ï¸ WARNING: target_coordinates is None! Checking HabitatStore...
ğŸ¯ HABITATSTORE: Using stored goal 'tv' at [-0.15094196796417236, 2.3524980545043945, -2.4764504432678223]
ğŸ¯ CONTINUOUS NAVIGATION to [-0.15094196796417236, 2.3524980545043945, -2.4764504432678223]
Current quat from Habitat: quaternion(0.932327345606034, 0, 0.361615431964962, 0)
ğŸ”„ Starting CONTINUOUS NAVIGATION to [-0.15094196796417236, 2.3524980545043945, -2.4764504432678223]
Target xyz : [-0.15094197  2.35249805 -2.47645044]
Current position : [-1.31  1.5   1.53]
Delta vector calculated : [ 1.15905803  0.85249805 -4.00645044]
ğŸ“Š Step 0: Distance to target = 4.17m
ğŸ” [STUCK_DETECT] Starting FIXED detection...
âš ï¸ [STUCK_DETECT] Not enough points for oscillation: 6 < 30
YAW : 0.7400000000000001
Desired YAW : -1.2891865838164553
YAW Error : -2.0291865838164553
ğŸ” [I-JEPA] Fetching from RAW predictions...
âœ… Using I-JEPA: pred_reason_1766634510284_1766634513785
   Has continuity_predictions: True
   Has structural_risks: True
âœ… [NAV] Got I-JEPA prediction with keys: ['prediction_id', 'predictor_type', 'timestamp', 'status', 'structural_analysis', 'continuity_predictions', 'structural_risks', 'processing_metrics', 'data_sources_used']
ğŸ“Š [NAV] Continuity keys: ['move_forward', 'turn_left', 'turn_right']
ğŸ“Š [NAV] Risks keys: ['move_forward', 'turn_left', 'turn_right']
ğŸ§  I-JEPA ANALYSIS: L:0.14 (risk:0.58) vs R:0.14 (risk:0.58)
ğŸ§  I-JEPA: NO BIAS (balanced: L:0.06 â‰ˆ R:0.06)
âœ… NORMAL MODE: lin=0.02 ang=-1.50 (yaw_err*2=-4.06 + ijepa=0.00)
ğŸ¯ FINAL VELOCITY: lin_vel=0.020 ang_vel=-1.500
Linear velocity : 0.02
Angular veloctiy : -1.5
ğŸ”® Predicted from ACTUAL pos: [-1.30852306  1.5         1.53134858]
Velocity metadata : {'target_xyz': [-0.15094196796417236, 2.3524980545043945, -2.4764504432678223], 'dist_remaining': 4.17073862475074, 'lin_vel': 0.02, 'ang_vel': -1.5, 'yaw_error_deg': -116.26382709725237, 'step_id': 0, 'current_pos': [-1.31, 1.5, 1.53], 'predicted_pos': [-1.3085230628825408, 1.5, 1.5313485758232563], 'continuous_nav': True, 'velocity_data': {'lin_vel': 0.02, 'ang_vel': -1.5, 'duration': 0.1}}
  ğŸ¯ Velocity: lin=0.02 m/s, ang=-1.50 rad/s
Velocity parameters : {'linear_velocity': [0.02, 0.0, 0.0], 'angular_velocity': [0.0, -1.5, 0.0], 'duration': 0.1, 'is_velocity_command': True}
ğŸ“¥ HABITATSTORE: Pushing action 'velocity_control' to queue
ğŸ’¾ TASKSTORE: Saved current state to experiments/results/task_store_current.json
ğŸ§  Reasoning cycle reason_1766634514668 completed - Action: {'type': 'move_forward', 'action': 'move_forward', 'parameters': {'target_object': 'object1', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}} (Status: pending_execution) (Confidence: 0.49)
ğŸ’¾ Fused prediction stored successfully
âœ… Fusion returned: <class 'dict'>
âœ… Fused keys: dict_keys(['timestamp', 'overall_safety_level', 'recommended_action', 'max_combined_risk', 'action_decisions', 'data_quality', 'risk_summary'])
ğŸ” PREDICTIONS DEBUG - Type: <class 'dict'>
ğŸ” PREDICTIONS DEBUG - Keys: ['timestamp', 'overall_safety_level', 'recommended_action', 'max_combined_risk', 'action_decisions', 'data_quality', 'risk_summary']
ğŸ¯ Using direct fused predictions structure
  ğŸ”® ACTION DECISIONS:
  ğŸ“Š move_forward: risk=0.204, decision=proceed
  ğŸ“Š turn_left: risk=0.204, decision=proceed
  ğŸ“Š turn_right: risk=0.204, decision=proceed
  ğŸ¯ Fused prediction suggests: move_forward
ğŸ’¡ Processing frame 0 brightness: 150.9
âœ… Frame 0 captured - RGB: (480, 640, 3), Depth: (480, 640)
ğŸ“Š Progress: 1/500 frames (0.2%)
ğŸ“¦ HABITATSTORE: Added metadata to action 'velocity_control'
âœ… HABITATSTORE: Action 'velocity_control' queued (ID: action_1766634524624_0)
ğŸ“Š HABITATSTORE: Queue size: 1
ğŸ“¤ Pushed velocity command (ID: action_1766634524624_0)
â³ No result yet for action_1766634524624_0
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
â³ No result yet for action_1766634524624_0
ğŸ¯ HABITAT ACTUAL POSITION: [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-1.1908181   1.         -0.06276327]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 0.9374m, threshold: 0.2400m
ğŸ”   rotation: 0.7000rad, threshold: 0.1000rad
ğŸ”   time_delta: 11.41s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 0.937m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” Mission check: 1 total goals, 1 remaining
   Mission goals: {'tv'}
   Remaining goals: ['tv']

--- FRAME 2/500 ---
ğŸ”„ Stepping simulator 3 times to warm up sensors...
DEBUG: about to step simulator
â³ No result yet for action_1766634524624_0
DEBUG: about to step simulator
ğŸ¦‰ OWL: starting to Process REAL frame 0 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
â³ No result yet for action_1766634524624_0
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
âš ï¸  Skipping frame 0 for orb_slam (already processed)
ğŸ¯ BUFFER DEBUG: Frame ID requested: 0
ğŸ¯ BUFFER DEBUG: Result type: <class 'NoneType'>
ğŸš¨ CRITICAL: Buffer returned None!
   This means ORB-SLAM will get NO FRAMES!
DEBUG: about to step simulator
âœ… Simulator warm-up complete, first frames ready
DEBUG: about to get sensor observations
  e   r   f   o   r   m   a   n   c   e   _   m   e   t   r   i   c   s   .   c   o   p   y   (   )   ,   
                                                                   "   câ³ No result yet for action_1766634524624_0
ğŸ¯ CURRENT SENSORS: ['rgba_camera', 'depth_camera']
DEBUG: about to extract rgb and depth
ğŸ¯ DEPTH STATS - Min: 0.0, Max: 10.202118873596191, Mean: 2.069
âœ… Depth frame has 282699 valid pixels
DEBUG: RGBA -> RGB conversion for frame 1
DEBUG: Transposing rgb_frame from (640, 480, 3) to (480, 640, 3)
DEBUG: Depth frame shape before transpose: (640, 480)
DEBUG: Depth frame shape after transpose: (480, 640)
DEBUG: Converted depth dtype to uint16
DEBUG: about to rotate rgb frame
DEBUG: about to write rgb frame
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 0 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object
â³ No result yet for action_1766634524624_0
â³ No result yet for action_1766634524624_0
DEBUG: about to scale depth
ğŸ¯ DEBUG: Frame 1 - RGB shape: (480, 640, 3), Depth shape: (480, 640)
ğŸ¯ DEBUG: RGB dtype: uint8, Depth dtype: uint16
âœ… DEBUG: Frame 1 written to buffer - Slot 1
âœ… DEBUG: Buffer RGB dtype: uint8, Depth dtype: uint16
  Wrote frame 1 to buffer successfully âœ…
  âœ… Frame 2 captured - RGB: (480, 640, 3), Depth: (480, 640)
ğŸŸ¦ DEBUG: Calling ORB-SLAM process_frame() for frame 1
ğŸŸ© DEBUG: Returned from ORB-SLAM process_frame() for frame 1
ğŸ” Checking for queued actions...
ğŸ¯ HABITATSTORE: Pulling next action from queue
ğŸš€ HABITATSTORE: Pulled VELOCITY CONTROL command
   ğŸ“¦ Metadata: ['target_xyz', 'dist_remaining', 'lin_vel', 'ang_vel', 'yaw_error_deg', 'step_id', 'current_pos', 'predicted_pos', 'continuous_nav', 'velocity_data']
ğŸ“Š HABITATSTORE: Remaining in queue: 0
ğŸ¯ MAINORCH: Executing action 'velocity_control'
================================================================================
ğŸ¯ Executing Habitat action: velocity_control
ğŸ“¦ Parameters: {'linear_velocity': [0.02, 0.0, 0.0], 'angular_velocity': [0.0, -1.5, 0.0], 'duration': 0.1, 'is_velocity_command': True}
ğŸ“¦ Metadata: {'target_xyz': [-0.15094196796417236, 2.3524980545043945, -2.4764504432678223], 'dist_remaining': 4.17073862475074, 'lin_vel': 0.02, 'ang_vel': -1.5, 'yaw_error_deg': -116.26382709725237, 'step_id': 0, 'current_pos': [-1.31, 1.5, 1.53], 'predicted_pos': [-1.3085230628825408, 1.5, 1.5313485758232563], 'continuous_nav': True, 'velocity_data': {'lin_vel': 0.02, 'ang_vel': -1.5, 'duration': 0.1}}
================================================================================
ğŸ§© Current agent position: [-1.6965339   0.16337794 -0.6166147 ]
ğŸ§© Current agent rotation (quat): quaternion(0.875802874565125, 0.123997122049332, 0.339691668748856, 0.319692999124527)
ğŸ“ prev_pos saved: [-1.6965339   0.16337794 -0.6166147 ]
ğŸ§® Linear velocity: [0.02, 0.0, 0.0], angular velocity: [0.0, -1.5, 0.0], dt=0.1
ğŸ“ RigidState before integrate: pos=Vector(-1.69653, 0.163378, -0.616615), rot=Quaternion({0.123997, 0.339692, 0.319693}, 0.875803)
ğŸ“ Predicted new_pos from VelocityControl: [-1.695404291152954, 0.16466636955738068, -0.6176461577415466]
ğŸ§­ is_navigable(new_pos) = True
âœ… Using XZ from new_pos with preserved Y: [-1.695404291152954, 1.0, -0.6176461577415466]
ğŸ“Œ Final applied position: [-1.695404291152954, 1.0, -0.6176461577415466]
ğŸ“Œ Final applied rotation (quat): quaternion(0.898793816566467, 0.147603049874306, 0.27311310172081, 0.309503227472305)
ğŸ›‘ STALL CHECK: moved=0.8366m, stalled=True
ğŸ”„ METRICS: Velocity control - tracking position
ğŸ“ METRICS: Post-velocity pos = [-1.6954043   1.         -0.61764616]
ğŸ“ METRICS: Trajectory len = 7
ğŸ¦‰ OWL: starting to Process REAL frame 1 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
Observations from _execute_habitat_action : velocity control : {'rgba_camera': array([[[165, 157, 148, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [166, 159, 151, 255],
        [165, 157, 148, 255],
        ...,
        [141, 131, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       ...,

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [161, 153, 144, 255]]], dtype=uint8), 'depth_camera': array([[1.2247428 , 1.2181447 , 1.2116172 , ..., 0.33664986, 0.33839086,
        0.34014854],
       [1.2328697 , 1.2261839 , 1.2195703 , ..., 0.3378777 , 0.3396314 ,
        0.34140205],
       [1.2411052 , 1.23433   , 1.2276285 , ..., 0.3391145 , 0.3408811 ,
        0.34266484],
       ...,
       [0.74431515, 0.745529  , 0.7467501 , ..., 2.5809772 , 2.5786366 ,
        2.5763798 ],
       [0.7435729 , 0.74478436, 0.74600303, ..., 2.5783198 , 2.5760236 ,
        2.573692  ],
       [0.74282885, 0.74403787, 0.7452541 , ..., 2.5756283 , 2.5733368 ,
        2.5710497 ]], dtype=float32)}
ğŸ‘ rgba_camera shape: (640, 480, 4)
ğŸŒŠ depth_camera shape: (640, 480)
âœ… Velocity motion applied (projected)
ğŸ“¤ Result (velocity_control): status=completed, collided=True, new_position=[-1.695404291152954, 1.0, -0.6176461577415466]
================================================================================
ğŸ“¤ Result (velocity_control):{'action_id': 'action_1766634524624_0', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[165, 157, 148, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [166, 159, 151, 255],
        [165, 157, 148, 255],
        ...,
        [141, 131, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       ...,

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [161, 153, 144, 255]]], dtype=uint8), 'depth_camera': array([[1.2247428 , 1.2181447 , 1.2116172 , ..., 0.33664986, 0.33839086,
        0.34014854],
       [1.2328697 , 1.2261839 , 1.2195703 , ..., 0.3378777 , 0.3396314 ,
        0.34140205],
       [1.2411052 , 1.23433   , 1.2276285 , ..., 0.3391145 , 0.3408811 ,
        0.34266484],
       ...,
       [0.74431515, 0.745529  , 0.7467501 , ..., 2.5809772 , 2.5786366 ,
        2.5763798 ],
       [0.7435729 , 0.74478436, 0.74600303, ..., 2.5783198 , 2.5760236 ,
        2.573692  ],
       [0.74282885, 0.74403787, 0.7452541 , ..., 2.5756283 , 2.5733368 ,
        2.5710497 ]], dtype=float32)}, 'collided': True, 'new_position': [-1.695404291152954, 1.0, -0.6176461577415466], 'new_rotation': quaternion(0.898793816566467, 0.147603049874306, 0.27311310172081, 0.309503227472305), 'timestamp': 1766634524.9947624}
Action in habitat simulation result : {'action_id': 'action_1766634524624_0', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[165, 157, 148, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [166, 159, 151, 255],
        [165, 157, 148, 255],
        ...,
        [141, 131, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       ...,

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [161, 153, 144, 255]]], dtype=uint8), 'depth_camera': array([[1.2247428 , 1.2181447 , 1.2116172 , ..., 0.33664986, 0.33839086,
        0.34014854],
       [1.2328697 , 1.2261839 , 1.2195703 , ..., 0.3378777 , 0.3396314 ,
        0.34140205],
       [1.2411052 , 1.23433   , 1.2276285 , ..., 0.3391145 , 0.3408811 ,
        0.34266484],
       ...,
       [0.74431515, 0.745529  , 0.7467501 , ..., 2.5809772 , 2.5786366 ,
        2.5763798 ],
       [0.7435729 , 0.74478436, 0.74600303, ..., 2.5783198 , 2.5760236 ,
        2.573692  ],
       [0.74282885, 0.74403787, 0.7452541 , ..., 2.5756283 , 2.5733368 ,
        2.5710497 ]], dtype=float32)}, 'collided': True, 'new_position': [-1.695404291152954, 1.0, -0.6176461577415466], 'new_rotation': quaternion(0.898793816566467, 0.147603049874306, 0.27311310172081, 0.309503227472305), 'timestamp': 1766634524.9947624}
âœ… Storing SINGLE result for action_1766634524624_0
Storing result using store_action_result() for action_1766634524624_0 | All stored IDs: ['action_1766634512479_0', 'action_1766634512600_1', 'action_1766634512722_2', 'action_1766634512845_3', 'action_1766634512966_4', 'action_1766634524624_0']
Stored results for id : action_1766634524624_0 : {'action_id': 'action_1766634524624_0', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[165, 157, 148, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [166, 159, 151, 255],
        [165, 157, 148, 255],
        ...,
        [141, 131, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       ...,

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [161, 153, 144, 255]]], dtype=uint8), 'depth_camera': array([[1.2247428 , 1.2181447 , 1.2116172 , ..., 0.33664986, 0.33839086,
        0.34014854],
       [1.2328697 , 1.2261839 , 1.2195703 , ..., 0.3378777 , 0.3396314 ,
        0.34140205],
       [1.2411052 , 1.23433   , 1.2276285 , ..., 0.3391145 , 0.3408811 ,
        0.34266484],
       ...,
       [0.74431515, 0.745529  , 0.7467501 , ..., 2.5809772 , 2.5786366 ,
        2.5763798 ],
       [0.7435729 , 0.74478436, 0.74600303, ..., 2.5783198 , 2.5760236 ,
        2.573692  ],
       [0.74282885, 0.74403787, 0.7452541 , ..., 2.5756283 , 2.5733368 ,
        2.5710497 ]], dtype=float32)}, 'collided': True, 'new_position': [-1.695404291152954, 1.0, -0.6176461577415466], 'new_rotation': quaternion(0.898793816566467, 0.147603049874306, 0.27311310172081, 0.309503227472305), 'timestamp': 1766634524.9947624}
âœ… Action 'velocity_control' completed, result stored
ğŸ¯ HABITATSTORE: Pulling next action from queue
ğŸ“­ HABITATSTORE: Action queue is empty
âœ… Retrieved result for action_1766634524624_0 : {'action_id': 'action_1766634524624_0', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[165, 157, 148, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [166, 159, 151, 255],
        [165, 157, 148, 255],
        ...,
        [141, 131, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       ...,

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [161, 153, 144, 255]]], dtype=uint8), 'depth_camera': array([[1.2247428 , 1.2181447 , 1.2116172 , ..., 0.33664986, 0.33839086,
        0.34014854],
       [1.2328697 , 1.2261839 , 1.2195703 , ..., 0.3378777 , 0.3396314 ,
        0.34140205],
       [1.2411052 , 1.23433   , 1.2276285 , ..., 0.3391145 , 0.3408811 ,
        0.34266484],
       ...,
       [0.74431515, 0.745529  , 0.7467501 , ..., 2.5809772 , 2.5786366 ,
        2.5763798 ],
       [0.7435729 , 0.74478436, 0.74600303, ..., 2.5783198 , 2.5760236 ,
        2.573692  ],
       [0.74282885, 0.74403787, 0.7452541 , ..., 2.5756283 , 2.5733368 ,
        2.5710497 ]], dtype=float32)}, 'collided': True, 'new_position': [-1.695404291152954, 1.0, -0.6176461577415466], 'new_rotation': quaternion(0.898793816566467, 0.147603049874306, 0.27311310172081, 0.309503227472305), 'timestamp': 1766634524.9947624}using get_action_result() 
âœ… Velocity result received: {'action_id': 'action_1766634524624_0', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[165, 157, 148, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [166, 159, 151, 255],
        [165, 157, 148, 255],
        ...,
        [141, 131, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       ...,

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [161, 153, 144, 255]]], dtype=uint8), 'depth_camera': array([[1.2247428 , 1.2181447 , 1.2116172 , ..., 0.33664986, 0.33839086,
        0.34014854],
       [1.2328697 , 1.2261839 , 1.2195703 , ..., 0.3378777 , 0.3396314 ,
        0.34140205],
       [1.2411052 , 1.23433   , 1.2276285 , ..., 0.3391145 , 0.3408811 ,
        0.34266484],
       ...,
       [0.74431515, 0.745529  , 0.7467501 , ..., 2.5809772 , 2.5786366 ,
        2.5763798 ],
       [0.7435729 , 0.74478436, 0.74600303, ..., 2.5783198 , 2.5760236 ,
        2.573692  ],
       [0.74282885, 0.74403787, 0.7452541 , ..., 2.5756283 , 2.5733368 ,
        2.5710497 ]], dtype=float32)}, 'collided': True, 'new_position': [-1.695404291152954, 1.0, -0.6176461577415466], 'new_rotation': quaternion(0.898793816566467, 0.147603049874306, 0.27311310172081, 0.309503227472305), 'timestamp': 1766634524.9947624}
Result from action executor real navigation: {'action_id': 'action_1766634524624_0', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[165, 157, 148, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [166, 159, 151, 255],
        [165, 157, 148, 255],
        ...,
        [141, 131, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       ...,

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [161, 153, 144, 255]]], dtype=uint8), 'depth_camera': array([[1.2247428 , 1.2181447 , 1.2116172 , ..., 0.33664986, 0.33839086,
        0.34014854],
       [1.2328697 , 1.2261839 , 1.2195703 , ..., 0.3378777 , 0.3396314 ,
        0.34140205],
       [1.2411052 , 1.23433   , 1.2276285 , ..., 0.3391145 , 0.3408811 ,
        0.34266484],
       ...,
       [0.74431515, 0.745529  , 0.7467501 , ..., 2.5809772 , 2.5786366 ,
        2.5763798 ],
       [0.7435729 , 0.74478436, 0.74600303, ..., 2.5783198 , 2.5760236 ,
        2.573692  ],
       [0.74282885, 0.74403787, 0.7452541 , ..., 2.5756283 , 2.5733368 ,
        2.5710497 ]], dtype=float32)}, 'collided': True, 'new_position': [-1.695404291152954, 1.0, -0.6176461577415466], 'new_rotation': quaternion(0.898793816566467, 0.147603049874306, 0.27311310172081, 0.309503227472305), 'timestamp': 1766634524.9947624}
observations : {'rgba_camera': array([[[165, 157, 148, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [166, 159, 151, 255],
        [165, 157, 148, 255],
        ...,
        [141, 131, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       ...,

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [161, 153, 144, 255]]], dtype=uint8), 'depth_camera': array([[1.2247428 , 1.2181447 , 1.2116172 , ..., 0.33664986, 0.33839086,
        0.34014854],
       [1.2328697 , 1.2261839 , 1.2195703 , ..., 0.3378777 , 0.3396314 ,
        0.34140205],
       [1.2411052 , 1.23433   , 1.2276285 , ..., 0.3391145 , 0.3408811 ,
        0.34266484],
       ...,
       [0.74431515, 0.745529  , 0.7467501 , ..., 2.5809772 , 2.5786366 ,
        2.5763798 ],
       [0.7435729 , 0.74478436, 0.74600303, ..., 2.5783198 , 2.5760236 ,
        2.573692  ],
       [0.74282885, 0.74403787, 0.7452541 , ..., 2.5756283 , 2.5733368 ,
        2.5710497 ]], dtype=float32)}
step collided data from results : True
ğŸ”„ Valid Habitat position update: [-1.69540429  1.         -0.61764616]
ğŸ”„ Using Habitat rotation (planar): [0.0, 0.3539211608200983, 0.0, 0.9352752599763099]
observations  _execute_continuous_navigation : {'rgba_camera': array([[[165, 157, 148, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [166, 159, 151, 255],
        [165, 157, 148, 255],
        ...,
        [141, 131, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       ...,

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [161, 153, 144, 255]]], dtype=uint8), 'depth_camera': array([[1.2247428 , 1.2181447 , 1.2116172 , ..., 0.33664986, 0.33839086,
        0.34014854],
       [1.2328697 , 1.2261839 , 1.2195703 , ..., 0.3378777 , 0.3396314 ,
        0.34140205],
       [1.2411052 , 1.23433   , 1.2276285 , ..., 0.3391145 , 0.3408811 ,
        0.34266484],
       ...,
       [0.74431515, 0.745529  , 0.7467501 , ..., 2.5809772 , 2.5786366 ,
        2.5763798 ],
       [0.7435729 , 0.74478436, 0.74600303, ..., 2.5783198 , 2.5760236 ,
        2.573692  ],
       [0.74282885, 0.74403787, 0.7452541 , ..., 2.5756283 , 2.5733368 ,
        2.5710497 ]], dtype=float32)}
Step Collided or not  _execute_continuous_navigation: True
âš ï¸ Collision at step 0 â€” CONTINUOUS RECOVERY
ğŸ” [STUCK_DETECT] Starting FIXED detection...
âš ï¸ [STUCK_DETECT] Not enough points for oscillation: 7 < 30
ğŸ”„ [COLLISION] Normal RIGHT recovery
Angular velocity for recovery : 0.8
Recovery params : {'linear_velocity': [0.0, 0.0, 0.0], 'angular_velocity': [0.0, 0.8, 0.0], 'duration': 0.1, 'is_velocity_command': True}
Recovery metadata : {'continuous_nav': True, 'recovery': True, 'attempt': 0, 'ang_vel': 0.8}
ğŸ”„ Recovery 1: ang_vel=0.80
ğŸ“¥ HABITATSTORE: Pushing action 'velocity_control' to queue
ğŸ’¡ Processing frame 1 brightness: 110.6
âœ… Frame 1 captured - RGB: (480, 640, 3), Depth: (480, 640)
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
ğŸ“¦ HABITATSTORE: Added metadata to action 'velocity_control'
âœ… HABITATSTORE: Action 'velocity_control' queued (ID: action_1766634525007_0)
ğŸ“Š HABITATSTORE: Queue size: 1
â³ No result yet for action_1766634525007_0

ğŸŸ¢ OWL DEBUG â”€â”€â”€ START FRAME 1 â”€â”€â”€
ğŸ“¦ Frame metadata:
   â€¢ frame_id      = 1
   â€¢ timestamp     = 0.033
   â€¢ rgb shape     = (480, 640, 3)
   â€¢ depth shape   = (480, 640)
ğŸ¯ Detection input: shape=(480, 640, 3), dtype=uint8
ğŸ” FRAME STATS:
   - Min pixel value: 0
   - Max pixel value: 255
   - Mean brightness: 110.6
   - Brightness std: 62.1
ğŸ¦‰ OWL DEBUG: Starting _get_text_queries_from_task_store
ğŸ¦‰ OWL DEBUG: self.task_store exists = True
ğŸ¯ OWL HABITATSTORE: Using stored goal 'tv'
ğŸ” CURRENT QUERIES DEBUG:
   - Query count: 1
   - First 3 queries: ['tv']
ğŸ” INPUTS DEBUG:
   - input_ids shape: torch.Size([1, 16])
   - attention_mask shape: torch.Size([1, 16])
   - pixel_values shape: torch.Size([1, 3, 768, 768])
   - pixel_values dtype: torch.float32
   - pixel_values min/max: -1.792/2.146
ğŸ” OWL DEBUG: Inputs keys: ['input_ids', 'attention_mask', 'pixel_values']
ğŸ” OWL DEBUG: Text queries: ['tv']
â³ No result yet for action_1766634525007_0
ğŸ” Mission check: 1 total goals, 1 remaining
   Mission goals: {'tv'}
   Remaining goals: ['tv']

--- FRAME 3/500 ---
ğŸ”„ Stepping simulator 3 times to warm up sensors...
DEBUG: about to step simulator
â³ No result yet for action_1766634525007_0
DEBUG: about to step simulator
â³ No result yet for action_1766634525007_0
DEBUG: about to step simulator
âœ… Simulator warm-up complete, first frames ready
DEBUG: about to get sensor observations
â³ No result yet for action_1766634525007_0
ğŸ¯ CURRENT SENSORS: ['rgba_camera', 'depth_camera']
DEBUG: about to extract rgb and depth
ğŸ¯ HABITAT ACTUAL POSITION: [-2.1321383   0.16337794 -1.22308   ]
ğŸ” MOTION CHECK DEBUG: Starting motion check
ğŸ”   last_processed_position: [-1.5029818   0.16337794  0.2222702 ]
ğŸ”   last_processed_time: 1766634513.285508
ğŸ”   skipped_frames: 0
ğŸ” MOTION CHECK: Current position = [-2.1321383   0.16337794 -1.22308   ]
ğŸ” MOTION CALCULATIONS:
ğŸ”   translation: 1.5763m, threshold: 0.2400m
ğŸ”   rotation: 0.7152rad, threshold: 0.1000rad
ğŸ”   time_delta: 11.96s, threshold: 1.0s
ğŸ”   motion_stats: avg_trans=0.300, avg_rot=0.100
ğŸ” MOTION CHECK: âœ… Translation exceeds threshold: 1.576m > 0.240m
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ¯ DEPTH STATS - Min: 0.021734539419412613, Max: 8.964672088623047, Mean: 0.996
âœ… Depth frame has 307200 valid pixels
DEBUG: RGBA -> RGB conversion for frame 2
DEBUG: Transposing rgb_frame from (640, 480, 3) to (480, 640, 3)
DEBUG: Depth frame shape before transpose: (640, 480)
DEBUG: Depth frame shape after transpose: (480, 640)
DEBUG: Converted depth dtype to uint16
DEBUG: about to rotate rgb frame
DEBUG: about to write rgb frame
ğŸ” DEBUG: stack[1].function = process_frame
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/orb_slam_integration.py
TRACK_REF_KF: Less than 15 matches!!
Fail to track local map!
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='process_frame [orb_slam_integration.py]', Model='orb_slam'
ğŸ¯ BUFFER DEBUG: Frame ID requested: 1
ğŸ¯ BUFFER DEBUG: Result type: <class 'tuple'>
âœ… BUFFER DEBUG: Got frame_dict type: <class 'dict'>
âœ… BUFFER DEBUG: Got metadata type: <class 'dict'>
ğŸ¯ DEPTH SCALE CHECK: range=(0.0, 10202.0) dtype=uint16
âœ… ORB-SLAM: PROCESSED FRAME #2 (buffer ID: 1) - translation_1.576m > 0.240m
   Current position: [-2.1321383   0.16337794 -1.22308   ]
   Actual movement: 1.576m
ğŸ”„ CONVERTING: Depth appears to be in millimeters
   Before: 0.0 to 10202.0 (uint16)
   After: 0.000m to 10.202m (float32)
ğŸ” DEPTH VALIDATION: 282699 valid pixels
ğŸ” RGB-D Frames: RGB=(480, 640, 3), Depth=(480, 640)
ğŸ” Depth info: dtype=float32, range=(0.000, 10.202)
ğŸ” DEBUG process_frame parameters:
   RGB: dtype=uint8, shape=(480, 640, 3)
   Depth: dtype=float32, shape=(480, 640)
   Timestamp: 0.033
ğŸ” ORB-SLAM RAW RESULT KEYS: ['current_pose', 'tracking_status', 'visible_points']
Current result dict pose : [[1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 0. 1.]]
ğŸ” ORB-SLAM INTERNAL DEBUG:
   Raw tracking_info keys: ['tracking_ok', 'tracking_lost', 'system_shutdown', 'visible_points_count']
   ğŸ“ tracking_ok: False
   ğŸ“ tracking_lost: False
   ğŸ“ system_shutdown: False
   ğŸ¯ TRACKING STATUS: âŒ NOT TRACKING
   ğŸ¯ LOST STATUS: âœ… NOT LOST
ğŸ” FEATURE DEBUG:
   ğŸ‘ï¸ Visible points: 0
ğŸš¨ CRITICAL: ORB-SLAM sees ZERO features!
   This means tracking CANNOT work!
   RGB brightness: 110.6 (should be > 50)
   Depth coverage: 92.0% (should be > 20%)
ğŸ¤– ORB-SLAM: Received valid 4x4 pose matrix
â³ Coordinates from orbslam Not aligned yet, storing SLAM-local pose
ğŸ¯ STORE DEBUG - Frame 1:
   ğŸ“¦ Current pose type: <class 'numpy.ndarray'>
   ğŸ“¦ Pose shape: (4, 4)
   ğŸ“¦ Translation: [[1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 0. 1.]]
ğŸ—ºï¸ ORB-SLAM: 0 tracked points this frame
âœ… ORB-SLAM: Pose stored successfully
ğŸš« ORB-SLAM: SKIPPING position - invalid tracking
   Reason: tracking_ok = False
   Reason: visible_points = 0 (need > 50)
ğŸ’¾ ORB-SLAM: Added 0 points to memory map
   ğŸ¯ Current Frame: 1
   ğŸ¤– Robot Position: [[1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 0. 1.]]
ğŸ’¾ ORB-SLAM: Frame 1 WORLD-ALIGNED data stored
ğŸ“Š ORB-SLAM FRAME 1 SUMMARY:
   Method: rgbd_processing
   Pose estimated: True
   Visible points: 0
   Tracking: FAILED
â³ No result yet for action_1766634525007_0
DEBUG: about to scale depth
ğŸ¯ DEBUG: Frame 2 - RGB shape: (480, 640, 3), Depth shape: (480, 640)
ğŸ¯ DEBUG: RGB dtype: uint8, Depth dtype: uint16
âœ… DEBUG: Frame 2 written to buffer - Slot 2
âœ… DEBUG: Buffer RGB dtype: uint8, Depth dtype: uint16
  Wrote frame 2 to buffer successfully âœ…
  âœ… Frame 3 captured - RGB: (480, 640, 3), Depth: (480, 640)
ğŸŸ¦ DEBUG: Calling ORB-SLAM process_frame() for frame 2
ğŸŸ© DEBUG: Returned from ORB-SLAM process_frame() for frame 2
ğŸ” Checking for queued actions...
ğŸ¯ HABITATSTORE: Pulling next action from queue
ğŸš€ HABITATSTORE: Pulled VELOCITY CONTROL command
   ğŸ“¦ Metadata: ['continuous_nav', 'recovery', 'attempt', 'ang_vel']
ğŸ“Š HABITATSTORE: Remaining in queue: 0
ğŸ¯ MAINORCH: Executing action 'velocity_control'
================================================================================
ğŸ¯ Executing Habitat action: velocity_control
ğŸ“¦ Parameters: {'linear_velocity': [0.0, 0.0, 0.0], 'angular_velocity': [0.0, 0.8, 0.0], 'duration': 0.1, 'is_velocity_command': True}
ğŸ“¦ Metadata: {'continuous_nav': True, 'recovery': True, 'attempt': 0, 'ang_vel': 0.8}
================================================================================
ğŸ§© Current agent position: [-2.1321383   0.16337794 -1.22308   ]
ğŸ§© Current agent rotation (quat): quaternion(0.898793816566467, 0.147603049874306, 0.27311310172081, 0.309503227472305)
ğŸ“ prev_pos saved: [-2.1321383   0.16337794 -1.22308   ]
ğŸ§® Linear velocity: [0.0, 0.0, 0.0], angular velocity: [0.0, 0.8, 0.0], dt=0.1
ğŸ“ RigidState before integrate: pos=Vector(-2.13214, 0.163378, -1.22308), rot=Quaternion({0.147603, 0.273113, 0.309503}, 0.898794)
ğŸ“ Predicted new_pos from VelocityControl: [-2.132138252258301, 0.16337794065475464, -1.223080039024353]
ğŸ§­ is_navigable(new_pos) = True
âœ… Using XZ from new_pos with preserved Y: [-2.132138252258301, 1.0, -1.223080039024353]
ğŸ“Œ Final applied position: [-2.132138252258301, 1.0, -1.223080039024353]
ğŸ“Œ Final applied rotation (quat): quaternion(0.887153267860413, 0.135108157992363, 0.308836817741394, 0.315158188343048)
ğŸ›‘ STALL CHECK: moved=0.8366m, stalled=True
ğŸ”„ METRICS: Velocity control - tracking position
ğŸ“ METRICS: Post-velocity pos = [-2.1321383  1.        -1.22308  ]
ğŸ“ METRICS: Trajectory len = 8
Observations from _execute_habitat_action : velocity control : {'rgba_camera': array([[[160, 153, 143, 255],
        [160, 153, 143, 255],
        [162, 154, 145, 255],
        ...,
        [140, 131, 115, 255],
        [142, 133, 117, 255],
        [144, 135, 119, 255]],

       [[159, 152, 142, 255],
        [160, 153, 143, 255],
        [160, 153, 143, 255],
        ...,
        [140, 131, 115, 255],
        [142, 133, 117, 255],
        [145, 135, 120, 255]],

       [[159, 152, 142, 255],
        [159, 152, 142, 255],
        [160, 153, 143, 255],
        ...,
        [140, 132, 115, 255],
        [143, 134, 118, 255],
        [145, 136, 120, 255]],

       ...,

       [[ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        ...,
        [142, 133, 134, 255],
        [142, 133, 134, 255],
        [142, 133, 132, 255]],

       [[ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        ...,
        [142, 133, 134, 255],
        [142, 133, 134, 255],
        [124, 118, 116, 255]],

       [[  5,   1,   5, 255],
        [ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        ...,
        [148, 138, 134, 255],
        [124, 118, 116, 255],
        [124, 118, 116, 255]]], dtype=uint8), 'depth_camera': array([[1.3974515 , 1.3887069 , 1.3800709 , ..., 0.34830037, 0.34775168,
        0.34720474],
       [1.4080421 , 1.3991648 , 1.3903986 , ..., 0.34895378, 0.34840307,
        0.34785408],
       [1.4187822 , 1.4097694 , 1.4008704 , ..., 0.3496097 , 0.3490569 ,
        0.34850582],
       ...,
       [0.78013664, 0.78171057, 0.78328717, ..., 2.8197758 , 2.8158479 ,
        2.8119779 ],
       [0.7785546 , 0.7801221 , 0.78169596, ..., 2.8168879 , 2.8129678 ,
        2.8073666 ],
       [0.77697897, 0.77854013, 0.7801076 , ..., 2.8139586 , 2.8090117 ,
        2.8016005 ]], dtype=float32)}
ğŸ‘ rgba_camera shape: (640, 480, 4)
ğŸŒŠ depth_camera shape: (640, 480)
âœ… Velocity motion applied (projected)
ğŸ“¤ Result (velocity_control): status=completed, collided=True, new_position=[-2.132138252258301, 1.0, -1.223080039024353]
================================================================================
ğŸ“¤ Result (velocity_control):{'action_id': 'action_1766634525007_0', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[160, 153, 143, 255],
        [160, 153, 143, 255],
        [162, 154, 145, 255],
        ...,
        [140, 131, 115, 255],
        [142, 133, 117, 255],
        [144, 135, 119, 255]],

       [[159, 152, 142, 255],
        [160, 153, 143, 255],
        [160, 153, 143, 255],
        ...,
        [140, 131, 115, 255],
        [142, 133, 117, 255],
        [145, 135, 120, 255]],

       [[159, 152, 142, 255],
        [159, 152, 142, 255],
        [160, 153, 143, 255],
        ...,
        [140, 132, 115, 255],
        [143, 134, 118, 255],
        [145, 136, 120, 255]],

       ...,

       [[ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        ...,
        [142, 133, 134, 255],
        [142, 133, 134, 255],
        [142, 133, 132, 255]],

       [[ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        ...,
        [142, 133, 134, 255],
        [142, 133, 134, 255],
        [124, 118, 116, 255]],

       [[  5,   1,   5, 255],
        [ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        ...,
        [148, 138, 134, 255],
        [124, 118, 116, 255],
        [124, 118, 116, 255]]], dtype=uint8), 'depth_camera': array([[1.3974515 , 1.3887069 , 1.3800709 , ..., 0.34830037, 0.34775168,
        0.34720474],
       [1.4080421 , 1.3991648 , 1.3903986 , ..., 0.34895378, 0.34840307,
        0.34785408],
       [1.4187822 , 1.4097694 , 1.4008704 , ..., 0.3496097 , 0.3490569 ,
        0.34850582],
       ...,
       [0.78013664, 0.78171057, 0.78328717, ..., 2.8197758 , 2.8158479 ,
        2.8119779 ],
       [0.7785546 , 0.7801221 , 0.78169596, ..., 2.8168879 , 2.8129678 ,
        2.8073666 ],
       [0.77697897, 0.77854013, 0.7801076 , ..., 2.8139586 , 2.8090117 ,
        2.8016005 ]], dtype=float32)}, 'collided': True, 'new_position': [-2.132138252258301, 1.0, -1.223080039024353], 'new_rotation': quaternion(0.887153267860413, 0.135108157992363, 0.308836817741394, 0.315158188343048), 'timestamp': 1766634525.3314662}
Action in habitat simulation result : {'action_id': 'action_1766634525007_0', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[160, 153, 143, 255],
        [160, 153, 143, 255],
        [162, 154, 145, 255],
        ...,
        [140, 131, 115, 255],
        [142, 133, 117, 255],
        [144, 135, 119, 255]],

       [[159, 152, 142, 255],
        [160, 153, 143, 255],
        [160, 153, 143, 255],
        ...,
        [140, 131, 115, 255],
        [142, 133, 117, 255],
        [145, 135, 120, 255]],

       [[159, 152, 142, 255],
        [159, 152, 142, 255],
        [160, 153, 143, 255],
        ...,
        [140, 132, 115, 255],
        [143, 134, 118, 255],
        [145, 136, 120, 255]],

       ...,

       [[ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        ...,
        [142, 133, 134, 255],
        [142, 133, 134, 255],
        [142, 133, 132, 255]],

       [[ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        ...,
        [142, 133, 134, 255],
        [142, 133, 134, 255],
        [124, 118, 116, 255]],

       [[  5,   1,   5, 255],
        [ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        ...,
        [148, 138, 134, 255],
        [124, 118, 116, 255],
        [124, 118, 116, 255]]], dtype=uint8), 'depth_camera': array([[1.3974515 , 1.3887069 , 1.3800709 , ..., 0.34830037, 0.34775168,
        0.34720474],
       [1.4080421 , 1.3991648 , 1.3903986 , ..., 0.34895378, 0.34840307,
        0.34785408],
       [1.4187822 , 1.4097694 , 1.4008704 , ..., 0.3496097 , 0.3490569 ,
        0.34850582],
       ...,
       [0.78013664, 0.78171057, 0.78328717, ..., 2.8197758 , 2.8158479 ,
        2.8119779 ],
       [0.7785546 , 0.7801221 , 0.78169596, ..., 2.8168879 , 2.8129678 ,
        2.8073666 ],
       [0.77697897, 0.77854013, 0.7801076 , ..., 2.8139586 , 2.8090117 ,
        2.8016005 ]], dtype=float32)}, 'collided': True, 'new_position': [-2.132138252258301, 1.0, -1.223080039024353], 'new_rotation': quaternion(0.887153267860413, 0.135108157992363, 0.308836817741394, 0.315158188343048), 'timestamp': 1766634525.3314662}
âœ… Storing SINGLE result for action_1766634525007_0
Storing result using store_action_result() for action_1766634525007_0 | All stored IDs: ['action_1766634512479_0', 'action_1766634512600_1', 'action_1766634512722_2', 'action_1766634512845_3', 'action_1766634512966_4', 'action_1766634525007_0']
Stored results for id : action_1766634525007_0 : {'action_id': 'action_1766634525007_0', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[160, 153, 143, 255],
        [160, 153, 143, 255],
        [162, 154, 145, 255],
        ...,
        [140, 131, 115, 255],
        [142, 133, 117, 255],
        [144, 135, 119, 255]],

       [[159, 152, 142, 255],
        [160, 153, 143, 255],
        [160, 153, 143, 255],
        ...,
        [140, 131, 115, 255],
        [142, 133, 117, 255],
        [145, 135, 120, 255]],

       [[159, 152, 142, 255],
        [159, 152, 142, 255],
        [160, 153, 143, 255],
        ...,
        [140, 132, 115, 255],
        [143, 134, 118, 255],
        [145, 136, 120, 255]],

       ...,

       [[ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        ...,
        [142, 133, 134, 255],
        [142, 133, 134, 255],
        [142, 133, 132, 255]],

       [[ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        ...,
        [142, 133, 134, 255],
        [142, 133, 134, 255],
        [124, 118, 116, 255]],

       [[  5,   1,   5, 255],
        [ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        ...,
        [148, 138, 134, 255],
        [124, 118, 116, 255],
        [124, 118, 116, 255]]], dtype=uint8), 'depth_camera': array([[1.3974515 , 1.3887069 , 1.3800709 , ..., 0.34830037, 0.34775168,
        0.34720474],
       [1.4080421 , 1.3991648 , 1.3903986 , ..., 0.34895378, 0.34840307,
        0.34785408],
       [1.4187822 , 1.4097694 , 1.4008704 , ..., 0.3496097 , 0.3490569 ,
        0.34850582],
       ...,
       [0.78013664, 0.78171057, 0.78328717, ..., 2.8197758 , 2.8158479 ,
        2.8119779 ],
       [0.7785546 , 0.7801221 , 0.78169596, ..., 2.8168879 , 2.8129678 ,
        2.8073666 ],
       [0.77697897, 0.77854013, 0.7801076 , ..., 2.8139586 , 2.8090117 ,
        2.8016005 ]], dtype=float32)}, 'collided': True, 'new_position': [-2.132138252258301, 1.0, -1.223080039024353], 'new_rotation': quaternion(0.887153267860413, 0.135108157992363, 0.308836817741394, 0.315158188343048), 'timestamp': 1766634525.3314662}
âœ… Action 'velocity_control' completed, result stored
ğŸ¯ HABITATSTORE: Pulling next action from queue
ğŸ“­ HABITATSTORE: Action queue is empty
  ğŸ–¼ Frame 2 shape: (480, 640, 3)
  âœ… Frame orientation: PORTRAIT (480x640)
âœ… Retrieved result for action_1766634525007_0 : {'action_id': 'action_1766634525007_0', 'status': 'completed', 'action': 'velocity_control', 'observations': {'rgba_camera': array([[[160, 153, 143, 255],
        [160, 153, 143, 255],
        [162, 154, 145, 255],
        ...,
        [140, 131, 115, 255],
        [142, 133, 117, 255],
        [144, 135, 119, 255]],

       [[159, 152, 142, 255],
        [160, 153, 143, 255],
        [160, 153, 143, 255],
        ...,
        [140, 131, 115, 255],
        [142, 133, 117, 255],
        [145, 135, 120, 255]],

       [[159, 152, 142, 255],
        [159, 152, 142, 255],
        [160, 153, 143, 255],
        ...,
        [140, 132, 115, 255],
        [143, 134, 118, 255],
        [145, 136, 120, 255]],

       ...,

       [[ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        ...,
        [142, 133, 134, 255],
        [142, 133, 134, 255],
        [142, 133, 132, 255]],

       [[ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        ...,
        [142, 133, 134, 255],
        [142, 133, 134, 255],
        [124, 118, 116, 255]],

       [[  5,   1,   5, 255],
        [ 10,   2,  10, 255],
        [ 10,   2,  10, 255],
        ...,
        [148, 138, 134, 255],
        [124, 118, 116, 255],
        [124, 118, 116, 255]]], dtype=uint8), 'depth_camera': array([[1.3974515 , 1.3887069 , 1.3800709 , ..., 0.34830037, 0.34775168,
        0.34720474],
       [1.4080421 , 1.3991648 , 1.3903986 , ..., 0.34895378, 0.34840307,
        0.34785408],
       [1.4187822 , 1.4097694 , 1.4008704 , ..., 0.3496097 , 0.3490569 ,
        0.34850582],
       ...,
       [0.78013664, 0.78171057, 0.78328717, ..., 2.8197758 , 2.8158479 ,
        2.8119779 ],
       [0.7785546 , 0.7801221 , 0.78169596, ..., 2.8168879 , 2.8129678 ,
        2.8073666 ],
       [0.77697897, 0.77854013, 0.7801076 , ..., 2.8139586 , 2.8090117 ,
        2.8016005 ]], dtype=float32)}, 'collided': True, 'new_position': [-2.132138252258301, 1.0, -1.223080039024353], 'new_rotation': quaternion(0.887153267860413, 0.135108157992363, 0.308836817741394, 0.315158188343048), 'timestamp': 1766634525.3314662}using get_action_result() 
âŒ Recovery still collided
Target xyz : [-0.15094197  2.35249805 -2.47645044]
Current position : [-1.69540429  1.         -0.61764616]
Delta vector calculated : [ 1.54446232  1.35249805 -1.85880429]
ğŸ“Š Step 0: Distance to target = 2.42m
ğŸ” [STUCK_DETECT] Starting FIXED detection...
âš ï¸ [STUCK_DETECT] Not enough points for oscillation: 8 < 30
âœ… REACHED TARGET! Distance: 2.42m
ğŸ¯ GOAL POSITION REACHED - Flag set!
ğŸ“¥ HABITATSTORE: Pushing action 'velocity_control' to queue
ğŸ” MODEL OUTPUT DEBUG:
   - Output type: <class 'transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput'>
   - logits shape: torch.Size([1, 576, 1])
   - logits min/max: -16.885/-6.075
   - pred_boxes shape: torch.Size([1, 576, 4])
ğŸ” POST-PROCESSING DEBUG:
   - target_sizes: tensor([[480, 640]], device='cuda:0')
   - confidence_threshold: 0.05
ğŸ” POST-PROCESS RESULTS DEBUG:
   - Results type: <class 'list'>
   - Results length: 1
ğŸ” RAW RESULT DEBUG:
   - Scores shape: torch.Size([0])
   - Labels shape: torch.Size([0])
   - Boxes shape: torch.Size([0, 4])
ğŸ¯ ALL SCORES:
ğŸ” CONFIDENCE STATS in:
   - Total predictions: 0
   - Passing threshold: 0
ğŸ¯ DEBUG: Raw results - scores: 0, labels: 0
âœ… OWL-ViT found 0 objects
ğŸ¯ 2D detections: 0
âœ… OWL: Got ALREADY-TRANSFORMED pose for frame 1
ğŸ“ Camera pose available
ğŸ”„ Projecting detections to 3D
ğŸ¯ PROJECTION: Starting 3D projection for 0 detections
ğŸ¯ PROJECTION: Camera pose available: True
ğŸ¯ PROJ DEBUG 1: Starting projection for frame 1
ğŸ¯ PROJ DEBUG 1.5: Detections count: 0
ğŸ¯ ORB-SLAM VERIFY: Camera pose OK for frame 1
   Position: [[1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 0. 1.]]
ğŸ¯ PROJ DEBUG 9: Using passed frame_dict, keys: dict_keys(['rgb', 'depth'])
ğŸ¯ PROJ DEBUG 10: depth_frame type: <class 'numpy.ndarray'>
ğŸ¯ PROJ DEBUG 13: Height: 480, Width: 640
ğŸ¯ PROJ DEBUG 14: Intrinsics: fx=320.00000000000006, fy=320.00000000000006
âœ… Projected 0 objects using pixel+depth+pose
ğŸ¯ PROJECTION: ORB-SLAM returned 0 objects
ğŸŒ 3D objects projected: 0
Tracking 0 objects after update
ğŸ§­ Objects after tracking: 0
Updated semantic store for frame 1 with 0 objects
ğŸ—ºï¸  Shared map store updated
â±ï¸  Processing time: 0.335s
ğŸ“Š Counters â†’ processed=2, avg_time=0.646s
âœ… OWL DEBUG â”€â”€â”€ END FRAME 1 â”€â”€â”€

ğŸ¦‰ OWL: starting to Process REAL frame 2 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ“¦ HABITATSTORE: Added metadata to action 'velocity_control'
âœ… HABITATSTORE: Action 'velocity_control' queued (ID: action_1766634525348_0)
ğŸ“Š HABITATSTORE: Queue size: 1
ğŸ›‘ STOP PUSHED (ID: action_1766634525348_0) - EXITING NAV LOOP
Eecution time: 0.7343308925628662
Result Dict : {'action': 'move_forward', 'success': True, 'execution_time': 0.7343308925628662, 'parameters': {'target_object': 'object1', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}, 'timestamp': 1766634525.3485382, 'notes': 'Continuous navigation completed', 'collision_occurred': True, 'movement_verified': True, 'collision_count': 1, 'is_stuck': False}
ğŸ“‹ Continuous navigation result: success=True, steps=0, final_dist=4.17m
Continuous Navigation Result : {'action': 'move_forward', 'success': True, 'execution_time': 0.7343308925628662, 'parameters': {'target_object': 'object1', 'search_pattern': 'pattern description', 'source': 'tree_of_thoughts', 'target_coordinates': None}, 'timestamp': 1766634525.3485382, 'notes': 'Continuous navigation completed', 'collision_occurred': True, 'movement_verified': True, 'collision_count': 1, 'is_stuck': False, 'observations': {'rgba_camera': array([[[165, 157, 148, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [165, 157, 148, 255],
        [165, 157, 148, 255],
        ...,
        [141, 132, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       [[166, 159, 151, 255],
        [166, 159, 151, 255],
        [165, 157, 148, 255],
        ...,
        [141, 131, 115, 255],
        [140, 131, 115, 255],
        [140, 131, 115, 255]],

       ...,

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [158, 153, 144, 255]],

       [[109,  92,  76, 255],
        [109,  92,  76, 255],
        [109,  92,  76, 255],
        ...,
        [158, 153, 144, 255],
        [158, 153, 144, 255],
        [161, 153, 144, 255]]], dtype=uint8), 'depth_camera': array([[1.2247428 , 1.2181447 , 1.2116172 , ..., 0.33664986, 0.33839086,
        0.34014854],
       [1.2328697 , 1.2261839 , 1.2195703 , ..., 0.3378777 , 0.3396314 ,
        0.34140205],
       [1.2411052 , 1.23433   , 1.2276285 , ..., 0.3391145 , 0.3408811 ,
        0.34266484],
       ...,
       [0.74431515, 0.745529  , 0.7467501 , ..., 2.5809772 , 2.5786366 ,
        2.5763798 ],
       [0.7435729 , 0.74478436, 0.74600303, ..., 2.5783198 , 2.5760236 ,
        2.573692  ],
       [0.74282885, 0.74403787, 0.7452541 , ..., 2.5756283 , 2.5733368 ,
        2.5710497 ]], dtype=float32)}, 'continuous_nav': {'target_xyz': [-0.15094196796417236, 2.3524980545043945, -2.4764504432678223], 'dist_remaining': 4.17073862475074, 'lin_vel': 0.02, 'ang_vel': -1.5, 'yaw_error_deg': -116.26382709725237, 'complete': True}, 'predicted_new_pos': [-1.3085230628825408, 1.5, 1.5313485758232563]}
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'

ğŸŸ¢ OWL DEBUG â”€â”€â”€ START FRAME 2 â”€â”€â”€
ğŸ“¦ Frame metadata:
   â€¢ frame_id      = 2
   â€¢ timestamp     = 0.066
   â€¢ rgb shape     = (480, 640, 3)
   â€¢ depth shape   = (480, 640)
ğŸ¯ Detection input: shape=(480, 640, 3), dtype=uint8
ğŸ” FRAME STATS:
   - Min pixel value: 0
   - Max pixel value: 255
   - Mean brightness: 141.2
   - Brightness std: 71.2
ğŸ¦‰ OWL DEBUG: Starting _get_text_queries_from_task_store
ğŸ¦‰ OWL DEBUG: self.task_store exists = True
ğŸ¯ OWL HABITATSTORE: Using stored goal 'tv'
ğŸ” CURRENT QUERIES DEBUG:
   - Query count: 1
   - First 3 queries: ['tv']
ğŸ” INPUTS DEBUG:
   - input_ids shape: torch.Size([1, 16])
   - attention_mask shape: torch.Size([1, 16])
   - pixel_values shape: torch.Size([1, 3, 768, 768])
   - pixel_values dtype: torch.float32
   - pixel_values min/max: -1.792/2.146
ğŸ” OWL DEBUG: Inputs keys: ['input_ids', 'attention_mask', 'pixel_values']
ğŸ” OWL DEBUG: Text queries: ['tv']
  ğŸ–¼ Saved to: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments/results/ours_full/00800_tv_003/ours_full_frame_2.png
  ğŸ–¼ Saved ours_full_frame_2.png
ğŸ’¡ Processing frame 2 brightness: 141.2
âœ… Frame 2 captured - RGB: (480, 640, 3), Depth: (480, 640)
ğŸ“Š Progress: 3/500 frames (0.6%)
ğŸ” MODEL OUTPUT DEBUG:
   - Output type: <class 'transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput'>
   - logits shape: torch.Size([1, 576, 1])
   - logits min/max: -14.642/-3.658
   - pred_boxes shape: torch.Size([1, 576, 4])
ğŸ” POST-PROCESSING DEBUG:
   - target_sizes: tensor([[480, 640]], device='cuda:0')
   - confidence_threshold: 0.05
ğŸ” POST-PROCESS RESULTS DEBUG:
   - Results type: <class 'list'>
   - Results length: 1
ğŸ” RAW RESULT DEBUG:
   - Scores shape: torch.Size([0])
   - Labels shape: torch.Size([0])
   - Boxes shape: torch.Size([0, 4])
ğŸ¯ ALL SCORES:
ğŸ” CONFIDENCE STATS in:
   - Total predictions: 0
   - Passing threshold: 0
ğŸ¯ DEBUG: Raw results - scores: 0, labels: 0
âœ… OWL-ViT found 0 objects
ğŸ¯ 2D detections: 0
âŒ OWL: get_pose_matrix_for_frame returned None for 2
âŒ OWL: Could not get transformed pose for frame 2
âš ï¸  Camera pose NOT available â€” skipping 3D projection
â­ï¸  Skipping 3D stage (no pose or no detections)
â±ï¸  Processing time: 0.068s
ğŸ“Š Counters â†’ processed=3, avg_time=0.454s
âœ… OWL DEBUG â”€â”€â”€ END FRAME 2 â”€â”€â”€

ğŸ¦‰ OWL: starting to Process REAL frame 2 
ğŸ” DEBUG: This is FrameBuffer version WITH FILENAME SUPPORT
ğŸ” DEBUG: stack[1].function = _continuous_processing_loop
ğŸ” DEBUG: stack[1].filename = /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/src/perception_pipeline/owl_integration.py
ğŸ” DEBUG: type of filename = <class 'str'>
ğŸ” FRAMEBUFFER DEBUG: Caller function='_continuous_processing_loop [owl_integration.py]', Model='owl'
âš ï¸  Skipping frame 2 for owl (already processed)
âŒ OWL processing error: cannot unpack non-iterable NoneType object

==================================================
ğŸ ORCHESTRATION COMPLETED
==================================================
ğŸ¯ Target frames: 500
âœ… Frames written to buffer: 3
âŒ Frames failed: 0
ğŸ“Š Total attempted: 3
ğŸ¯ Next frame ID (if continued): 3
â±ï¸  Total time: 12.41 seconds
ğŸ“ˆ Average FPS: 0.24

ğŸ”§ Buffer Statistics:
   Utilization: 4.7%
   Health Score: 100/100
   Active Readers: Not available
ğŸ” Mission check: 1 total goals, 1 remaining
   Mission goals: {'tv'}
   Remaining goals: ['tv']

ğŸ¯ Mission status: INCOMPLETE
==================================================
ğŸ›‘ Shutting down main orchestration...
ğŸ”š Shutting down run_action_pipeline execution...
Action Executor shutdown complete
ğŸ’¾ Saving final predictions to disk...
ğŸ’¾ 1 predictions saved to experiments/results/predictions/final_predictions.json
âœ… Final predictions saved to: experiments/results/predictions/final_predictions.json
Tree of Thoughts shutdown
Integrated reasoning pipeline shutdown complete
Shutting down ORB-SLAM3 integration...
âŒ TRAJECTORY FAILED: experiments/results/trajectory/all_frames.tum
âŒ KEYFRAMES FAILED: experiments/results/trajectory/keyframes.tum
Shutting down OWL integration...
ğŸ›‘ Stopping OWL thread...
ğŸ›‘ OWL continuous processing loop stopped
â±ï¸  OWL TOTAL STATS:
   ğŸ“Š Frames processed: 3
   â° Total wall time: 14.06s
   ğŸ”„ Total processing time: 1.36s
   ğŸ“ˆ Average FPS: 0.21
   âš¡ Processing efficiency: 9.7%
ğŸ›‘ OWL: Stopped continuous processing
OWL performance stats: {'total_frames_processed': 3, 'avg_processing_time': 0.4537197748819987, 'max_processing_time': 0.9580132961273193, 'min_processing_time': 0.06836247444152832, 'current_objects_count': 0, 'last_processed_frame': -1, 'text_queries_count': 100, 'total_processing_time': 1.361159324645996, 'batch_size': 2, 'batch_efficiency': 0.22685988744099936}
OWL integration shutdown complete
ğŸ›‘ HabitatStore shutdown complete
ğŸ§¹ Shutting down DEADLOCK-FREE Frame Buffer...
âœ… Shared memory released
ğŸ¯ DEADLOCK-FREE Buffer shutdown complete
ğŸ”„ MapStore shutdown - saving final map...
ğŸ“ Found ORB-SLAM map: projects/hybrid_zero_shot_slam_nav/main/experiments/results/logs/orbslam_map.json
âœ… Using camera parameters from MapStore cache
ğŸ’¾ NPZ saved: experiments/results/final_maps/orb_slam_map_20251225_034845.npz
âœ… MapStore shutdown complete. Maps saved:
   ğŸ“ Complete: experiments/results/final_maps/orb_slam_map_20251225_034845.json
ğŸ—ºï¸ FINAL MAP SAVED: experiments/results/final_maps/orb_slam_map_20251225_034845.json
âœ… DEBUG: Orchestration completed!
ğŸ“ˆ DEBUG: Trajectory extracted, len = 8
ğŸ’¥ DEBUG: Collisions extracted = 0
ğŸ“Š DEBUG: Computing final metrics...
ğŸ DEBUG: Final pos = [-2.1321383  1.        -1.22308  ]
ğŸ“ DEBUG: Final distance = 2.707m
âœ… DEBUG: Success = 0 (threshold 0.2m)
ğŸ›¤ï¸ 2D XZ Path length = 11.7m (8 points)
ğŸ“ DEBUG: SPL = 0.0 (no success or short trajectory)
ğŸ”¥ DEBUG: Failure mode = 'distance'
ğŸ“‹ DEBUG: Final metrics = {'success': 0, 'spl': 0.0, 'path_length': 11.661485, 'collisions': 0, 'final_distance': 2.706534144357405, 'failure_mode': 'distance', 'trajectory': [array([-1.31,  1.5 ,  1.53], dtype=float32), array([-1.5029818,  1.       ,  0.2222702], dtype=float32), array([-1.3989272 ,  1.        ,  0.12725905], dtype=float32), array([-1.2948726 ,  1.        ,  0.03224789], dtype=float32), array([-1.1908181 ,  1.        , -0.06276327], dtype=float32), array([-1.1908181 ,  1.        , -0.06276327], dtype=float32), array([-1.6954043 ,  1.        , -0.61764616], dtype=float32), array([-2.1321383,  1.       , -1.22308  ], dtype=float32)]}
ğŸ›‘ DEBUG: Shutting down orchestrator...
ğŸ›‘ Shutting down main orchestration...
ğŸ”š Shutting down run_action_pipeline execution...
Action Executor shutdown complete
ğŸ’¾ Saving final predictions to disk...
ğŸ’¾ 1 predictions saved to experiments/results/predictions/final_predictions.json
âœ… Final predictions saved to: experiments/results/predictions/final_predictions.json
Tree of Thoughts shutdown
Integrated reasoning pipeline shutdown complete
Shutting down ORB-SLAM3 integration...
âŒ TRAJECTORY FAILED: experiments/results/trajectory/all_frames.tum
âŒ KEYFRAMES FAILED: experiments/results/trajectory/keyframes.tum
Shutting down OWL integration...
â±ï¸  OWL TOTAL STATS:
   ğŸ“Š Frames processed: 3
   â° Total wall time: 14.48s
   ğŸ”„ Total processing time: 1.36s
   ğŸ“ˆ Average FPS: 0.21
   âš¡ Processing efficiency: 9.4%
ğŸ›‘ OWL: Stopped continuous processing
OWL performance stats: {'total_frames_processed': 3, 'avg_processing_time': 0.4537197748819987, 'max_processing_time': 0.9580132961273193, 'min_processing_time': 0.06836247444152832, 'current_objects_count': 0, 'last_processed_frame': -1, 'text_queries_count': 100, 'total_processing_time': 1.361159324645996, 'batch_size': 2, 'batch_efficiency': 0.22685988744099936}
OWL integration shutdown complete
ğŸ›‘ HabitatStore shutdown complete
ğŸ”„ MapStore shutdown - saving final map...
ğŸ“ Found ORB-SLAM map: projects/hybrid_zero_shot_slam_nav/main/experiments/results/logs/orbslam_map.json
âœ… Using camera parameters from MapStore cache
ğŸ’¾ NPZ saved: experiments/results/final_maps/orb_slam_map_20251225_034846.npz
âœ… MapStore shutdown complete. Maps saved:
   ğŸ“ Complete: experiments/results/final_maps/orb_slam_map_20251225_034846.json
ğŸ—ºï¸ FINAL MAP SAVED: experiments/results/final_maps/orb_slam_map_20251225_034846.json
âœ… DEBUG: Shutdown complete!
ğŸ” DEBUG: About to plot trajectory...
   Trajectory length: 8
   Start pos: [-1.31  1.5   1.53]
   Final pos: [-2.1321383  1.        -1.22308  ]
   Goal pos: [-0.15094196796417236, 2.3524980545043945, -2.4764504432678223]
   Frame dir: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments/results/ours_full/00800_tv_003
   Final distance: 2.707m
âœ… frame_dir exists: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments/results/ours_full/00800_tv_003
ğŸ¨ INSIDE plot_trajectory function...
   Inputs: trajectory_len=8, save_dir=/mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments/results/ours_full/00800_tv_003
   X values: 8 points, Z values: 8 points
ğŸ¯ ACTION EXECUTOR: Starting execution of execute_single_action: action type : 'move_forward'
ğŸ“ Valid initial pose: [-1.31, 1.5, 1.53]
Target coordinates inside action executor: None
âš ï¸ WARNING: target_coordinates is None! Checking HabitatStore...
ğŸ¯ HABITATSTORE: Using stored goal 'tv' at [-0.15094196796417236, 2.3524980545043945, -2.4764504432678223]
ğŸ¯ CONTINUOUS NAVIGATION to [-0.15094196796417236, 2.3524980545043945, -2.4764504432678223]
Current quat from Habitat: quaternion(0.932327345606034, 0, 0.361615431964962, 0)
ğŸ”„ Starting CONTINUOUS NAVIGATION to [-0.15094196796417236, 2.3524980545043945, -2.4764504432678223]
Target xyz : [-0.15094197  2.35249805 -2.47645044]
Current position : [-1.31  1.5   1.53]
Delta vector calculated : [ 1.15905803  0.85249805 -4.00645044]
ğŸ“Š Step 0: Distance to target = 4.17m
ğŸ” [STUCK_DETECT] Starting FIXED detection...
âš ï¸ [STUCK_DETECT] Not enough points for oscillation: 8 < 30
YAW : 0.7400000000000001
Desired YAW : -1.2891865838164553
YAW Error : -2.0291865838164553
ğŸ” [I-JEPA] Fetching from RAW predictions...
âœ… Using I-JEPA: pred_reason_1766634510284_1766634513785
   Has continuity_predictions: True
   Has structural_risks: True
âœ… [NAV] Got I-JEPA prediction with keys: ['prediction_id', 'predictor_type', 'timestamp', 'status', 'structural_analysis', 'continuity_predictions', 'structural_risks', 'processing_metrics', 'data_sources_used']
ğŸ“Š [NAV] Continuity keys: ['move_forward', 'turn_left', 'turn_right']
ğŸ“Š [NAV] Risks keys: ['move_forward', 'turn_left', 'turn_right']
ğŸ§  I-JEPA ANALYSIS: L:0.14 (risk:0.58) vs R:0.14 (risk:0.58)
ğŸ§  I-JEPA: NO BIAS (balanced: L:0.06 â‰ˆ R:0.06)
âœ… NORMAL MODE: lin=0.02 ang=-1.50 (yaw_err*2=-4.06 + ijepa=0.00)
ğŸ¯ FINAL VELOCITY: lin_vel=0.020 ang_vel=-1.500
Linear velocity : 0.02
Angular veloctiy : -1.5
ğŸ”® Predicted from ACTUAL pos: [-1.30852306  1.5         1.53134858]
Velocity metadata : {'target_xyz': [-0.15094196796417236, 2.3524980545043945, -2.4764504432678223], 'dist_remaining': 4.17073862475074, 'lin_vel': 0.02, 'ang_vel': -1.5, 'yaw_error_deg': -116.26382709725237, 'step_id': 0, 'current_pos': [-1.31, 1.5, 1.53], 'predicted_pos': [-1.3085230628825408, 1.5, 1.5313485758232563], 'continuous_nav': True, 'velocity_data': {'lin_vel': 0.02, 'ang_vel': -1.5, 'duration': 0.1}}
  ğŸ¯ Velocity: lin=0.02 m/s, ang=-1.50 rad/s
Velocity parameters : {'linear_velocity': [0.02, 0.0, 0.0], 'angular_velocity': [0.0, -1.5, 0.0], 'duration': 0.1, 'is_velocity_command': True}
ğŸ“¥ HABITATSTORE: Pushing action 'velocity_control' to queue
ğŸ“¦ HABITATSTORE: Added metadata to action 'velocity_control'
âœ… HABITATSTORE: Action 'velocity_control' queued (ID: action_1766634526363_1)
ğŸ“Š HABITATSTORE: Queue size: 2
ğŸ“¤ Pushed velocity command (ID: action_1766634526363_1)
â³ No result yet for action_1766634526363_1
   Saving plot to: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments/results/ours_full/00800_tv_003/trajectory_plot.png
â³ No result yet for action_1766634526363_1
â³ No result yet for action_1766634526363_1
â³ No result yet for action_1766634526363_1
ğŸ“ˆ Trajectory plot saved to: /mnt/d/Coding/Business/Kulfi_Startup_Code/robotics_workspace/projects/hybrid_zero_shot_slam_nav/main/experiments/paper_experiments/Main_Experiments/results/ours_full/00800_tv_003/trajectory_plot.png
âœ… plot_trajectory() call completed
ğŸ† DEBUG: Returning metrics to runner!
[DEBUG] Logging results | policy=ours_full | episode=00800_tv_003
[DEBUG] Appended row to existing CSV
âœ… Logged ours_full: success=0, SPL=0.000
[DEBUG] Saving failure data | policy=ours_full | episode=00800_tv_003
âŒ ours_full FAILED | error=[Errno 2] No such file or directory: 'logs/failures'
â³ No result yet for action_1766634526363_1
