Actively Building !! 
Work in Progress !! 

# ğŸ¤– Modular Robotics Navigation Pipeline

This repository contains a multi-stage pipeline for autonomous robot navigation, featuring SLAM integration, I-JEPA-based world modeling, Tree-of-Thoughts planning, and continuous velocity control.

## ğŸ“‚ Project Structure

```text
main/src/
â”œâ”€â”€ perception_pipeline/          # ğŸ‘ï¸ External World Sensing
â”‚   â”œâ”€â”€ ORB_SLAM3/                # C++ SLAM core
â”‚   â”œâ”€â”€ orb_slam_integration.py   # Python wrapper for SLAM tracking
â”‚   â”œâ”€â”€ orbslam_pybind.cpp        # Pybind11 bindings for C++/Python bridge
â”‚   â”œâ”€â”€ owl_integration.py        # Open-World Localization/Detection
â”‚   â””â”€â”€ umeyama_alignment.py      # Spatial coordinate synchronization
â”‚
â”œâ”€â”€ prediction_pipeline/          # ğŸ§  Future State Modeling (I-JEPA)
â”‚   â”œâ”€â”€ ijepa_model/              # Trained weights and architecture
â”‚   â”œâ”€â”€ ijepa_integration_predictor.py  # I-JEPA inference logic
â”‚   â”œâ”€â”€ collision_risk_predictor.py     # Structural risk assessment
â”‚   â””â”€â”€ ijepa_prototypes.json     # Encoded environmental feature templates
â”‚
â”œâ”€â”€ reasoning_planning_pipeline/  # âš–ï¸ Decision Making
â”‚   â”œâ”€â”€ tree_of_thoughts_integration.py # Multi-path heuristic planning
â”‚   â””â”€â”€ spatial_reasoning_integration.py # NavMesh & geometric reasoning
â”‚
â”œâ”€â”€ action_pipeline/              # âš™ï¸ Execution & Control
â”‚   â”œâ”€â”€ action_executor.py        # High-level action dispatcher
â”‚   â””â”€â”€ (ContinuousNavAgent logic) # PID/Velocity control loops
â”‚
â””â”€â”€ stores/                       # ğŸ—„ï¸ Central Data Hubs (IPC)
    â”œâ”€â”€ habitat_store.py          # Bridge to Habitat Sim / Physics feedback
    â”œâ”€â”€ central_map_store.py      # Global SLAM map & occupancy
    â”œâ”€â”€ prediction_store.py       # I-JEPA bias & risk scores
    â””â”€â”€ task_store.py             # Mission status & goal tracking

```

---

## ğŸš€ Pipeline Workflow

The system operates in a reactive-semantic loop, allowing for both precise geometric navigation and intelligent "intuition" based on visual features.

### 1. Perception Layer

* **ORB_SLAM3:** Tracks the robot's pose in real-time and builds a point cloud.
* **OWL-ViT:** Identifies semantic objects in the frame.
* **Coordinate Alignment:** Uses the Umeyama algorithm to align the SLAM coordinate system with the simulator's global NavMesh.

### 2. Prediction Layer (I-JEPA Integration)

Unlike traditional pathfinders, this pipeline uses **I-JEPA** (Image Joint-Embedding Predictive Architecture) to predict the "continuity" of the environment.

* **Semantic Bias:** Predicts whether turning left or right leads to better "free space" travel.
* **Structural Risk:** Identifies potential collisions before they appear on the NavMesh.

### 3. Reasoning & Planning

* **Tree of Thoughts (ToT):** Evaluates multiple potential navigation paths based on task requirements.
* **Spatial Reasoning:** Converts high-level LLM commands into specific 3D target coordinates.

### 4. Action & Execution

* **Continuous Navigation:** Executes a `while` loop that pushes velocity commands (`lin_vel`, `ang_vel`) to the Habitat Simulator.
* **Reactive Tweak:** Implements a vector-based "push" that steers the robot away from obstacles using real-time NavMesh distance feedback.

---

## ğŸ› ï¸ Key Components Detail

### **Action Executor (`action_executor.py`)**

Handles the transition from discrete plans to continuous movement. It manages:

* **Single-Turn Alignment:** Rotating toward the goal before moving.
* **Velocity Control:** Scaling speed based on distance and obstacle proximity.
* **Recovery Loops:** Automatic "Circle Escapes" when the robot detects it is stuck.

### **Habitat Store (`habitat_store.py`)**

The primary interface for asynchronous communication with the simulator. It allows the planning thread to "push" actions and "pull" results (position, rotation, collision flags) without blocking the simulation engine.

### **I-JEPA Predictor (`ijepa_integration_predictor.py`)**

Analyzes incoming frames against a set of prototype images to generate a `continuity_score`. This score is used as an **Angular Velocity Bias**, nudging the robot toward open areas.

---

## ğŸ“‹ Requirements

* **Python 3.8+**
* **Habitat-Sim** & **Habitat-Lab**
* **Pybind11** (for ORB_SLAM3 bindings)
* **PyTorch** (for I-JEPA inference)
* **NumPy / SciPy** (for Umeyama alignment and vector math)
 
